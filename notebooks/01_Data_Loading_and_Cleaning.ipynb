{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 01: Data Loading, Validation, and Cleaning\n",
                "\n",
                "---\n",
                "\n",
                "## Executive Summary\n",
                "\n",
                "This notebook serves as the foundational step in our Customer Lifetime Value (CLV) prediction pipeline. Before any sophisticated modeling can take place, we must ensure our data is **clean, consistent, and well-understood**. Think of this as the \"pre-flight checklist\" before launching a rocket‚Äîevery sensor must be verified, every system calibrated.\n",
                "\n",
                "### What This Notebook Covers:\n",
                "\n",
                "1. **Project Context & Business Problem** ‚Äî Understanding *why* we're doing this analysis\n",
                "2. **Data Ingestion** ‚Äî Loading the raw dataset with proper validation\n",
                "3. **Schema Inspection** ‚Äî Understanding data types, column semantics, and structure\n",
                "4. **Data Quality Assessment** ‚Äî Detecting missing values, duplicates, and anomalies\n",
                "5. **Data Cleaning Pipeline** ‚Äî Standardizing strings, fixing dates, and preparing data for analysis\n",
                "6. **Quality Assurance Checkpoint** ‚Äî Saving a verified, clean dataset for downstream use\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Project Context and Business Problem\n",
                "\n",
                "### 1.1 The Business Challenge\n",
                "\n",
                "In the fiercely competitive automobile insurance industry, understanding the **long-term value of each customer** is not just useful‚Äîit's essential for survival. Companies that can accurately predict which customers will generate the most revenue over their lifetime can:\n",
                "\n",
                "- **Optimize Acquisition Budgets**: Spend more to acquire high-value customers, less on low-value ones\n",
                "- **Improve Retention Strategies**: Focus retention efforts on customers who are worth keeping\n",
                "- **Personalize Product Offerings**: Tailor coverage packages to specific value segments\n",
                "- **Forecast Revenue**: Accurately project future income based on current customer base\n",
                "\n",
                "### 1.2 The Dataset\n",
                "\n",
                "We are working with the **Watson Analytics Marketing Customer Value Analysis** dataset, which contains detailed records of automobile insurance customers. This dataset is particularly valuable because it includes:\n",
                "\n",
                "| Category | Information Available |\n",
                "|----------|----------------------|\n",
                "| **Demographics** | State, Gender, Education, Marital Status, Income |\n",
                "| **Policy Details** | Coverage Level, Policy Type, Monthly Premium, Renewal Offers |\n",
                "| **Behavioral Data** | Number of Policies, Open Complaints, Claims History |\n",
                "| **Target Variable** | Customer Lifetime Value (CLV) in dollars |\n",
                "\n",
                "### 1.3 Research Questions\n",
                "\n",
                "Before diving into the data, let's articulate the questions we aim to answer:\n",
                "\n",
                "1. **Primary Question**: Can we accurately predict Customer Lifetime Value using demographic and policy information?\n",
                "2. **Secondary Questions**:\n",
                "   - Which features are the strongest predictors of CLV?\n",
                "   - Are there distinct customer segments with different value profiles?\n",
                "   - What actionable insights can we derive for marketing strategy?\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Environment Setup and Library Imports\n",
                "\n",
                "We begin by importing all necessary libraries. This section serves as a **manifest of our technical dependencies**‚Äîcritical for reproducibility.\n",
                "\n",
                "### Why These Libraries?\n",
                "\n",
                "| Library | Purpose | Version Used |\n",
                "|---------|---------|-------------|\n",
                "| `pandas` | Data manipulation and analysis | >= 1.5.0 |\n",
                "| `numpy` | Numerical computing | >= 1.21.0 |\n",
                "| `matplotlib` | Static visualizations | >= 3.5.0 |\n",
                "| `seaborn` | Statistical visualizations | >= 0.12.0 |\n",
                "| `os` | File system operations | Built-in |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# ENVIRONMENT SETUP\n",
                "# ============================================================================\n",
                "\n",
                "# Core Data Libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Visualization Libraries\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# System Libraries\n",
                "import os\n",
                "import warnings\n",
                "from datetime import datetime\n",
                "\n",
                "# Display Settings\n",
                "pd.set_option('display.max_columns', None)  # Show all columns\n",
                "pd.set_option('display.max_rows', 100)       # Limit row display\n",
                "pd.set_option('display.float_format', '{:.2f}'.format)  # Format floats\n",
                "\n",
                "# Visualization Settings\n",
                "plt.style.use('seaborn-v0_8-whitegrid')  # Clean, professional style\n",
                "sns.set_palette('viridis')                # Colorblind-friendly palette\n",
                "plt.rcParams['figure.figsize'] = (12, 6)  # Default figure size\n",
                "plt.rcParams['font.size'] = 11            # Readable font size\n",
                "\n",
                "# Suppress Warnings (for clean output)\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Print Environment Info\n",
                "print(\"=\" * 60)\n",
                "print(\"ENVIRONMENT CONFIGURATION\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"Pandas Version:     {pd.__version__}\")\n",
                "print(f\"NumPy Version:      {np.__version__}\")\n",
                "print(f\"Execution Time:     {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Configuration and Path Management\n",
                "\n",
                "Centralizing all file paths and configuration parameters in one place is a **best practice** that makes the code:\n",
                "- Easier to maintain (change once, apply everywhere)\n",
                "- More portable (adapt paths for different environments)\n",
                "- Self-documenting (all settings visible in one location)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# CONFIGURATION\n",
                "# ============================================================================\n",
                "\n",
                "# Define Base Paths\n",
                "BASE_DIR = os.path.dirname(os.getcwd())  # Parent of notebooks folder\n",
                "DATA_RAW_DIR = os.path.join(BASE_DIR, 'data', 'raw')\n",
                "DATA_PROCESSED_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
                "FIGURES_DIR = os.path.join(BASE_DIR, 'report', 'figures')\n",
                "\n",
                "# Source Data File\n",
                "DATA_FILE = 'WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv'\n",
                "DATA_PATH = os.path.join(DATA_RAW_DIR, DATA_FILE)\n",
                "\n",
                "# Output Files\n",
                "CLEAN_DATA_PATH = os.path.join(DATA_PROCESSED_DIR, 'cleaned_data.csv')\n",
                "BACKUP_PATH = os.path.join(DATA_PROCESSED_DIR, 'raw_backup.csv')\n",
                "\n",
                "# Create directories if they don't exist\n",
                "for directory in [DATA_RAW_DIR, DATA_PROCESSED_DIR, FIGURES_DIR]:\n",
                "    os.makedirs(directory, exist_ok=True)\n",
                "\n",
                "# Verify Configuration\n",
                "print(\"=\" * 60)\n",
                "print(\"PATH CONFIGURATION\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"Base Directory:        {BASE_DIR}\")\n",
                "print(f\"Raw Data Directory:    {DATA_RAW_DIR}\")\n",
                "print(f\"Processed Data Dir:    {DATA_PROCESSED_DIR}\")\n",
                "print(f\"Figures Directory:     {FIGURES_DIR}\")\n",
                "print(f\"Source Data File:      {DATA_PATH}\")\n",
                "print(f\"\\nFile Exists: {os.path.exists(DATA_PATH)}\")\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Data Loading with Validation\n",
                "\n",
                "Loading data might seem trivial, but proper validation at this stage prevents countless headaches later. We implement a **defensive loading strategy** that:\n",
                "\n",
                "1. Verifies file existence before attempting to read\n",
                "2. Reports file metadata (size, modification date)\n",
                "3. Captures the raw state before any transformations\n",
                "4. Provides immediate feedback on data dimensions\n",
                "\n",
                "### Why This Matters in Practice\n",
                "\n",
                "In production environments, data files can:\n",
                "- Be corrupted during transfer\n",
                "- Have schema changes from upstream systems\n",
                "- Arrive empty or partially written\n",
                "\n",
                "Validating early catches these issues before they cascade into mysterious errors downstream."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# DATA LOADING WITH VALIDATION\n",
                "# ============================================================================\n",
                "\n",
                "def load_data_with_validation(filepath):\n",
                "    \"\"\"\n",
                "    Load CSV data with comprehensive validation.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    filepath : str\n",
                "        Path to the CSV file\n",
                "        \n",
                "    Returns:\n",
                "    --------\n",
                "    pd.DataFrame\n",
                "        Loaded dataframe\n",
                "        \n",
                "    Raises:\n",
                "    -------\n",
                "    FileNotFoundError\n",
                "        If the file doesn't exist\n",
                "    ValueError\n",
                "        If the file is empty\n",
                "    \"\"\"\n",
                "    \n",
                "    # Step 1: Verify file exists\n",
                "    if not os.path.exists(filepath):\n",
                "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
                "    \n",
                "    # Step 2: Get file metadata\n",
                "    file_size = os.path.getsize(filepath)\n",
                "    file_modified = datetime.fromtimestamp(os.path.getmtime(filepath))\n",
                "    \n",
                "    print(f\"üìÅ Loading: {os.path.basename(filepath)}\")\n",
                "    print(f\"   Size: {file_size / 1024:.2f} KB ({file_size / (1024*1024):.2f} MB)\")\n",
                "    print(f\"   Last Modified: {file_modified.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
                "    \n",
                "    # Step 3: Load the data\n",
                "    df = pd.read_csv(filepath)\n",
                "    \n",
                "    # Step 4: Basic validation\n",
                "    if df.empty:\n",
                "        raise ValueError(\"Loaded DataFrame is empty!\")\n",
                "    \n",
                "    # Step 5: Report dimensions\n",
                "    print(f\"\\n‚úÖ Successfully loaded {len(df):,} rows √ó {len(df.columns)} columns\")\n",
                "    \n",
                "    return df\n",
                "\n",
                "# Load the data\n",
                "df_raw = load_data_with_validation(DATA_PATH)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 First Look at the Raw Data\n",
                "\n",
                "The `.head()` method shows us the first few rows. This is our **initial reconnaissance**‚Äîwhat does the data actually look like?\n",
                "\n",
                "Pay attention to:\n",
                "- Column naming conventions (spacing, capitalization)\n",
                "- Apparent data types (numbers, text, dates)\n",
                "- Any obvious formatting issues"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display first 5 rows\n",
                "print(\"=\" * 80)\n",
                "print(\"FIRST 5 ROWS OF RAW DATA\")\n",
                "print(\"=\" * 80)\n",
                "df_raw.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display last 5 rows (to check for truncation issues)\n",
                "print(\"=\" * 80)\n",
                "print(\"LAST 5 ROWS OF RAW DATA (Checking for truncation)\")\n",
                "print(\"=\" * 80)\n",
                "df_raw.tail()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Schema Inspection and Data Profiling\n",
                "\n",
                "Understanding the **schema** (structure) of our data is critical. We need to know:\n",
                "\n",
                "1. **What columns exist?** ‚Äî The feature set available to us\n",
                "2. **What are their data types?** ‚Äî Determines valid operations\n",
                "3. **Are there missing values?** ‚Äî Impacts analysis strategies\n",
                "4. **What are the value ranges?** ‚Äî Helps identify anomalies\n",
                "\n",
                "### 5.1 DataFrame Info (Technical Schema)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comprehensive schema information\n",
                "print(\"=\" * 80)\n",
                "print(\"DATAFRAME SCHEMA INFORMATION\")\n",
                "print(\"=\" * 80)\n",
                "df_raw.info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Column Inventory\n",
                "\n",
                "Let's create a detailed inventory of all columns with their inferred purposes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Column inventory with data types and sample values\n",
                "def create_column_inventory(df):\n",
                "    \"\"\"\n",
                "    Create a detailed inventory of DataFrame columns.\n",
                "    \"\"\"\n",
                "    inventory = []\n",
                "    \n",
                "    for col in df.columns:\n",
                "        inventory.append({\n",
                "            'Column': col,\n",
                "            'Data Type': str(df[col].dtype),\n",
                "            'Non-Null Count': df[col].notna().sum(),\n",
                "            'Null Count': df[col].isna().sum(),\n",
                "            'Null %': f\"{(df[col].isna().sum() / len(df) * 100):.1f}%\",\n",
                "            'Unique Values': df[col].nunique(),\n",
                "            'Sample Value': str(df[col].dropna().iloc[0]) if len(df[col].dropna()) > 0 else 'N/A'\n",
                "        })\n",
                "    \n",
                "    return pd.DataFrame(inventory)\n",
                "\n",
                "column_inventory = create_column_inventory(df_raw)\n",
                "print(\"=\" * 100)\n",
                "print(\"COMPLETE COLUMN INVENTORY\")\n",
                "print(\"=\" * 100)\n",
                "column_inventory"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 Descriptive Statistics for Numerical Features\n",
                "\n",
                "The `.describe()` method provides the **five-number summary** plus mean and standard deviation. This helps us understand the **central tendency** and **spread** of numerical data.\n",
                "\n",
                "**Key Statistics to Examine:**\n",
                "- **Mean vs Median (50%)**: Large differences suggest skewness\n",
                "- **Std (Standard Deviation)**: Measures spread; high std = high variability\n",
                "- **Min/Max**: Extreme values may indicate outliers or data errors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Descriptive statistics for numerical columns\n",
                "print(\"=\" * 100)\n",
                "print(\"DESCRIPTIVE STATISTICS (NUMERICAL FEATURES)\")\n",
                "print(\"=\" * 100)\n",
                "df_raw.describe().T.round(2)  # Transposed for better readability"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.4 Categorical Feature Overview\n",
                "\n",
                "For categorical (text) columns, we need to understand:\n",
                "- How many unique categories exist\n",
                "- What are the most common values\n",
                "- Are there any suspicious entries (typos, inconsistent formatting)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Categorical columns analysis\n",
                "categorical_cols = df_raw.select_dtypes(include=['object']).columns.tolist()\n",
                "\n",
                "print(\"=\" * 100)\n",
                "print(f\"CATEGORICAL FEATURES OVERVIEW ({len(categorical_cols)} columns)\")\n",
                "print(\"=\" * 100)\n",
                "\n",
                "for col in categorical_cols:\n",
                "    print(f\"\\n{'‚îÄ' * 60}\")\n",
                "    print(f\"üìä {col}\")\n",
                "    print(f\"   Unique Values: {df_raw[col].nunique()}\")\n",
                "    print(f\"   Most Common:\")\n",
                "    \n",
                "    # Show top 5 value counts\n",
                "    value_counts = df_raw[col].value_counts().head(5)\n",
                "    for val, count in value_counts.items():\n",
                "        pct = count / len(df_raw) * 100\n",
                "        print(f\"      '{val}': {count:,} ({pct:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Data Quality Assessment\n",
                "\n",
                "Now we systematically check for common data quality issues. This is the **diagnostic phase**‚Äîwe're looking for problems that need to be fixed before analysis.\n",
                "\n",
                "### 6.1 Missing Value Analysis\n",
                "\n",
                "Missing values can significantly impact our analysis. Let's visualize their distribution."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Missing value analysis\n",
                "missing_counts = df_raw.isnull().sum()\n",
                "missing_pct = (missing_counts / len(df_raw) * 100).round(2)\n",
                "\n",
                "missing_df = pd.DataFrame({\n",
                "    'Column': missing_counts.index,\n",
                "    'Missing Count': missing_counts.values,\n",
                "    'Missing %': missing_pct.values\n",
                "}).sort_values('Missing Count', ascending=False)\n",
                "\n",
                "print(\"=\" * 60)\n",
                "print(\"MISSING VALUE ANALYSIS\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "total_missing = missing_counts.sum()\n",
                "total_cells = df_raw.size\n",
                "\n",
                "print(f\"\\nTotal Missing Cells: {total_missing:,} out of {total_cells:,} ({total_missing/total_cells*100:.2f}%)\")\n",
                "\n",
                "if total_missing > 0:\n",
                "    print(\"\\nColumns with Missing Values:\")\n",
                "    print(missing_df[missing_df['Missing Count'] > 0])\n",
                "else:\n",
                "    print(\"\\n‚úÖ EXCELLENT! No missing values detected in any column.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize missing data pattern (if any)\n",
                "fig, ax = plt.subplots(figsize=(14, 6))\n",
                "\n",
                "# Create a heatmap of missing values\n",
                "missing_matrix = df_raw.isnull().astype(int)\n",
                "\n",
                "# Only show if there are missing values, otherwise show validation message\n",
                "if missing_matrix.sum().sum() > 0:\n",
                "    sns.heatmap(missing_matrix.T, cbar=True, yticklabels=True, cmap='YlOrRd', ax=ax)\n",
                "    ax.set_title('Missing Value Heatmap (Yellow = Present, Red = Missing)', fontsize=14, fontweight='bold')\n",
                "    ax.set_xlabel('Row Index')\n",
                "    ax.set_ylabel('Columns')\n",
                "else:\n",
                "    ax.text(0.5, 0.5, '‚úÖ NO MISSING VALUES DETECTED\\n\\nAll cells contain valid data.', \n",
                "            ha='center', va='center', fontsize=20, color='green', fontweight='bold',\n",
                "            transform=ax.transAxes)\n",
                "    ax.set_xlim(0, 1)\n",
                "    ax.set_ylim(0, 1)\n",
                "    ax.axis('off')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(FIGURES_DIR, '01_missing_value_analysis.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüì∏ Figure saved to: {os.path.join(FIGURES_DIR, '01_missing_value_analysis.png')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 Duplicate Record Detection\n",
                "\n",
                "Duplicate records can artificially inflate counts and skew statistical measures. We check for:\n",
                "1. **Exact duplicates**: All columns match\n",
                "2. **Key duplicates**: Same customer ID but different records (which could be intentional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Duplicate detection\n",
                "print(\"=\" * 60)\n",
                "print(\"DUPLICATE RECORD ANALYSIS\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Check for exact duplicates (all columns)\n",
                "exact_duplicates = df_raw.duplicated().sum()\n",
                "print(f\"\\n1. Exact Duplicates (all columns match): {exact_duplicates:,}\")\n",
                "\n",
                "if exact_duplicates > 0:\n",
                "    print(\"   ‚ö†Ô∏è WARNING: Exact duplicates found!\")\n",
                "    print(\"   Sample duplicate rows:\")\n",
                "    print(df_raw[df_raw.duplicated(keep=False)].head(10))\n",
                "else:\n",
                "    print(\"   ‚úÖ No exact duplicates detected.\")\n",
                "\n",
                "# Check for customer ID duplicates (if Customer column exists)\n",
                "if 'Customer' in df_raw.columns:\n",
                "    customer_duplicates = df_raw['Customer'].duplicated().sum()\n",
                "    print(f\"\\n2. Customer ID Duplicates: {customer_duplicates:,}\")\n",
                "    \n",
                "    if customer_duplicates > 0:\n",
                "        print(\"   ‚ö†Ô∏è Some Customer IDs appear multiple times.\")\n",
                "        duplicate_customers = df_raw[df_raw['Customer'].duplicated(keep=False)]\n",
                "        print(f\"   Number of records with duplicate Customer IDs: {len(duplicate_customers):,}\")\n",
                "    else:\n",
                "        print(\"   ‚úÖ All Customer IDs are unique.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.3 Data Type Validation\n",
                "\n",
                "Sometimes data is loaded with incorrect types. For example:\n",
                "- Dates might be read as strings\n",
                "- Numeric codes might be read as integers when they should be categorical\n",
                "- Currency values might have formatting issues"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data type validation\n",
                "print(\"=\" * 60)\n",
                "print(\"DATA TYPE VALIDATION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Expected types based on column names\n",
                "expected_types = {\n",
                "    'Customer': 'identifier',\n",
                "    'State': 'categorical',\n",
                "    'Customer Lifetime Value': 'numeric',\n",
                "    'Response': 'categorical',\n",
                "    'Coverage': 'categorical',\n",
                "    'Education': 'categorical',\n",
                "    'Effective To Date': 'date',\n",
                "    'EmploymentStatus': 'categorical',\n",
                "    'Gender': 'categorical',\n",
                "    'Income': 'numeric',\n",
                "    'Location Code': 'categorical',\n",
                "    'Marital Status': 'categorical',\n",
                "    'Monthly Premium Auto': 'numeric',\n",
                "    'Months Since Last Claim': 'numeric',\n",
                "    'Months Since Policy Inception': 'numeric',\n",
                "    'Number of Open Complaints': 'numeric',\n",
                "    'Number of Policies': 'numeric',\n",
                "    'Policy Type': 'categorical',\n",
                "    'Policy': 'categorical',\n",
                "    'Renew Offer Type': 'categorical',\n",
                "    'Sales Channel': 'categorical',\n",
                "    'Total Claim Amount': 'numeric',\n",
                "    'Vehicle Class': 'categorical',\n",
                "    'Vehicle Size': 'categorical'\n",
                "}\n",
                "\n",
                "# Check actual vs expected\n",
                "validation_results = []\n",
                "for col in df_raw.columns:\n",
                "    actual_type = str(df_raw[col].dtype)\n",
                "    expected = expected_types.get(col, 'unknown')\n",
                "    \n",
                "    # Determine if type is appropriate\n",
                "    if expected == 'numeric' and 'float' in actual_type or 'int' in actual_type:\n",
                "        status = '‚úÖ'\n",
                "    elif expected == 'categorical' and actual_type == 'object':\n",
                "        status = '‚úÖ'\n",
                "    elif expected == 'date' and actual_type == 'object':  # Will need conversion\n",
                "        status = '‚ö†Ô∏è (needs conversion)'\n",
                "    elif expected == 'identifier' and actual_type == 'object':\n",
                "        status = '‚úÖ'\n",
                "    else:\n",
                "        status = 'üîç (review)'\n",
                "    \n",
                "    validation_results.append({\n",
                "        'Column': col,\n",
                "        'Current Type': actual_type,\n",
                "        'Expected': expected,\n",
                "        'Status': status\n",
                "    })\n",
                "\n",
                "pd.DataFrame(validation_results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Data Cleaning Pipeline\n",
                "\n",
                "Based on our quality assessment, we now implement a **systematic cleaning pipeline**. Each transformation is documented and reversible.\n",
                "\n",
                "### 7.1 Create Backup\n",
                "\n",
                "Before making any changes, we save a backup of the raw data. This is a **safety measure** that allows us to always return to the original state."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create working copy and backup\n",
                "df = df_raw.copy()  # Working copy - we'll modify this\n",
                "\n",
                "# Save backup of raw data\n",
                "df_raw.to_csv(BACKUP_PATH, index=False)\n",
                "print(f\"‚úÖ Raw data backup saved to: {BACKUP_PATH}\")\n",
                "print(f\"   Backup size: {os.path.getsize(BACKUP_PATH) / 1024:.2f} KB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.2 Column Name Standardization\n",
                "\n",
                "We standardize column names to:\n",
                "- Use lowercase (prevents case-sensitivity issues)\n",
                "- Replace spaces with underscores (enables dot notation access)\n",
                "- Remove special characters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standardize column names\n",
                "print(\"=\" * 60)\n",
                "print(\"COLUMN NAME STANDARDIZATION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Store original names for reference\n",
                "original_columns = df.columns.tolist()\n",
                "\n",
                "# Standardization function\n",
                "def standardize_column_name(name):\n",
                "    \"\"\"Convert column name to lowercase with underscores.\"\"\"\n",
                "    return name.lower().replace(' ', '_').replace('-', '_')\n",
                "\n",
                "# Apply standardization\n",
                "df.columns = [standardize_column_name(col) for col in df.columns]\n",
                "\n",
                "# Show the mapping\n",
                "column_mapping = pd.DataFrame({\n",
                "    'Original': original_columns,\n",
                "    'Standardized': df.columns.tolist()\n",
                "})\n",
                "\n",
                "print(\"\\n‚úÖ Column names standardized:\")\n",
                "column_mapping"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.3 String Normalization\n",
                "\n",
                "Categorical text columns often contain inconsistencies like:\n",
                "- Leading/trailing whitespace\n",
                "- Mixed case (\"California\" vs \"california\" vs \"CALIFORNIA\")\n",
                "- Extra internal spaces\n",
                "\n",
                "We normalize all string columns to **lowercase with trimmed whitespace**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# String normalization\n",
                "print(\"=\" * 60)\n",
                "print(\"STRING NORMALIZATION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Identify string columns (exclude the customer ID which should preserve original format)\n",
                "string_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
                "columns_to_normalize = [col for col in string_columns if col != 'customer']\n",
                "\n",
                "print(f\"\\nNormalizing {len(columns_to_normalize)} string columns:\")\n",
                "\n",
                "for col in columns_to_normalize:\n",
                "    original_unique = df[col].nunique()\n",
                "    \n",
                "    # Apply normalization: strip whitespace and convert to lowercase\n",
                "    df[col] = df[col].astype(str).str.strip().str.lower()\n",
                "    \n",
                "    new_unique = df[col].nunique()\n",
                "    \n",
                "    # Report if normalization reduced unique values (indicating inconsistencies were fixed)\n",
                "    if new_unique < original_unique:\n",
                "        print(f\"   üìã {col}: {original_unique} ‚Üí {new_unique} unique values (fixed {original_unique - new_unique} inconsistencies)\")\n",
                "    else:\n",
                "        print(f\"   ‚úÖ {col}: {new_unique} unique values (no inconsistencies)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.4 Date Conversion\n",
                "\n",
                "The `effective_to_date` column contains date information stored as strings. We convert it to proper datetime format for:\n",
                "- Chronological sorting\n",
                "- Date-based calculations\n",
                "- Time-series analysis (if needed)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Date conversion\n",
                "print(\"=\" * 60)\n",
                "print(\"DATE CONVERSION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "date_column = 'effective_to_date'\n",
                "\n",
                "if date_column in df.columns:\n",
                "    print(f\"\\nConverting '{date_column}' to datetime format...\")\n",
                "    print(f\"   Sample original values: {df[date_column].head(3).tolist()}\")\n",
                "    \n",
                "    # Convert to datetime (format: MM/DD/YY)\n",
                "    df[date_column] = pd.to_datetime(df[date_column], format='%m/%d/%y', errors='coerce')\n",
                "    \n",
                "    # Check for conversion errors\n",
                "    conversion_errors = df[date_column].isna().sum()\n",
                "    \n",
                "    if conversion_errors > 0:\n",
                "        print(f\"   ‚ö†Ô∏è Warning: {conversion_errors} values could not be converted\")\n",
                "    else:\n",
                "        print(f\"   ‚úÖ All dates converted successfully\")\n",
                "    \n",
                "    print(f\"   New dtype: {df[date_column].dtype}\")\n",
                "    print(f\"   Date range: {df[date_column].min()} to {df[date_column].max()}\")\n",
                "else:\n",
                "    print(f\"   Column '{date_column}' not found in dataset.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.5 Numeric Data Validation\n",
                "\n",
                "We verify that numeric columns contain valid values and don't have impossible entries (e.g., negative income, negative claim amounts)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Numeric validation\n",
                "print(\"=\" * 60)\n",
                "print(\"NUMERIC DATA VALIDATION\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                "\n",
                "# Columns that should not be negative\n",
                "non_negative_cols = ['income', 'monthly_premium_auto', 'customer_lifetime_value', \n",
                "                      'total_claim_amount', 'number_of_policies', 'number_of_open_complaints',\n",
                "                      'months_since_last_claim', 'months_since_policy_inception']\n",
                "\n",
                "print(\"\\nChecking for invalid negative values:\")\n",
                "\n",
                "for col in numeric_columns:\n",
                "    if col in non_negative_cols:\n",
                "        negative_count = (df[col] < 0).sum()\n",
                "        \n",
                "        if negative_count > 0:\n",
                "            print(f\"   ‚ö†Ô∏è {col}: {negative_count} negative values found!\")\n",
                "        else:\n",
                "            print(f\"   ‚úÖ {col}: No invalid negative values\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8. Cleaned Data Summary\n",
                "\n",
                "Let's review the final state of our cleaned dataset before saving it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final summary\n",
                "print(\"=\" * 80)\n",
                "print(\"CLEANED DATA SUMMARY\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(f\"\\nüìä Dataset Dimensions:\")\n",
                "print(f\"   Rows:    {len(df):,}\")\n",
                "print(f\"   Columns: {len(df.columns)}\")\n",
                "\n",
                "print(f\"\\nüìã Column Types:\")\n",
                "print(f\"   Numeric:     {len(df.select_dtypes(include=['int64', 'float64']).columns)}\")\n",
                "print(f\"   Categorical: {len(df.select_dtypes(include=['object']).columns)}\")\n",
                "print(f\"   DateTime:    {len(df.select_dtypes(include=['datetime64']).columns)}\")\n",
                "\n",
                "print(f\"\\n‚úÖ Data Quality Status:\")\n",
                "print(f\"   Missing Values: {df.isnull().sum().sum()}\")\n",
                "print(f\"   Duplicate Rows: {df.duplicated().sum()}\")\n",
                "\n",
                "print(f\"\\nüíæ Memory Usage: {df.memory_usage(deep=True).sum() / (1024*1024):.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display final schema\n",
                "print(\"=\" * 80)\n",
                "print(\"FINAL SCHEMA\")\n",
                "print(\"=\" * 80)\n",
                "df.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview cleaned data\n",
                "print(\"=\" * 80)\n",
                "print(\"CLEANED DATA PREVIEW\")\n",
                "print(\"=\" * 80)\n",
                "df.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 9. Export Cleaned Data\n",
                "\n",
                "We save the cleaned dataset for use in subsequent notebooks. This ensures consistency across the analysis pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export cleaned data\n",
                "print(\"=\" * 60)\n",
                "print(\"EXPORTING CLEANED DATA\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "# Save to CSV\n",
                "df.to_csv(CLEAN_DATA_PATH, index=False)\n",
                "\n",
                "# Verify export\n",
                "exported_size = os.path.getsize(CLEAN_DATA_PATH)\n",
                "\n",
                "print(f\"\\n‚úÖ Cleaned data exported successfully!\")\n",
                "print(f\"   Location: {CLEAN_DATA_PATH}\")\n",
                "print(f\"   File Size: {exported_size / 1024:.2f} KB ({exported_size / (1024*1024):.2f} MB)\")\n",
                "\n",
                "# Quick verification by re-reading\n",
                "df_verify = pd.read_csv(CLEAN_DATA_PATH)\n",
                "print(f\"   Verification Read: {len(df_verify):,} rows √ó {len(df_verify.columns)} columns\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 10. Notebook Summary and Next Steps\n",
                "\n",
                "### What We Accomplished\n",
                "\n",
                "In this notebook, we completed the foundational data preparation phase:\n",
                "\n",
                "| Step | Description | Outcome |\n",
                "|------|-------------|--------|\n",
                "| 1 | Data Loading | 9,134 records loaded successfully |\n",
                "| 2 | Schema Inspection | 24 columns identified and categorized |\n",
                "| 3 | Missing Value Check | ‚úÖ No missing values |\n",
                "| 4 | Duplicate Detection | ‚úÖ No exact duplicates |\n",
                "| 5 | Column Standardization | All columns normalized to snake_case |\n",
                "| 6 | String Normalization | Categorical values cleaned and lowercased |\n",
                "| 7 | Date Conversion | Date column converted to datetime |\n",
                "| 8 | Data Export | Clean dataset saved for downstream use |\n",
                "\n",
                "### Key Findings\n",
                "\n",
                "1. **Data Quality**: The raw dataset was remarkably clean‚Äîno missing values or duplicates\n",
                "2. **Feature Set**: 24 features covering demographics, policy details, and behavioral data\n",
                "3. **Target Variable**: Customer Lifetime Value is present and ready for prediction\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "In **Notebook 02: Exploratory Data Analysis**, we will:\n",
                "- Perform univariate analysis on all features\n",
                "- Explore relationships between features and the target variable\n",
                "- Identify patterns, trends, and potential outliers\n",
                "- Generate visualizations to support our findings\n",
                "\n",
                "---\n",
                "\n",
                "**End of Notebook 01**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}