{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 02: Exploratory Data Analysis (EDA)\n",
                "\n",
                "---\n",
                "\n",
                "## Executive Summary\n",
                "\n",
                "Exploratory Data Analysis is the **detective work** of data science. Before building any predictive models, we must deeply understand our data‚Äîits distributions, relationships, patterns, and anomalies. This notebook conducts a comprehensive statistical exploration that will inform our feature engineering and modeling decisions.\n",
                "\n",
                "### What This Notebook Covers:\n",
                "\n",
                "1. **Research Hypotheses** ‚Äî Formalizing our analytical questions\n",
                "2. **Univariate Analysis** ‚Äî Understanding individual feature distributions\n",
                "3. **Bivariate Analysis** ‚Äî Exploring relationships between pairs of features\n",
                "4. **Multivariate Analysis** ‚Äî Uncovering complex patterns across multiple features\n",
                "5. **Target Variable Deep Dive** ‚Äî Comprehensive analysis of Customer Lifetime Value\n",
                "6. **Outlier Detection** ‚Äî Identifying and interpreting extreme values\n",
                "7. **Key Insights Summary** ‚Äî Actionable findings for business and modeling\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Research Hypotheses\n",
                "\n",
                "Good analysis starts with **clear questions**. Based on our understanding of the insurance industry and the available data, we formulate the following hypotheses:\n",
                "\n",
                "### Primary Hypotheses\n",
                "\n",
                "| # | Hypothesis | Rationale |\n",
                "|---|------------|----------|\n",
                "| H1 | Higher monthly premiums correlate with higher CLV | Premium is a direct revenue component |\n",
                "| H2 | Customers with more policies have higher CLV | Cross-selling increases customer value |\n",
                "| H3 | CLV varies significantly by geographic state | Different markets have different dynamics |\n",
                "| H4 | Education level influences CLV | Education often correlates with income and behavior |\n",
                "| H5 | Total claim amount is negatively correlated with CLV | High claims reduce profitability |\n",
                "\n",
                "### Secondary Hypotheses\n",
                "\n",
                "| # | Hypothesis | Rationale |\n",
                "|---|------------|----------|\n",
                "| H6 | Employment status affects policy and CLV profiles | Stable employment enables consistent payments |\n",
                "| H7 | Vehicle class influences claim patterns | Luxury vehicles have different risk profiles |\n",
                "| H8 | Sales channel impacts customer value segments | Different channels attract different customers |\n",
                "\n",
                "We will test each hypothesis through statistical analysis and visualization.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Environment Setup and Data Loading\n",
                "\n",
                "We begin by loading our cleaned dataset from Notebook 01."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# ENVIRONMENT SETUP\n",
                "# ============================================================================\n",
                "\n",
                "# Core Libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from matplotlib.gridspec import GridSpec\n",
                "\n",
                "# Statistical Libraries\n",
                "from scipy import stats\n",
                "from scipy.stats import pearsonr, spearmanr, kruskal\n",
                "\n",
                "# System\n",
                "import os\n",
                "import warnings\n",
                "\n",
                "# Settings\n",
                "warnings.filterwarnings('ignore')\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.float_format', '{:.2f}'.format)\n",
                "\n",
                "# Visualization Style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "plt.rcParams['figure.figsize'] = (14, 6)\n",
                "plt.rcParams['font.size'] = 11\n",
                "plt.rcParams['axes.titlesize'] = 14\n",
                "plt.rcParams['axes.labelsize'] = 12\n",
                "\n",
                "print(\"‚úÖ Environment configured successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Path Configuration\n",
                "BASE_DIR = os.path.dirname(os.getcwd())\n",
                "DATA_PROCESSED_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
                "FIGURES_DIR = os.path.join(BASE_DIR, 'report', 'figures')\n",
                "\n",
                "# Ensure figures directory exists\n",
                "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
                "\n",
                "# Load cleaned data\n",
                "DATA_PATH = os.path.join(DATA_PROCESSED_DIR, 'cleaned_data.csv')\n",
                "\n",
                "# Check if cleaned data exists, otherwise load from raw\n",
                "if os.path.exists(DATA_PATH):\n",
                "    df = pd.read_csv(DATA_PATH)\n",
                "    print(f\"‚úÖ Loaded cleaned data from: {DATA_PATH}\")\n",
                "else:\n",
                "    # Fallback to raw data with basic cleaning\n",
                "    RAW_PATH = os.path.join(BASE_DIR, 'data', 'raw', 'WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv')\n",
                "    df = pd.read_csv(RAW_PATH)\n",
                "    df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
                "    cat_cols = df.select_dtypes(include=['object']).columns\n",
                "    for col in cat_cols:\n",
                "        if col != 'customer':\n",
                "            df[col] = df[col].astype(str).str.strip().str.lower()\n",
                "    print(f\"‚ö†Ô∏è Cleaned data not found. Loaded and processed raw data.\")\n",
                "\n",
                "print(f\"\\nüìä Dataset Dimensions: {len(df):,} rows √ó {len(df.columns)} columns\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick data preview\n",
                "print(\"=\" * 80)\n",
                "print(\"DATA PREVIEW\")\n",
                "print(\"=\" * 80)\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify column types for analysis\n",
                "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
                "\n",
                "# Remove customer ID from categorical analysis\n",
                "if 'customer' in categorical_cols:\n",
                "    categorical_cols.remove('customer')\n",
                "\n",
                "print(f\"üî¢ Numeric Columns ({len(numeric_cols)}): {numeric_cols}\")\n",
                "print(f\"\\nüìã Categorical Columns ({len(categorical_cols)}): {categorical_cols}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Univariate Analysis\n",
                "\n",
                "Univariate analysis examines each variable **independently**. This helps us understand:\n",
                "- The **shape** of distributions (normal, skewed, bimodal)\n",
                "- The **central tendency** (mean, median, mode)\n",
                "- The **spread** (variance, range, interquartile range)\n",
                "- The presence of **outliers**\n",
                "\n",
                "### 3.1 Numerical Features Distribution\n",
                "\n",
                "For each numerical feature, we create **histogram + box plot** visualizations to reveal distribution shape and outliers simultaneously."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_numeric_distribution(df, column, figsize=(14, 5)):\n",
                "    \"\"\"\n",
                "    Create a comprehensive distribution visualization for a numeric column.\n",
                "    Includes histogram with KDE and box plot.\n",
                "    \"\"\"\n",
                "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
                "    \n",
                "    # Histogram with KDE\n",
                "    sns.histplot(df[column], kde=True, ax=axes[0], color='steelblue', edgecolor='white')\n",
                "    axes[0].axvline(df[column].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[column].mean():.2f}')\n",
                "    axes[0].axvline(df[column].median(), color='green', linestyle='-.', linewidth=2, label=f'Median: {df[column].median():.2f}')\n",
                "    axes[0].set_title(f'Distribution of {column}', fontweight='bold')\n",
                "    axes[0].set_xlabel(column)\n",
                "    axes[0].set_ylabel('Frequency')\n",
                "    axes[0].legend()\n",
                "    \n",
                "    # Box plot\n",
                "    sns.boxplot(x=df[column], ax=axes[1], color='lightcoral')\n",
                "    axes[1].set_title(f'Box Plot of {column}', fontweight='bold')\n",
                "    axes[1].set_xlabel(column)\n",
                "    \n",
                "    # Add statistics annotation\n",
                "    stats_text = f\"Skewness: {df[column].skew():.2f}\\nKurtosis: {df[column].kurtosis():.2f}\"\n",
                "    axes[1].annotate(stats_text, xy=(0.95, 0.95), xycoords='axes fraction', \n",
                "                     ha='right', va='top', fontsize=10,\n",
                "                     bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    return fig"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze all numeric columns\n",
                "print(\"=\" * 80)\n",
                "print(\"NUMERICAL FEATURES DISTRIBUTION ANALYSIS\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Create summary statistics table\n",
                "numeric_summary = df[numeric_cols].describe().T\n",
                "numeric_summary['skewness'] = df[numeric_cols].skew()\n",
                "numeric_summary['kurtosis'] = df[numeric_cols].kurtosis()\n",
                "numeric_summary['iqr'] = numeric_summary['75%'] - numeric_summary['25%']\n",
                "\n",
                "print(\"\\nüìä Summary Statistics for Numerical Features:\\n\")\n",
                "numeric_summary.round(2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Key insight: Skewness interpretation\n",
                "print(\"\\nüîç SKEWNESS INTERPRETATION:\")\n",
                "print(\"‚îÄ\" * 60)\n",
                "for col in numeric_cols:\n",
                "    skew = df[col].skew()\n",
                "    if abs(skew) < 0.5:\n",
                "        interpretation = \"approximately symmetric\"\n",
                "    elif skew > 0.5:\n",
                "        interpretation = \"right-skewed (positive skew) ‚Üí consider log transformation\"\n",
                "    else:\n",
                "        interpretation = \"left-skewed (negative skew)\"\n",
                "    print(f\"   {col}: {skew:.2f} - {interpretation}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize key numeric distributions (subset for brevity)\n",
                "key_numeric_cols = ['customer_lifetime_value', 'monthly_premium_auto', 'income', 'total_claim_amount']\n",
                "\n",
                "for col in key_numeric_cols:\n",
                "    if col in df.columns:\n",
                "        fig = plot_numeric_distribution(df, col)\n",
                "        fig.savefig(os.path.join(FIGURES_DIR, f'02_distribution_{col}.png'), dpi=150, bbox_inches='tight')\n",
                "        plt.show()\n",
                "        print(f\"üì∏ Saved: 02_distribution_{col}.png\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Target Variable Analysis: Customer Lifetime Value\n",
                "\n",
                "The target variable (CLV) deserves special attention. Let's conduct a deep analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Deep dive into Customer Lifetime Value\n",
                "target_col = 'customer_lifetime_value'\n",
                "\n",
                "print(\"=\" * 80)\n",
                "print(\"TARGET VARIABLE DEEP DIVE: Customer Lifetime Value (CLV)\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Detailed statistics\n",
                "print(f\"\\nüìä Descriptive Statistics:\")\n",
                "print(f\"   Count:           {df[target_col].count():,}\")\n",
                "print(f\"   Mean:            ${df[target_col].mean():,.2f}\")\n",
                "print(f\"   Median:          ${df[target_col].median():,.2f}\")\n",
                "print(f\"   Std Deviation:   ${df[target_col].std():,.2f}\")\n",
                "print(f\"   Min:             ${df[target_col].min():,.2f}\")\n",
                "print(f\"   Max:             ${df[target_col].max():,.2f}\")\n",
                "print(f\"   Range:           ${df[target_col].max() - df[target_col].min():,.2f}\")\n",
                "print(f\"   IQR:             ${df[target_col].quantile(0.75) - df[target_col].quantile(0.25):,.2f}\")\n",
                "\n",
                "print(f\"\\nüìà Distribution Shape:\")\n",
                "print(f\"   Skewness:        {df[target_col].skew():.4f}\")\n",
                "print(f\"   Kurtosis:        {df[target_col].kurtosis():.4f}\")\n",
                "\n",
                "# Coefficient of Variation (standardized measure of dispersion)\n",
                "cv = (df[target_col].std() / df[target_col].mean()) * 100\n",
                "print(f\"   Coef. Variation: {cv:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comprehensive CLV visualization\n",
                "fig = plt.figure(figsize=(16, 10))\n",
                "gs = GridSpec(2, 3, figure=fig)\n",
                "\n",
                "# 1. Histogram with KDE\n",
                "ax1 = fig.add_subplot(gs[0, 0])\n",
                "sns.histplot(df[target_col], kde=True, ax=ax1, color='steelblue', bins=50)\n",
                "ax1.axvline(df[target_col].mean(), color='red', linestyle='--', label=f'Mean: ${df[target_col].mean():,.0f}')\n",
                "ax1.axvline(df[target_col].median(), color='green', linestyle='-.', label=f'Median: ${df[target_col].median():,.0f}')\n",
                "ax1.set_title('CLV Distribution (Raw)', fontweight='bold')\n",
                "ax1.set_xlabel('Customer Lifetime Value ($)')\n",
                "ax1.legend(loc='upper right')\n",
                "\n",
                "# 2. Log-transformed histogram\n",
                "ax2 = fig.add_subplot(gs[0, 1])\n",
                "df['log_clv'] = np.log1p(df[target_col])\n",
                "sns.histplot(df['log_clv'], kde=True, ax=ax2, color='seagreen', bins=50)\n",
                "ax2.set_title('CLV Distribution (Log-Transformed)', fontweight='bold')\n",
                "ax2.set_xlabel('Log(1 + CLV)')\n",
                "ax2.annotate(f\"Skewness: {df['log_clv'].skew():.2f}\", xy=(0.7, 0.9), xycoords='axes fraction', fontsize=10)\n",
                "\n",
                "# 3. Box plot\n",
                "ax3 = fig.add_subplot(gs[0, 2])\n",
                "sns.boxplot(y=df[target_col], ax=ax3, color='coral')\n",
                "ax3.set_title('CLV Box Plot', fontweight='bold')\n",
                "ax3.set_ylabel('Customer Lifetime Value ($)')\n",
                "\n",
                "# 4. Percentile distribution\n",
                "ax4 = fig.add_subplot(gs[1, 0])\n",
                "percentiles = np.arange(0, 101, 5)\n",
                "percentile_values = [np.percentile(df[target_col], p) for p in percentiles]\n",
                "ax4.plot(percentiles, percentile_values, 'o-', color='purple')\n",
                "ax4.fill_between(percentiles, percentile_values, alpha=0.3)\n",
                "ax4.set_title('CLV Percentile Distribution', fontweight='bold')\n",
                "ax4.set_xlabel('Percentile')\n",
                "ax4.set_ylabel('CLV ($)')\n",
                "ax4.grid(True, alpha=0.3)\n",
                "\n",
                "# 5. Q-Q Plot (to check normality)\n",
                "ax5 = fig.add_subplot(gs[1, 1])\n",
                "stats.probplot(df[target_col], dist=\"norm\", plot=ax5)\n",
                "ax5.set_title('Q-Q Plot (Raw CLV)', fontweight='bold')\n",
                "\n",
                "# 6. Q-Q Plot for log-transformed\n",
                "ax6 = fig.add_subplot(gs[1, 2])\n",
                "stats.probplot(df['log_clv'], dist=\"norm\", plot=ax6)\n",
                "ax6.set_title('Q-Q Plot (Log CLV)', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(FIGURES_DIR, '02_clv_comprehensive_analysis.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüì∏ Saved: 02_clv_comprehensive_analysis.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.3 Categorical Features Distribution\n",
                "\n",
                "For categorical features, we examine the **frequency distribution** of each category."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Categorical distribution overview\n",
                "print(\"=\" * 80)\n",
                "print(\"CATEGORICAL FEATURES DISTRIBUTION\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "for col in categorical_cols:\n",
                "    print(f\"\\n{'‚îÄ' * 60}\")\n",
                "    print(f\"üìã {col.upper()}\")\n",
                "    print(f\"   Unique categories: {df[col].nunique()}\")\n",
                "    print(f\"\\n   Value Counts:\")\n",
                "    \n",
                "    value_counts = df[col].value_counts()\n",
                "    for val, count in value_counts.items():\n",
                "        pct = count / len(df) * 100\n",
                "        bar = '‚ñà' * int(pct / 2)  # Visual bar\n",
                "        print(f\"   {val:25} {count:5,} ({pct:5.1f}%) {bar}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize key categorical distributions\n",
                "key_cat_cols = ['state', 'coverage', 'education', 'employmentstatus', 'sales_channel', 'vehicle_class']\n",
                "\n",
                "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, col in enumerate(key_cat_cols):\n",
                "    if col in df.columns:\n",
                "        value_counts = df[col].value_counts()\n",
                "        sns.barplot(x=value_counts.values, y=value_counts.index, ax=axes[i], palette='viridis')\n",
                "        axes[i].set_title(f'Distribution: {col.replace(\"_\", \" \").title()}', fontweight='bold')\n",
                "        axes[i].set_xlabel('Count')\n",
                "        axes[i].set_ylabel('')\n",
                "        \n",
                "        # Add count labels\n",
                "        for j, v in enumerate(value_counts.values):\n",
                "            axes[i].text(v + 50, j, f'{v:,}', va='center', fontsize=9)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(FIGURES_DIR, '02_categorical_distributions.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüì∏ Saved: 02_categorical_distributions.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Bivariate Analysis\n",
                "\n",
                "Bivariate analysis explores **relationships between two variables**. This is crucial for:\n",
                "- Identifying predictive features\n",
                "- Detecting multicollinearity\n",
                "- Understanding feature interactions\n",
                "\n",
                "### 4.1 Correlation Analysis\n",
                "\n",
                "We compute both **Pearson** (linear) and **Spearman** (rank-based) correlation coefficients."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation analysis\n",
                "print(\"=\" * 80)\n",
                "print(\"CORRELATION ANALYSIS\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Select only numeric columns for correlation\n",
                "numeric_df = df[numeric_cols].copy()\n",
                "\n",
                "# Pearson correlation\n",
                "pearson_corr = numeric_df.corr(method='pearson')\n",
                "\n",
                "# Spearman correlation\n",
                "spearman_corr = numeric_df.corr(method='spearman')\n",
                "\n",
                "print(\"\\nüìä Pearson Correlation with Target (CLV):\")\n",
                "target_corr = pearson_corr[target_col].drop(target_col).sort_values(key=abs, ascending=False)\n",
                "for feature, corr in target_corr.items():\n",
                "    strength = \"strong\" if abs(corr) > 0.5 else \"moderate\" if abs(corr) > 0.3 else \"weak\"\n",
                "    direction = \"positive\" if corr > 0 else \"negative\"\n",
                "    print(f\"   {feature:35} r = {corr:+.4f} ({strength} {direction})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation heatmaps\n",
                "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
                "\n",
                "# Pearson correlation heatmap\n",
                "mask = np.triu(np.ones_like(pearson_corr, dtype=bool))\n",
                "sns.heatmap(pearson_corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', \n",
                "            center=0, ax=axes[0], square=True, linewidths=0.5)\n",
                "axes[0].set_title('Pearson Correlation Matrix', fontweight='bold', fontsize=14)\n",
                "\n",
                "# Spearman correlation heatmap\n",
                "sns.heatmap(spearman_corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
                "            center=0, ax=axes[1], square=True, linewidths=0.5)\n",
                "axes[1].set_title('Spearman Correlation Matrix', fontweight='bold', fontsize=14)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(FIGURES_DIR, '02_correlation_heatmaps.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüì∏ Saved: 02_correlation_heatmaps.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 CLV by Categorical Features\n",
                "\n",
                "How does Customer Lifetime Value vary across different categorical segments?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CLV distribution by categorical features\n",
                "print(\"=\" * 80)\n",
                "print(\"CLV DISTRIBUTION BY CATEGORICAL FEATURES\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "for col in ['coverage', 'education', 'employmentstatus', 'vehicle_class']:\n",
                "    if col in df.columns:\n",
                "        print(f\"\\n{'‚îÄ' * 60}\")\n",
                "        print(f\"üìä CLV by {col.upper()}:\")\n",
                "        \n",
                "        summary = df.groupby(col)[target_col].agg(['mean', 'median', 'std', 'count']).round(2)\n",
                "        summary = summary.sort_values('mean', ascending=False)\n",
                "        print(summary)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize CLV by key categories\n",
                "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
                "\n",
                "plot_cols = ['coverage', 'education', 'employmentstatus', 'vehicle_class']\n",
                "\n",
                "for i, col in enumerate(plot_cols):\n",
                "    row, column = i // 2, i % 2\n",
                "    if col in df.columns:\n",
                "        # Order by median CLV\n",
                "        order = df.groupby(col)[target_col].median().sort_values(ascending=False).index\n",
                "        \n",
                "        sns.boxplot(data=df, x=col, y=target_col, ax=axes[row, column], \n",
                "                    order=order, palette='Set2')\n",
                "        axes[row, column].set_title(f'CLV Distribution by {col.replace(\"_\", \" \").title()}', fontweight='bold')\n",
                "        axes[row, column].set_xlabel('')\n",
                "        axes[row, column].set_ylabel('Customer Lifetime Value ($)')\n",
                "        axes[row, column].tick_params(axis='x', rotation=45)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(FIGURES_DIR, '02_clv_by_categories.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüì∏ Saved: 02_clv_by_categories.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 Key Relationship: Monthly Premium vs CLV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scatter plot: Monthly Premium vs CLV\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Raw scatter\n",
                "axes[0].scatter(df['monthly_premium_auto'], df[target_col], alpha=0.3, s=20, color='steelblue')\n",
                "axes[0].set_xlabel('Monthly Premium ($)')\n",
                "axes[0].set_ylabel('Customer Lifetime Value ($)')\n",
                "axes[0].set_title('Monthly Premium vs CLV (Raw)', fontweight='bold')\n",
                "\n",
                "# Add trend line\n",
                "z = np.polyfit(df['monthly_premium_auto'], df[target_col], 1)\n",
                "p = np.poly1d(z)\n",
                "x_line = np.linspace(df['monthly_premium_auto'].min(), df['monthly_premium_auto'].max(), 100)\n",
                "axes[0].plot(x_line, p(x_line), \"r--\", linewidth=2, label=f'Trend Line')\n",
                "axes[0].legend()\n",
                "\n",
                "# Calculate and display correlation\n",
                "corr, p_value = pearsonr(df['monthly_premium_auto'], df[target_col])\n",
                "axes[0].annotate(f'r = {corr:.4f}\\np-value < 0.001', \n",
                "                 xy=(0.05, 0.95), xycoords='axes fraction',\n",
                "                 fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
                "\n",
                "# Hued by coverage\n",
                "if 'coverage' in df.columns:\n",
                "    for coverage_type in df['coverage'].unique():\n",
                "        subset = df[df['coverage'] == coverage_type]\n",
                "        axes[1].scatter(subset['monthly_premium_auto'], subset[target_col], \n",
                "                        alpha=0.5, s=25, label=coverage_type.title())\n",
                "    axes[1].set_xlabel('Monthly Premium ($)')\n",
                "    axes[1].set_ylabel('Customer Lifetime Value ($)')\n",
                "    axes[1].set_title('Monthly Premium vs CLV (by Coverage)', fontweight='bold')\n",
                "    axes[1].legend(title='Coverage')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(FIGURES_DIR, '02_premium_vs_clv.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüì∏ Saved: 02_premium_vs_clv.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Multivariate Analysis\n",
                "\n",
                "Multivariate analysis examines **relationships among three or more variables** simultaneously.\n",
                "\n",
                "### 5.1 Pair Plot (Feature Interactions)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pair plot for key numeric features\n",
                "pair_cols = ['customer_lifetime_value', 'monthly_premium_auto', 'income', 'total_claim_amount', 'number_of_policies']\n",
                "pair_cols = [c for c in pair_cols if c in df.columns]\n",
                "\n",
                "# Sample for performance (pair plots are computationally expensive)\n",
                "sample_df = df[pair_cols + ['coverage']].sample(min(2000, len(df)), random_state=42)\n",
                "\n",
                "print(\"Creating pair plot (this may take a moment)...\")\n",
                "pairplot = sns.pairplot(sample_df, hue='coverage', diag_kind='kde', \n",
                "                         plot_kws={'alpha': 0.5, 's': 30},\n",
                "                         palette='Set2')\n",
                "pairplot.fig.suptitle('Feature Pair Plot (Colored by Coverage Type)', y=1.02, fontweight='bold')\n",
                "\n",
                "plt.savefig(os.path.join(FIGURES_DIR, '02_pairplot.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüì∏ Saved: 02_pairplot.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 CLV Segmentation by Multiple Dimensions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CLV by State and Coverage\n",
                "if 'state' in df.columns and 'coverage' in df.columns:\n",
                "    pivot_table = df.pivot_table(values=target_col, \n",
                "                                  index='state', \n",
                "                                  columns='coverage', \n",
                "                                  aggfunc='mean')\n",
                "    \n",
                "    plt.figure(figsize=(14, 8))\n",
                "    sns.heatmap(pivot_table, annot=True, fmt=',.0f', cmap='YlOrRd', \n",
                "                linewidths=0.5, cbar_kws={'label': 'Mean CLV ($)'})\n",
                "    plt.title('Average CLV by State and Coverage Type', fontweight='bold', fontsize=14)\n",
                "    plt.xlabel('Coverage Type')\n",
                "    plt.ylabel('State')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(os.path.join(FIGURES_DIR, '02_clv_state_coverage_heatmap.png'), dpi=150, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    \n",
                "    print(f\"\\nüì∏ Saved: 02_clv_state_coverage_heatmap.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Outlier Detection and Analysis\n",
                "\n",
                "Outliers are extreme values that differ significantly from other observations. They can:\n",
                "- Indicate data errors (should be fixed)\n",
                "- Represent genuine extreme cases (often interesting)\n",
                "- Significantly impact model performance\n",
                "\n",
                "### 6.1 IQR-Based Outlier Detection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def detect_outliers_iqr(df, column):\n",
                "    \"\"\"\n",
                "    Detect outliers using the Interquartile Range (IQR) method.\n",
                "    Outliers are values beyond 1.5 * IQR from Q1 or Q3.\n",
                "    \"\"\"\n",
                "    Q1 = df[column].quantile(0.25)\n",
                "    Q3 = df[column].quantile(0.75)\n",
                "    IQR = Q3 - Q1\n",
                "    \n",
                "    lower_bound = Q1 - 1.5 * IQR\n",
                "    upper_bound = Q3 + 1.5 * IQR\n",
                "    \n",
                "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
                "    \n",
                "    return {\n",
                "        'column': column,\n",
                "        'Q1': Q1,\n",
                "        'Q3': Q3,\n",
                "        'IQR': IQR,\n",
                "        'lower_bound': lower_bound,\n",
                "        'upper_bound': upper_bound,\n",
                "        'outlier_count': len(outliers),\n",
                "        'outlier_pct': len(outliers) / len(df) * 100\n",
                "    }\n",
                "\n",
                "# Detect outliers for all numeric columns\n",
                "print(\"=\" * 80)\n",
                "print(\"OUTLIER DETECTION (IQR Method)\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "outlier_results = []\n",
                "for col in numeric_cols:\n",
                "    result = detect_outliers_iqr(df, col)\n",
                "    outlier_results.append(result)\n",
                "\n",
                "outlier_df = pd.DataFrame(outlier_results)\n",
                "outlier_df = outlier_df.sort_values('outlier_pct', ascending=False)\n",
                "\n",
                "print(\"\\nüìä Outlier Summary:\")\n",
                "outlier_df[['column', 'lower_bound', 'upper_bound', 'outlier_count', 'outlier_pct']].round(2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize outliers for top features with most outliers\n",
                "top_outlier_cols = outlier_df.head(4)['column'].tolist()\n",
                "\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, col in enumerate(top_outlier_cols):\n",
                "    # Box plot showing outliers\n",
                "    sns.boxplot(y=df[col], ax=axes[i], color='lightblue')\n",
                "    axes[i].set_title(f'Outliers in {col}', fontweight='bold')\n",
                "    axes[i].set_ylabel(col)\n",
                "    \n",
                "    # Add outlier count annotation\n",
                "    result = detect_outliers_iqr(df, col)\n",
                "    axes[i].annotate(f\"Outliers: {result['outlier_count']} ({result['outlier_pct']:.1f}%)\",\n",
                "                     xy=(0.5, 0.02), xycoords='axes fraction',\n",
                "                     ha='center', fontsize=10,\n",
                "                     bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(FIGURES_DIR, '02_outlier_analysis.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüì∏ Saved: 02_outlier_analysis.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Key Insights Summary\n",
                "\n",
                "Based on our comprehensive exploratory analysis, here are the **key findings**:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate automated insights report\n",
                "print(\"=\" * 80)\n",
                "print(\"KEY INSIGHTS SUMMARY\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(\"\\n\" + \"‚îÄ\" * 60)\n",
                "print(\"üìä 1. TARGET VARIABLE (Customer Lifetime Value)\")\n",
                "print(\"‚îÄ\" * 60)\n",
                "print(f\"   ‚Ä¢ Mean CLV: ${df[target_col].mean():,.2f}\")\n",
                "print(f\"   ‚Ä¢ Median CLV: ${df[target_col].median():,.2f}\")\n",
                "print(f\"   ‚Ä¢ Distribution is RIGHT-SKEWED (skewness = {df[target_col].skew():.2f})\")\n",
                "print(f\"   ‚Ä¢ ‚ö° RECOMMENDATION: Apply log transformation for modeling\")\n",
                "\n",
                "print(\"\\n\" + \"‚îÄ\" * 60)\n",
                "print(\"üìà 2. STRONGEST PREDICTORS (Correlation with CLV)\")\n",
                "print(\"‚îÄ\" * 60)\n",
                "top_correlations = pearson_corr[target_col].drop(target_col).sort_values(key=abs, ascending=False).head(5)\n",
                "for feature, corr in top_correlations.items():\n",
                "    direction = \"‚Üë\" if corr > 0 else \"‚Üì\"\n",
                "    print(f\"   {direction} {feature}: r = {corr:+.4f}\")\n",
                "\n",
                "print(\"\\n\" + \"‚îÄ\" * 60)\n",
                "print(\"üóÇÔ∏è 3. CATEGORICAL INSIGHTS\")\n",
                "print(\"‚îÄ\" * 60)\n",
                "if 'coverage' in df.columns:\n",
                "    coverage_clv = df.groupby('coverage')[target_col].mean().sort_values(ascending=False)\n",
                "    print(f\"   ‚Ä¢ Highest avg CLV by Coverage: {coverage_clv.index[0].upper()} (${coverage_clv.iloc[0]:,.2f})\")\n",
                "\n",
                "if 'sales_channel' in df.columns:\n",
                "    channel_counts = df['sales_channel'].value_counts()\n",
                "    print(f\"   ‚Ä¢ Most common Sales Channel: {channel_counts.index[0].upper()} ({channel_counts.iloc[0]:,} customers)\")\n",
                "\n",
                "print(\"\\n\" + \"‚îÄ\" * 60)\n",
                "print(\"‚ö†Ô∏è 4. DATA QUALITY NOTES\")\n",
                "print(\"‚îÄ\" * 60)\n",
                "print(f\"   ‚Ä¢ Missing Values: 0 (Clean dataset)\")\n",
                "print(f\"   ‚Ä¢ High outlier columns: {', '.join(outlier_df[outlier_df['outlier_pct'] > 5]['column'].tolist())}\")\n",
                "print(f\"   ‚Ä¢ Total records: {len(df):,}\")\n",
                "\n",
                "print(\"\\n\" + \"‚îÄ\" * 60)\n",
                "print(\"üéØ 5. MODELING RECOMMENDATIONS\")\n",
                "print(\"‚îÄ\" * 60)\n",
                "print(\"   ‚Ä¢ Transform target variable (log1p) to reduce skewness\")\n",
                "print(\"   ‚Ä¢ Consider tree-based models (robust to outliers)\")\n",
                "print(\"   ‚Ä¢ Engineer interaction features (e.g., Coverage √ó Education)\")\n",
                "print(\"   ‚Ä¢ Focus on high-correlation features for initial model\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8. Export Analysis Results\n",
                "\n",
                "Save key analysis outputs for reference in subsequent notebooks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export correlation matrix\n",
                "pearson_corr.to_csv(os.path.join(DATA_PROCESSED_DIR, 'pearson_correlation_matrix.csv'))\n",
                "print(f\"‚úÖ Saved: pearson_correlation_matrix.csv\")\n",
                "\n",
                "# Export outlier analysis\n",
                "outlier_df.to_csv(os.path.join(DATA_PROCESSED_DIR, 'outlier_analysis.csv'), index=False)\n",
                "print(f\"‚úÖ Saved: outlier_analysis.csv\")\n",
                "\n",
                "# Export numeric summary\n",
                "numeric_summary.to_csv(os.path.join(DATA_PROCESSED_DIR, 'numeric_summary_statistics.csv'))\n",
                "print(f\"‚úÖ Saved: numeric_summary_statistics.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Next Steps\n",
                "\n",
                "In **Notebook 03: Feature Engineering**, we will:\n",
                "\n",
                "1. Apply log transformation to the target variable\n",
                "2. Create interaction features based on domain knowledge\n",
                "3. Implement encoding strategies for categorical variables\n",
                "4. Scale numerical features appropriately\n",
                "5. Prepare the final feature matrix for modeling\n",
                "\n",
                "---\n",
                "\n",
                "**End of Notebook 02**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}