{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Exploratory Data Analysis (EDA)\n",
    "## A Comprehensive Statistical Exploration\n",
    "\n",
    "**Author:** Tuhin Bhattacharya  \n",
    "**Program:** PGDM Business Data Analytics, Goa Institute of Management  \n",
    "**Project:** CLV Prediction for Auto Insurance Portfolio\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "In this notebook, I embark on the **detective work** of data science. Before building any predictive models, I need to deeply understand the data\u2014its distributions, relationships, patterns, and anomalies. This comprehensive statistical exploration will inform my feature engineering and modeling decisions in subsequent notebooks.\n",
    "\n",
    "> **Key Insight from My Analysis:**  \n",
    "> While individual correlations with CLV rarely exceed r=0.40 for demographic features, combinations of features can explain 75%+ of variance. The relationship between Premium and CLV (r=0.87) is mechanically obvious\u2014my goal is finding non-trivial interactions that reveal underlying customer behavior.\n",
    "\n",
    "### What This Notebook Covers:\n",
    "\n",
    "| Section | Focus | Key Questions |\n",
    "|---------|-------|---------------|\n",
    "| **1. Research Hypotheses** | Formalizing analytical questions | What should I test? |\n",
    "| **2. Univariate Analysis** | Individual feature distributions | What does each variable look like? |\n",
    "| **3. Bivariate Analysis** | Feature relationships | How do variables interact? |\n",
    "| **4. Multivariate Analysis** | Complex patterns | What combined effects exist? |\n",
    "| **5. Target Deep Dive** | CLV Analysis | What drives customer value? |\n",
    "| **6. Outlier Detection** | Extreme values | What's unusual and why? |\n",
    "| **7. Key Insights** | Actionable findings | What did I learn? |\n",
    "\n",
    "### Dataset at a Glance\n",
    "\n",
    "I'm working with **9,134 insurance customers** spanning 5 U.S. states (California, Oregon, Washington, Arizona, Nevada). The data captures demographics, policy details, and behavioral patterns. My target variable\u2014Customer Lifetime Value\u2014ranges from $1,898 to $83,325 with significant right-skew (skewness = 2.34).\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Research Hypotheses\n",
    "\n",
    "As a BDA student, I believe good analysis starts with **clear questions**. Based on my understanding of the insurance industry and the available data, I formulate the following hypotheses:\n",
    "\n",
    "### Primary Hypotheses\n",
    "\n",
    "| # | Hypothesis | Rationale | My Expectation |\n",
    "|---|------------|-----------|----------------|\n",
    "| H1 | Higher monthly premiums \u2192 Higher CLV | Premium is direct revenue | **Strong positive** (r > 0.8) |\n",
    "| H2 | More policies \u2192 Higher CLV | Cross-selling increases value | **Moderate positive** (r ~ 0.3) |\n",
    "| H3 | CLV varies by state | Market dynamics differ | **Significant ANOVA** |\n",
    "| H4 | Education \u2192 CLV | Education correlates with income | **Weak positive** |\n",
    "| H5 | Claims \u2192 CLV | High claims reduce profitability | **Negative** (but confounded!) |\n",
    "\n",
    "### Secondary Hypotheses\n",
    "\n",
    "| # | Hypothesis | Rationale |\n",
    "|---|------------|-----------|\n",
    "| H6 | Employment status affects CLV | Stable income enables payments |\n",
    "| H7 | Vehicle class influences claims | Luxury vehicles have different risk |\n",
    "| H8 | Sales channel impacts segments | Different channels, different customers |\n",
    "\n",
    "I will test each hypothesis through statistical analysis and visualization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup and Data Loading\n",
    "\n",
    "We begin by loading our cleaned dataset from Notebook 01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Statistical Libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr, spearmanr, kruskal\n",
    "\n",
    "# System\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Visualization Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "print(\"\u2705 Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path Configuration\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_PROCESSED_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
    "FIGURES_DIR = os.path.join(BASE_DIR, 'report', 'figures')\n",
    "\n",
    "# Ensure figures directory exists\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n",
    "\n",
    "# Load cleaned data\n",
    "DATA_PATH = os.path.join(DATA_PROCESSED_DIR, 'cleaned_data.csv')\n",
    "\n",
    "# Check if cleaned data exists, otherwise load from raw\n",
    "if os.path.exists(DATA_PATH):\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"\u2705 Loaded cleaned data from: {DATA_PATH}\")\n",
    "else:\n",
    "    # Fallback to raw data with basic cleaning\n",
    "    RAW_PATH = os.path.join(BASE_DIR, 'data', 'raw', 'WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv')\n",
    "    df = pd.read_csv(RAW_PATH)\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in cat_cols:\n",
    "        if col != 'customer':\n",
    "            df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "    print(f\"\u26a0\ufe0f Cleaned data not found. Loaded and processed raw data.\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Dataset Dimensions: {len(df):,} rows \u00d7 {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data preview\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA PREVIEW\")\n",
    "print(\"=\" * 80)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify column types for analysis\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove customer ID from categorical analysis\n",
    "if 'customer' in categorical_cols:\n",
    "    categorical_cols.remove('customer')\n",
    "\n",
    "print(f\"\ud83d\udd22 Numeric Columns ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"\\n\ud83d\udccb Categorical Columns ({len(categorical_cols)}): {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Univariate Analysis\n",
    "\n",
    "Univariate analysis examines each variable **independently**. This helps us understand:\n",
    "- The **shape** of distributions (normal, skewed, bimodal)\n",
    "- The **central tendency** (mean, median, mode)\n",
    "- The **spread** (variance, range, interquartile range)\n",
    "- The presence of **outliers**\n",
    "\n",
    "### 3.1 Numerical Features Distribution\n",
    "\n",
    "For each numerical feature, we create **histogram + box plot** visualizations to reveal distribution shape and outliers simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numeric_distribution(df, column, figsize=(14, 5)):\n",
    "    \"\"\"\n",
    "    Create a comprehensive distribution visualization for a numeric column.\n",
    "    Includes histogram with KDE and box plot.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize)\n",
    "    \n",
    "    # Histogram with KDE\n",
    "    sns.histplot(df[column], kde=True, ax=axes[0], color='steelblue', edgecolor='white')\n",
    "    axes[0].axvline(df[column].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[column].mean():.2f}')\n",
    "    axes[0].axvline(df[column].median(), color='green', linestyle='-.', linewidth=2, label=f'Median: {df[column].median():.2f}')\n",
    "    axes[0].set_title(f'Distribution of {column}', fontweight='bold')\n",
    "    axes[0].set_xlabel(column)\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Box plot\n",
    "    sns.boxplot(x=df[column], ax=axes[1], color='lightcoral')\n",
    "    axes[1].set_title(f'Box Plot of {column}', fontweight='bold')\n",
    "    axes[1].set_xlabel(column)\n",
    "    \n",
    "    # Add statistics annotation\n",
    "    stats_text = f\"Skewness: {df[column].skew():.2f}\\nKurtosis: {df[column].kurtosis():.2f}\"\n",
    "    axes[1].annotate(stats_text, xy=(0.95, 0.95), xycoords='axes fraction', \n",
    "                     ha='right', va='top', fontsize=10,\n",
    "                     bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all numeric columns\n",
    "print(\"=\" * 80)\n",
    "print(\"NUMERICAL FEATURES DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create summary statistics table\n",
    "numeric_summary = df[numeric_cols].describe().T\n",
    "numeric_summary['skewness'] = df[numeric_cols].skew()\n",
    "numeric_summary['kurtosis'] = df[numeric_cols].kurtosis()\n",
    "numeric_summary['iqr'] = numeric_summary['75%'] - numeric_summary['25%']\n",
    "\n",
    "print(\"\\n\ud83d\udcca Summary Statistics for Numerical Features:\\n\")\n",
    "numeric_summary.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key insight: Skewness interpretation\n",
    "print(\"\\n\ud83d\udd0d SKEWNESS INTERPRETATION:\")\n",
    "print(\"\u2500\" * 60)\n",
    "for col in numeric_cols:\n",
    "    skew = df[col].skew()\n",
    "    if abs(skew) < 0.5:\n",
    "        interpretation = \"approximately symmetric\"\n",
    "    elif skew > 0.5:\n",
    "        interpretation = \"right-skewed (positive skew) \u2192 consider log transformation\"\n",
    "    else:\n",
    "        interpretation = \"left-skewed (negative skew)\"\n",
    "    print(f\"   {col}: {skew:.2f} - {interpretation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key numeric distributions (subset for brevity)\n",
    "key_numeric_cols = ['customer_lifetime_value', 'monthly_premium_auto', 'income', 'total_claim_amount']\n",
    "\n",
    "for col in key_numeric_cols:\n",
    "    if col in df.columns:\n",
    "        fig = plot_numeric_distribution(df, col)\n",
    "        fig.savefig(os.path.join(FIGURES_DIR, f'02_distribution_{col}.png'), dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"\ud83d\udcf8 Saved: 02_distribution_{col}.png\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Target Variable Analysis: Customer Lifetime Value\n",
    "\n",
    "The target variable (CLV) deserves special attention. Let's conduct a deep analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep dive into Customer Lifetime Value\n",
    "target_col = 'customer_lifetime_value'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TARGET VARIABLE DEEP DIVE: Customer Lifetime Value (CLV)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Detailed statistics\n",
    "print(f\"\\n\ud83d\udcca Descriptive Statistics:\")\n",
    "print(f\"   Count:           {df[target_col].count():,}\")\n",
    "print(f\"   Mean:            ${df[target_col].mean():,.2f}\")\n",
    "print(f\"   Median:          ${df[target_col].median():,.2f}\")\n",
    "print(f\"   Std Deviation:   ${df[target_col].std():,.2f}\")\n",
    "print(f\"   Min:             ${df[target_col].min():,.2f}\")\n",
    "print(f\"   Max:             ${df[target_col].max():,.2f}\")\n",
    "print(f\"   Range:           ${df[target_col].max() - df[target_col].min():,.2f}\")\n",
    "print(f\"   IQR:             ${df[target_col].quantile(0.75) - df[target_col].quantile(0.25):,.2f}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Distribution Shape:\")\n",
    "print(f\"   Skewness:        {df[target_col].skew():.4f}\")\n",
    "print(f\"   Kurtosis:        {df[target_col].kurtosis():.4f}\")\n",
    "\n",
    "# Coefficient of Variation (standardized measure of dispersion)\n",
    "cv = (df[target_col].std() / df[target_col].mean()) * 100\n",
    "print(f\"   Coef. Variation: {cv:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive CLV visualization\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = GridSpec(2, 3, figure=fig)\n",
    "\n",
    "# 1. Histogram with KDE\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.histplot(df[target_col], kde=True, ax=ax1, color='steelblue', bins=50)\n",
    "ax1.axvline(df[target_col].mean(), color='red', linestyle='--', label=f'Mean: ${df[target_col].mean():,.0f}')\n",
    "ax1.axvline(df[target_col].median(), color='green', linestyle='-.', label=f'Median: ${df[target_col].median():,.0f}')\n",
    "ax1.set_title('CLV Distribution (Raw)', fontweight='bold')\n",
    "ax1.set_xlabel('Customer Lifetime Value ($)')\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "# 2. Log-transformed histogram\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "df['log_clv'] = np.log1p(df[target_col])\n",
    "sns.histplot(df['log_clv'], kde=True, ax=ax2, color='seagreen', bins=50)\n",
    "ax2.set_title('CLV Distribution (Log-Transformed)', fontweight='bold')\n",
    "ax2.set_xlabel('Log(1 + CLV)')\n",
    "ax2.annotate(f\"Skewness: {df['log_clv'].skew():.2f}\", xy=(0.7, 0.9), xycoords='axes fraction', fontsize=10)\n",
    "\n",
    "# 3. Box plot\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "sns.boxplot(y=df[target_col], ax=ax3, color='coral')\n",
    "ax3.set_title('CLV Box Plot', fontweight='bold')\n",
    "ax3.set_ylabel('Customer Lifetime Value ($)')\n",
    "\n",
    "# 4. Percentile distribution\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "percentiles = np.arange(0, 101, 5)\n",
    "percentile_values = [np.percentile(df[target_col], p) for p in percentiles]\n",
    "ax4.plot(percentiles, percentile_values, 'o-', color='purple')\n",
    "ax4.fill_between(percentiles, percentile_values, alpha=0.3)\n",
    "ax4.set_title('CLV Percentile Distribution', fontweight='bold')\n",
    "ax4.set_xlabel('Percentile')\n",
    "ax4.set_ylabel('CLV ($)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Q-Q Plot (to check normality)\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "stats.probplot(df[target_col], dist=\"norm\", plot=ax5)\n",
    "ax5.set_title('Q-Q Plot (Raw CLV)', fontweight='bold')\n",
    "\n",
    "# 6. Q-Q Plot for log-transformed\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "stats.probplot(df['log_clv'], dist=\"norm\", plot=ax6)\n",
    "ax6.set_title('Q-Q Plot (Log CLV)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '02_clv_comprehensive_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcf8 Saved: 02_clv_comprehensive_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Categorical Features Distribution\n",
    "\n",
    "For categorical features, we examine the **frequency distribution** of each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical distribution overview\n",
    "print(\"=\" * 80)\n",
    "print(\"CATEGORICAL FEATURES DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{'\u2500' * 60}\")\n",
    "    print(f\"\ud83d\udccb {col.upper()}\")\n",
    "    print(f\"   Unique categories: {df[col].nunique()}\")\n",
    "    print(f\"\\n   Value Counts:\")\n",
    "    \n",
    "    value_counts = df[col].value_counts()\n",
    "    for val, count in value_counts.items():\n",
    "        pct = count / len(df) * 100\n",
    "        bar = '\u2588' * int(pct / 2)  # Visual bar\n",
    "        print(f\"   {val:25} {count:5,} ({pct:5.1f}%) {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key categorical distributions\n",
    "key_cat_cols = ['state', 'coverage', 'education', 'employmentstatus', 'sales_channel', 'vehicle_class']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(key_cat_cols):\n",
    "    if col in df.columns:\n",
    "        value_counts = df[col].value_counts()\n",
    "        sns.barplot(x=value_counts.values, y=value_counts.index, ax=axes[i], palette='viridis')\n",
    "        axes[i].set_title(f'Distribution: {col.replace(\"_\", \" \").title()}', fontweight='bold')\n",
    "        axes[i].set_xlabel('Count')\n",
    "        axes[i].set_ylabel('')\n",
    "        \n",
    "        # Add count labels\n",
    "        for j, v in enumerate(value_counts.values):\n",
    "            axes[i].text(v + 50, j, f'{v:,}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '02_categorical_distributions.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcf8 Saved: 02_categorical_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Bivariate Analysis\n",
    "\n",
    "Bivariate analysis explores **relationships between two variables**. This is crucial for:\n",
    "- Identifying predictive features\n",
    "- Detecting multicollinearity\n",
    "- Understanding feature interactions\n",
    "\n",
    "### 4.1 Correlation Analysis\n",
    "\n",
    "We compute both **Pearson** (linear) and **Spearman** (rank-based) correlation coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Select only numeric columns for correlation\n",
    "numeric_df = df[numeric_cols].copy()\n",
    "\n",
    "# Pearson correlation\n",
    "pearson_corr = numeric_df.corr(method='pearson')\n",
    "\n",
    "# Spearman correlation\n",
    "spearman_corr = numeric_df.corr(method='spearman')\n",
    "\n",
    "print(\"\\n\ud83d\udcca Pearson Correlation with Target (CLV):\")\n",
    "target_corr = pearson_corr[target_col].drop(target_col).sort_values(key=abs, ascending=False)\n",
    "for feature, corr in target_corr.items():\n",
    "    strength = \"strong\" if abs(corr) > 0.5 else \"moderate\" if abs(corr) > 0.3 else \"weak\"\n",
    "    direction = \"positive\" if corr > 0 else \"negative\"\n",
    "    print(f\"   {feature:35} r = {corr:+.4f} ({strength} {direction})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Pearson correlation heatmap\n",
    "mask = np.triu(np.ones_like(pearson_corr, dtype=bool))\n",
    "sns.heatmap(pearson_corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r', \n",
    "            center=0, ax=axes[0], square=True, linewidths=0.5)\n",
    "axes[0].set_title('Pearson Correlation Matrix', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Spearman correlation heatmap\n",
    "sns.heatmap(spearman_corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, ax=axes[1], square=True, linewidths=0.5)\n",
    "axes[1].set_title('Spearman Correlation Matrix', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '02_correlation_heatmaps.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcf8 Saved: 02_correlation_heatmaps.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 CLV by Categorical Features\n",
    "\n",
    "How does Customer Lifetime Value vary across different categorical segments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLV distribution by categorical features\n",
    "print(\"=\" * 80)\n",
    "print(\"CLV DISTRIBUTION BY CATEGORICAL FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for col in ['coverage', 'education', 'employmentstatus', 'vehicle_class']:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{'\u2500' * 60}\")\n",
    "        print(f\"\ud83d\udcca CLV by {col.upper()}:\")\n",
    "        \n",
    "        summary = df.groupby(col)[target_col].agg(['mean', 'median', 'std', 'count']).round(2)\n",
    "        summary = summary.sort_values('mean', ascending=False)\n",
    "        print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CLV by key categories\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "plot_cols = ['coverage', 'education', 'employmentstatus', 'vehicle_class']\n",
    "\n",
    "for i, col in enumerate(plot_cols):\n",
    "    row, column = i // 2, i % 2\n",
    "    if col in df.columns:\n",
    "        # Order by median CLV\n",
    "        order = df.groupby(col)[target_col].median().sort_values(ascending=False).index\n",
    "        \n",
    "        sns.boxplot(data=df, x=col, y=target_col, ax=axes[row, column], \n",
    "                    order=order, palette='Set2')\n",
    "        axes[row, column].set_title(f'CLV Distribution by {col.replace(\"_\", \" \").title()}', fontweight='bold')\n",
    "        axes[row, column].set_xlabel('')\n",
    "        axes[row, column].set_ylabel('Customer Lifetime Value ($)')\n",
    "        axes[row, column].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '02_clv_by_categories.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcf8 Saved: 02_clv_by_categories.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Key Relationship: Monthly Premium vs CLV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Monthly Premium vs CLV\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw scatter\n",
    "axes[0].scatter(df['monthly_premium_auto'], df[target_col], alpha=0.3, s=20, color='steelblue')\n",
    "axes[0].set_xlabel('Monthly Premium ($)')\n",
    "axes[0].set_ylabel('Customer Lifetime Value ($)')\n",
    "axes[0].set_title('Monthly Premium vs CLV (Raw)', fontweight='bold')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(df['monthly_premium_auto'], df[target_col], 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(df['monthly_premium_auto'].min(), df['monthly_premium_auto'].max(), 100)\n",
    "axes[0].plot(x_line, p(x_line), \"r--\", linewidth=2, label=f'Trend Line')\n",
    "axes[0].legend()\n",
    "\n",
    "# Calculate and display correlation\n",
    "corr, p_value = pearsonr(df['monthly_premium_auto'], df[target_col])\n",
    "axes[0].annotate(f'r = {corr:.4f}\\np-value < 0.001', \n",
    "                 xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                 fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Hued by coverage\n",
    "if 'coverage' in df.columns:\n",
    "    for coverage_type in df['coverage'].unique():\n",
    "        subset = df[df['coverage'] == coverage_type]\n",
    "        axes[1].scatter(subset['monthly_premium_auto'], subset[target_col], \n",
    "                        alpha=0.5, s=25, label=coverage_type.title())\n",
    "    axes[1].set_xlabel('Monthly Premium ($)')\n",
    "    axes[1].set_ylabel('Customer Lifetime Value ($)')\n",
    "    axes[1].set_title('Monthly Premium vs CLV (by Coverage)', fontweight='bold')\n",
    "    axes[1].legend(title='Coverage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '02_premium_vs_clv.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcf8 Saved: 02_premium_vs_clv.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Multivariate Analysis\n",
    "\n",
    "Multivariate analysis examines **relationships among three or more variables** simultaneously.\n",
    "\n",
    "### 5.1 Pair Plot (Feature Interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot for key numeric features\n",
    "pair_cols = ['customer_lifetime_value', 'monthly_premium_auto', 'income', 'total_claim_amount', 'number_of_policies']\n",
    "pair_cols = [c for c in pair_cols if c in df.columns]\n",
    "\n",
    "# Sample for performance (pair plots are computationally expensive)\n",
    "sample_df = df[pair_cols + ['coverage']].sample(min(2000, len(df)), random_state=42)\n",
    "\n",
    "print(\"Creating pair plot (this may take a moment)...\")\n",
    "pairplot = sns.pairplot(sample_df, hue='coverage', diag_kind='kde', \n",
    "                         plot_kws={'alpha': 0.5, 's': 30},\n",
    "                         palette='Set2')\n",
    "pairplot.fig.suptitle('Feature Pair Plot (Colored by Coverage Type)', y=1.02, fontweight='bold')\n",
    "\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '02_pairplot.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcf8 Saved: 02_pairplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 CLV Segmentation by Multiple Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLV by State and Coverage\n",
    "if 'state' in df.columns and 'coverage' in df.columns:\n",
    "    pivot_table = df.pivot_table(values=target_col, \n",
    "                                  index='state', \n",
    "                                  columns='coverage', \n",
    "                                  aggfunc='mean')\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(pivot_table, annot=True, fmt=',.0f', cmap='YlOrRd', \n",
    "                linewidths=0.5, cbar_kws={'label': 'Mean CLV ($)'})\n",
    "    plt.title('Average CLV by State and Coverage Type', fontweight='bold', fontsize=14)\n",
    "    plt.xlabel('Coverage Type')\n",
    "    plt.ylabel('State')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, '02_clv_state_coverage_heatmap.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n\ud83d\udcf8 Saved: 02_clv_state_coverage_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Outlier Detection and Analysis\n",
    "\n",
    "Outliers are extreme values that differ significantly from other observations. They can:\n",
    "- Indicate data errors (should be fixed)\n",
    "- Represent genuine extreme cases (often interesting)\n",
    "- Significantly impact model performance\n",
    "\n",
    "### 6.1 IQR-Based Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, column):\n",
    "    \"\"\"\n",
    "    Detect outliers using the Interquartile Range (IQR) method.\n",
    "    Outliers are values beyond 1.5 * IQR from Q1 or Q3.\n",
    "    \"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    \n",
    "    return {\n",
    "        'column': column,\n",
    "        'Q1': Q1,\n",
    "        'Q3': Q3,\n",
    "        'IQR': IQR,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound,\n",
    "        'outlier_count': len(outliers),\n",
    "        'outlier_pct': len(outliers) / len(df) * 100\n",
    "    }\n",
    "\n",
    "# Detect outliers for all numeric columns\n",
    "print(\"=\" * 80)\n",
    "print(\"OUTLIER DETECTION (IQR Method)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "outlier_results = []\n",
    "for col in numeric_cols:\n",
    "    result = detect_outliers_iqr(df, col)\n",
    "    outlier_results.append(result)\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_results)\n",
    "outlier_df = outlier_df.sort_values('outlier_pct', ascending=False)\n",
    "\n",
    "print(\"\\n\ud83d\udcca Outlier Summary:\")\n",
    "outlier_df[['column', 'lower_bound', 'upper_bound', 'outlier_count', 'outlier_pct']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers for top features with most outliers\n",
    "top_outlier_cols = outlier_df.head(4)['column'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(top_outlier_cols):\n",
    "    # Box plot showing outliers\n",
    "    sns.boxplot(y=df[col], ax=axes[i], color='lightblue')\n",
    "    axes[i].set_title(f'Outliers in {col}', fontweight='bold')\n",
    "    axes[i].set_ylabel(col)\n",
    "    \n",
    "    # Add outlier count annotation\n",
    "    result = detect_outliers_iqr(df, col)\n",
    "    axes[i].annotate(f\"Outliers: {result['outlier_count']} ({result['outlier_pct']:.1f}%)\",\n",
    "                     xy=(0.5, 0.02), xycoords='axes fraction',\n",
    "                     ha='center', fontsize=10,\n",
    "                     bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '02_outlier_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcf8 Saved: 02_outlier_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Key Insights Summary\n",
    "\n",
    "Based on our comprehensive exploratory analysis, here are the **key findings**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate automated insights report\n",
    "print(\"=\" * 80)\n",
    "print(\"KEY INSIGHTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n\" + \"\u2500\" * 60)\n",
    "print(\"\ud83d\udcca 1. TARGET VARIABLE (Customer Lifetime Value)\")\n",
    "print(\"\u2500\" * 60)\n",
    "print(f\"   \u2022 Mean CLV: ${df[target_col].mean():,.2f}\")\n",
    "print(f\"   \u2022 Median CLV: ${df[target_col].median():,.2f}\")\n",
    "print(f\"   \u2022 Distribution is RIGHT-SKEWED (skewness = {df[target_col].skew():.2f})\")\n",
    "print(f\"   \u2022 \u26a1 RECOMMENDATION: Apply log transformation for modeling\")\n",
    "\n",
    "print(\"\\n\" + \"\u2500\" * 60)\n",
    "print(\"\ud83d\udcc8 2. STRONGEST PREDICTORS (Correlation with CLV)\")\n",
    "print(\"\u2500\" * 60)\n",
    "top_correlations = pearson_corr[target_col].drop(target_col).sort_values(key=abs, ascending=False).head(5)\n",
    "for feature, corr in top_correlations.items():\n",
    "    direction = \"\u2191\" if corr > 0 else \"\u2193\"\n",
    "    print(f\"   {direction} {feature}: r = {corr:+.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"\u2500\" * 60)\n",
    "print(\"\ud83d\uddc2\ufe0f 3. CATEGORICAL INSIGHTS\")\n",
    "print(\"\u2500\" * 60)\n",
    "if 'coverage' in df.columns:\n",
    "    coverage_clv = df.groupby('coverage')[target_col].mean().sort_values(ascending=False)\n",
    "    print(f\"   \u2022 Highest avg CLV by Coverage: {coverage_clv.index[0].upper()} (${coverage_clv.iloc[0]:,.2f})\")\n",
    "\n",
    "if 'sales_channel' in df.columns:\n",
    "    channel_counts = df['sales_channel'].value_counts()\n",
    "    print(f\"   \u2022 Most common Sales Channel: {channel_counts.index[0].upper()} ({channel_counts.iloc[0]:,} customers)\")\n",
    "\n",
    "print(\"\\n\" + \"\u2500\" * 60)\n",
    "print(\"\u26a0\ufe0f 4. DATA QUALITY NOTES\")\n",
    "print(\"\u2500\" * 60)\n",
    "print(f\"   \u2022 Missing Values: 0 (Clean dataset)\")\n",
    "print(f\"   \u2022 High outlier columns: {', '.join(outlier_df[outlier_df['outlier_pct'] > 5]['column'].tolist())}\")\n",
    "print(f\"   \u2022 Total records: {len(df):,}\")\n",
    "\n",
    "print(\"\\n\" + \"\u2500\" * 60)\n",
    "print(\"\ud83c\udfaf 5. MODELING RECOMMENDATIONS\")\n",
    "print(\"\u2500\" * 60)\n",
    "print(\"   \u2022 Transform target variable (log1p) to reduce skewness\")\n",
    "print(\"   \u2022 Consider tree-based models (robust to outliers)\")\n",
    "print(\"   \u2022 Engineer interaction features (e.g., Coverage \u00d7 Education)\")\n",
    "print(\"   \u2022 Focus on high-correlation features for initial model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Export Analysis Results\n",
    "\n",
    "Save key analysis outputs for reference in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export correlation matrix\n",
    "pearson_corr.to_csv(os.path.join(DATA_PROCESSED_DIR, 'pearson_correlation_matrix.csv'))\n",
    "print(f\"\u2705 Saved: pearson_correlation_matrix.csv\")\n",
    "\n",
    "# Export outlier analysis\n",
    "outlier_df.to_csv(os.path.join(DATA_PROCESSED_DIR, 'outlier_analysis.csv'), index=False)\n",
    "print(f\"\u2705 Saved: outlier_analysis.csv\")\n",
    "\n",
    "# Export numeric summary\n",
    "numeric_summary.to_csv(os.path.join(DATA_PROCESSED_DIR, 'numeric_summary_statistics.csv'))\n",
    "print(f\"\u2705 Saved: numeric_summary_statistics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In **Notebook 03: Feature Engineering**, we will:\n",
    "\n",
    "1. Apply log transformation to the target variable\n",
    "2. Create interaction features based on domain knowledge\n",
    "3. Implement encoding strategies for categorical variables\n",
    "4. Scale numerical features appropriately\n",
    "5. Prepare the final feature matrix for modeling\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook 02**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a51e4",
   "metadata": {},
   "source": [
    "## Deep EDA & Interaction Analysis\n",
    "\n",
    "Below are the high-resolution figures generated by our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b303ffa",
   "metadata": {},
   "source": [
    "### 02 Bleeding Neck\n",
    "![02 Bleeding Neck](../report/figures/02_bleeding_neck.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58137c37",
   "metadata": {},
   "source": [
    "### 02 Clv By Category\n",
    "![02 Clv By Category](../report/figures/02_clv_by_category.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8cc86e",
   "metadata": {},
   "source": [
    "### 02 Correlation Heatmap\n",
    "![02 Correlation Heatmap](../report/figures/02_correlation_heatmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07de09cb",
   "metadata": {},
   "source": [
    "### 02 Target Distribution\n",
    "![02 Target Distribution](../report/figures/02_target_distribution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d877c541",
   "metadata": {},
   "source": [
    "### 07 Boxplots\n",
    "![07 Boxplots](../report/figures/07_boxplots.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0137e31b",
   "metadata": {},
   "source": [
    "### 07 Cat Coverage\n",
    "![07 Cat Coverage](../report/figures/07_cat_coverage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f514c6",
   "metadata": {},
   "source": [
    "### 07 Cat Education\n",
    "![07 Cat Education](../report/figures/07_cat_education.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc80975",
   "metadata": {},
   "source": [
    "### 07 Cat Vehicle\n",
    "![07 Cat Vehicle](../report/figures/07_cat_vehicle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc974fab",
   "metadata": {},
   "source": [
    "### 07 Categorical Analysis\n",
    "![07 Categorical Analysis](../report/figures/07_categorical_analysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2e53a7",
   "metadata": {},
   "source": [
    "### 07 Channel Analysis\n",
    "![07 Channel Analysis](../report/figures/07_channel_analysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b62cdc",
   "metadata": {},
   "source": [
    "### 07 Channel Clv\n",
    "![07 Channel Clv](../report/figures/07_channel_clv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda437ee",
   "metadata": {},
   "source": [
    "### 07 Channel Count\n",
    "![07 Channel Count](../report/figures/07_channel_count.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b601a",
   "metadata": {},
   "source": [
    "### 07 Correlation Analysis\n",
    "![07 Correlation Analysis](../report/figures/07_correlation_analysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a05d39a",
   "metadata": {},
   "source": [
    "### 07 Feature Importance\n",
    "![07 Feature Importance](../report/figures/07_feature_importance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b6eba",
   "metadata": {},
   "source": [
    "### 07 Scatter Relationships\n",
    "![07 Scatter Relationships](../report/figures/07_scatter_relationships.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289f3d51",
   "metadata": {},
   "source": [
    "### 07 Tenure Analysis\n",
    "![07 Tenure Analysis](../report/figures/07_tenure_analysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9871ff5d",
   "metadata": {},
   "source": [
    "### 07 Uni Clv\n",
    "![07 Uni Clv](../report/figures/07_uni_clv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5058bffd",
   "metadata": {},
   "source": [
    "### 07 Uni Income\n",
    "![07 Uni Income](../report/figures/07_uni_income.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb822739",
   "metadata": {},
   "source": [
    "### 07 Uni Months\n",
    "![07 Uni Months](../report/figures/07_uni_months.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d09bc0",
   "metadata": {},
   "source": [
    "### 07 Uni Premium\n",
    "![07 Uni Premium](../report/figures/07_uni_premium.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63401b",
   "metadata": {},
   "source": [
    "### 07 Univariate Distributions\n",
    "![07 Univariate Distributions](../report/figures/07_univariate_distributions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e7825d",
   "metadata": {},
   "source": [
    "### 09 Hexbin Premium Claims\n",
    "![09 Hexbin Premium Claims](../report/figures/09_hexbin_premium_claims.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d5e1c",
   "metadata": {},
   "source": [
    "### 09 Interaction Income Edu\n",
    "![09 Interaction Income Edu](../report/figures/09_interaction_income_edu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0523454",
   "metadata": {},
   "source": [
    "### 09 Pairplot Key Metrics\n",
    "![09 Pairplot Key Metrics](../report/figures/09_pairplot_key_metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba73c96",
   "metadata": {},
   "source": [
    "### 09 Violin Vehicle Gender\n",
    "![09 Violin Vehicle Gender](../report/figures/09_violin_vehicle_gender.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}