{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 03: Feature Engineering and Preprocessing\n",
                "\n",
                "---\n",
                "\n",
                "## Executive Summary\n",
                "\n",
                "Feature engineering is often called the **\"secret sauce\"** of machine learning. Raw data, however clean, rarely maximizes model performance. This notebook transforms our cleaned dataset into a **feature matrix optimized for predictive modeling**.\n",
                "\n",
                "### What This Notebook Covers:\n",
                "\n",
                "1. **Feature Selection Rationale** ‚Äî Choosing which features to include and why\n",
                "2. **Target Variable Transformation** ‚Äî Log transformation to address skewness\n",
                "3. **Feature Creation** ‚Äî Engineering new predictive features from existing data\n",
                "4. **Categorical Encoding** ‚Äî Converting text to numbers machine learning can use\n",
                "5. **Numerical Scaling** ‚Äî Standardizing feature ranges\n",
                "6. **Final Feature Matrix** ‚Äî Prepared data ready for modeling\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Environment Setup and Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# ENVIRONMENT SETUP\n",
                "# ============================================================================\n",
                "\n",
                "# Core Libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Scikit-learn Preprocessing\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Statistical Libraries\n",
                "from scipy import stats\n",
                "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
                "\n",
                "# System\n",
                "import os\n",
                "import warnings\n",
                "import joblib\n",
                "\n",
                "# Settings\n",
                "warnings.filterwarnings('ignore')\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.float_format', '{:.4f}'.format)\n",
                "\n",
                "# Visualization\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "\n",
                "# Random seed for reproducibility\n",
                "RANDOM_STATE = 42\n",
                "np.random.seed(RANDOM_STATE)\n",
                "\n",
                "print(\"‚úÖ Environment configured successfully\")\n",
                "print(f\"   Random State: {RANDOM_STATE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Path Configuration\n",
                "BASE_DIR = os.path.dirname(os.getcwd())\n",
                "DATA_PROCESSED_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
                "FIGURES_DIR = os.path.join(BASE_DIR, 'report', 'figures')\n",
                "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
                "\n",
                "# Ensure directories exist\n",
                "for directory in [DATA_PROCESSED_DIR, FIGURES_DIR, MODELS_DIR]:\n",
                "    os.makedirs(directory, exist_ok=True)\n",
                "\n",
                "# Load cleaned data\n",
                "DATA_PATH = os.path.join(DATA_PROCESSED_DIR, 'cleaned_data.csv')\n",
                "\n",
                "if os.path.exists(DATA_PATH):\n",
                "    df = pd.read_csv(DATA_PATH)\n",
                "    print(f\"‚úÖ Loaded cleaned data from: {DATA_PATH}\")\n",
                "else:\n",
                "    # Fallback\n",
                "    RAW_PATH = os.path.join(BASE_DIR, 'data', 'raw', 'WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv')\n",
                "    df = pd.read_csv(RAW_PATH)\n",
                "    df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
                "    print(f\"‚ö†Ô∏è Loaded raw data with basic cleaning.\")\n",
                "\n",
                "print(f\"\\nüìä Dataset: {len(df):,} rows √ó {len(df.columns)} columns\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Feature Selection Rationale\n",
                "\n",
                "Before engineering features, we must decide **which raw features to use**. Not all available features are appropriate for modeling.\n",
                "\n",
                "### 2.1 Feature Categories\n",
                "\n",
                "| Category | Include? | Reason |\n",
                "|----------|----------|--------|\n",
                "| **Identifier (Customer ID)** | ‚ùå No | Not predictive; would cause overfitting |\n",
                "| **Target Variable (CLV)** | Target | What we're predicting |\n",
                "| **Dates** | ‚ùå No | Need special handling; extract features instead |\n",
                "| **Demographics** | ‚úÖ Yes | Predictive of customer behavior |\n",
                "| **Policy Details** | ‚úÖ Yes | Core business features |\n",
                "| **Behavioral Data** | ‚úÖ Yes | Strong signals of customer value |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature categorization\n",
                "print(\"=\" * 80)\n",
                "print(\"FEATURE CATEGORIZATION\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Target variable\n",
                "TARGET = 'customer_lifetime_value'\n",
                "\n",
                "# Columns to exclude from features\n",
                "EXCLUDE_COLS = [\n",
                "    'customer',              # Identifier - not predictive\n",
                "    'customer_lifetime_value',  # Target variable\n",
                "    'effective_to_date',     # Date - needs special handling\n",
                "]\n",
                "\n",
                "# Get all feature columns\n",
                "all_cols = df.columns.tolist()\n",
                "feature_cols = [col for col in all_cols if col not in EXCLUDE_COLS]\n",
                "\n",
                "print(f\"\\nüìã Total Columns: {len(all_cols)}\")\n",
                "print(f\"üìã Excluded Columns: {EXCLUDE_COLS}\")\n",
                "print(f\"üìã Feature Columns: {len(feature_cols)}\")\n",
                "print(f\"\\nFeatures to use: {feature_cols}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Separate feature types\n",
                "numeric_features = df[feature_cols].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                "categorical_features = df[feature_cols].select_dtypes(include=['object']).columns.tolist()\n",
                "\n",
                "print(f\"\\nüî¢ Numerical Features ({len(numeric_features)}):\")\n",
                "for f in numeric_features:\n",
                "    print(f\"   ‚Ä¢ {f}\")\n",
                "\n",
                "print(f\"\\nüìã Categorical Features ({len(categorical_features)}):\")\n",
                "for f in categorical_features:\n",
                "    n_unique = df[f].nunique()\n",
                "    print(f\"   ‚Ä¢ {f} ({n_unique} categories)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Multicollinearity Check (VIF)\n",
                "\n",
                "**Variance Inflation Factor (VIF)** measures how much the variance of a regression coefficient is inflated due to collinearity. \n",
                "\n",
                "- VIF = 1: No correlation with other features\n",
                "- VIF > 5: Moderate multicollinearity (concerning)\n",
                "- VIF > 10: High multicollinearity (problematic)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate VIF for numerical features\n",
                "print(\"=\" * 80)\n",
                "print(\"MULTICOLLINEARITY CHECK (VIF)\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "def calculate_vif(df, features):\n",
                "    \"\"\"Calculate Variance Inflation Factor for each feature.\"\"\"\n",
                "    vif_data = pd.DataFrame()\n",
                "    vif_data['Feature'] = features\n",
                "    \n",
                "    # Handle missing values and create feature matrix\n",
                "    X = df[features].dropna()\n",
                "    \n",
                "    vif_values = []\n",
                "    for i in range(len(features)):\n",
                "        try:\n",
                "            vif = variance_inflation_factor(X.values, i)\n",
                "            vif_values.append(vif)\n",
                "        except:\n",
                "            vif_values.append(np.nan)\n",
                "    \n",
                "    vif_data['VIF'] = vif_values\n",
                "    vif_data['Interpretation'] = vif_data['VIF'].apply(\n",
                "        lambda x: 'Good' if x < 5 else ('Moderate' if x < 10 else 'High')\n",
                "    )\n",
                "    \n",
                "    return vif_data.sort_values('VIF', ascending=False)\n",
                "\n",
                "vif_results = calculate_vif(df, numeric_features)\n",
                "print(\"\\nüìä VIF Analysis Results:\")\n",
                "vif_results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Target Variable Transformation\n",
                "\n",
                "From our EDA, we identified that CLV is **right-skewed**. While tree-based models are robust to skewness, transformation often improves performance and interpretability.\n",
                "\n",
                "### Why Log Transformation?\n",
                "\n",
                "The `log1p` transformation ($\\ln(1+x)$) is preferred over `log` because:\n",
                "1. It handles zero values (log(0) is undefined, but log(1) = 0)\n",
                "2. It compresses the range of extreme values\n",
                "3. It makes the distribution more Gaussian-like"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target transformation\n",
                "print(\"=\" * 80)\n",
                "print(\"TARGET VARIABLE TRANSFORMATION\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Original distribution stats\n",
                "print(f\"\\nüìä Original CLV Statistics:\")\n",
                "print(f\"   Mean:     ${df[TARGET].mean():,.2f}\")\n",
                "print(f\"   Median:   ${df[TARGET].median():,.2f}\")\n",
                "print(f\"   Skewness: {df[TARGET].skew():.4f}\")\n",
                "print(f\"   Kurtosis: {df[TARGET].kurtosis():.4f}\")\n",
                "\n",
                "# Apply log1p transformation\n",
                "df['log_clv'] = np.log1p(df[TARGET])\n",
                "\n",
                "print(f\"\\nüìä Log-Transformed CLV Statistics:\")\n",
                "print(f\"   Mean:     {df['log_clv'].mean():.4f}\")\n",
                "print(f\"   Median:   {df['log_clv'].median():.4f}\")\n",
                "print(f\"   Skewness: {df['log_clv'].skew():.4f}\")\n",
                "print(f\"   Kurtosis: {df['log_clv'].kurtosis():.4f}\")\n",
                "\n",
                "print(f\"\\n‚úÖ Skewness reduced from {df[TARGET].skew():.2f} to {df['log_clv'].skew():.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize transformation effect\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Original\n",
                "sns.histplot(df[TARGET], kde=True, ax=axes[0], color='steelblue', bins=50)\n",
                "axes[0].set_title('Original CLV Distribution', fontweight='bold')\n",
                "axes[0].set_xlabel('Customer Lifetime Value ($)')\n",
                "axes[0].axvline(df[TARGET].mean(), color='red', linestyle='--', label=f'Mean')\n",
                "axes[0].axvline(df[TARGET].median(), color='green', linestyle='-.', label=f'Median')\n",
                "axes[0].legend()\n",
                "\n",
                "# Log-transformed\n",
                "sns.histplot(df['log_clv'], kde=True, ax=axes[1], color='seagreen', bins=50)\n",
                "axes[1].set_title('Log-Transformed CLV Distribution', fontweight='bold')\n",
                "axes[1].set_xlabel('log(1 + CLV)')\n",
                "axes[1].axvline(df['log_clv'].mean(), color='red', linestyle='--', label=f'Mean')\n",
                "axes[1].axvline(df['log_clv'].median(), color='green', linestyle='-.', label=f'Median')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(FIGURES_DIR, '03_target_transformation.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüì∏ Saved: 03_target_transformation.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Feature Engineering\n",
                "\n",
                "We create new features based on **domain knowledge** and **statistical relationships** identified during EDA.\n",
                "\n",
                "### 4.1 Interaction Features\n",
                "\n",
                "Interaction features capture the **combined effect** of two or more features that may not be captured by the individual features alone."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Engineering\n",
                "print(\"=\" * 80)\n",
                "print(\"FEATURE ENGINEERING\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# 1. Interaction Feature: Coverage √ó Education\n",
                "# Rationale: Different education levels may have different risk profiles per coverage type\n",
                "if 'coverage' in df.columns and 'education' in df.columns:\n",
                "    df['coverage_education'] = df['coverage'] + '_' + df['education']\n",
                "    print(f\"\\n‚úÖ Created: coverage_education (Interaction feature)\")\n",
                "    print(f\"   Unique combinations: {df['coverage_education'].nunique()}\")\n",
                "\n",
                "# 2. Insurance Loss Ratio (ILR)\n",
                "# Rationale: Claims relative to premium is a standard insurance metric\n",
                "if 'total_claim_amount' in df.columns and 'monthly_premium_auto' in df.columns:\n",
                "    # Avoid division by zero\n",
                "    df['insurance_loss_ratio'] = df['total_claim_amount'] / (df['monthly_premium_auto'] + 1)\n",
                "    print(f\"\\n‚úÖ Created: insurance_loss_ratio (Claims / Premium)\")\n",
                "    print(f\"   Mean ILR: {df['insurance_loss_ratio'].mean():.2f}\")\n",
                "\n",
                "# 3. Premium per Policy\n",
                "# Rationale: Average premium contribution per policy\n",
                "if 'monthly_premium_auto' in df.columns and 'number_of_policies' in df.columns:\n",
                "    df['premium_per_policy'] = df['monthly_premium_auto'] / (df['number_of_policies'] + 1)\n",
                "    print(f\"\\n‚úÖ Created: premium_per_policy (Premium / # Policies)\")\n",
                "\n",
                "# 4. Customer Engagement Score (Complaints + Response)\n",
                "if 'number_of_open_complaints' in df.columns:\n",
                "    # Lower is better (fewer complaints)\n",
                "    df['complaint_flag'] = (df['number_of_open_complaints'] > 0).astype(int)\n",
                "    print(f\"\\n‚úÖ Created: complaint_flag (Binary: has complaints)\")\n",
                "    print(f\"   Customers with complaints: {df['complaint_flag'].sum():,} ({df['complaint_flag'].mean()*100:.1f}%)\")\n",
                "\n",
                "# 5. Policy Tenure Category\n",
                "if 'months_since_policy_inception' in df.columns:\n",
                "    df['tenure_category'] = pd.cut(\n",
                "        df['months_since_policy_inception'],\n",
                "        bins=[0, 12, 36, 60, np.inf],\n",
                "        labels=['new', 'established', 'loyal', 'veteran']\n",
                "    )\n",
                "    print(f\"\\n‚úÖ Created: tenure_category (Binned tenure)\")\n",
                "    print(df['tenure_category'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary of engineered features\n",
                "engineered_features = ['coverage_education', 'insurance_loss_ratio', 'premium_per_policy', \n",
                "                        'complaint_flag', 'tenure_category']\n",
                "engineered_features = [f for f in engineered_features if f in df.columns]\n",
                "\n",
                "print(f\"\\nüìã Summary of Engineered Features:\")\n",
                "for f in engineered_features:\n",
                "    dtype = df[f].dtype\n",
                "    print(f\"   ‚Ä¢ {f}: {dtype}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Prepare Final Feature Set\n",
                "\n",
                "Now we define the final set of features to use for modeling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define final feature set\n",
                "print(\"=\" * 80)\n",
                "print(\"FINAL FEATURE SET DEFINITION\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Columns to drop from features\n",
                "drop_for_modeling = [\n",
                "    'customer',                    # Identifier\n",
                "    'customer_lifetime_value',     # Original target\n",
                "    'log_clv',                     # Will be our target\n",
                "    'effective_to_date',           # Date column\n",
                "    'policy',                      # Redundant with policy_type\n",
                "]\n",
                "\n",
                "# Get final features\n",
                "final_feature_cols = [col for col in df.columns if col not in drop_for_modeling]\n",
                "\n",
                "# Separate by type\n",
                "final_numeric = df[final_feature_cols].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                "final_categorical = df[final_feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()\n",
                "\n",
                "print(f\"\\nüìä Final Feature Count: {len(final_feature_cols)}\")\n",
                "print(f\"   Numerical: {len(final_numeric)}\")\n",
                "print(f\"   Categorical: {len(final_categorical)}\")\n",
                "\n",
                "print(f\"\\nüî¢ Numerical Features:\")\n",
                "for f in final_numeric:\n",
                "    print(f\"   ‚Ä¢ {f}\")\n",
                "\n",
                "print(f\"\\nüìã Categorical Features:\")\n",
                "for f in final_categorical:\n",
                "    print(f\"   ‚Ä¢ {f} ({df[f].nunique()} categories)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Train-Test Split\n",
                "\n",
                "Before preprocessing, we split the data to prevent **data leakage**. The preprocessing (scaling, encoding) must be fit only on training data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare X and y\n",
                "print(\"=\" * 80)\n",
                "print(\"TRAIN-TEST SPLIT\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Target\n",
                "y = df['log_clv']\n",
                "\n",
                "# Features\n",
                "X = df[final_feature_cols].copy()\n",
                "\n",
                "# Handle any remaining missing values in engineered features\n",
                "for col in X.columns:\n",
                "    if X[col].dtype in ['object', 'category']:\n",
                "        X[col] = X[col].fillna('unknown')\n",
                "    else:\n",
                "        X[col] = X[col].fillna(X[col].median())\n",
                "\n",
                "# Split: 80% train, 20% test\n",
                "TEST_SIZE = 0.2\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, \n",
                "    test_size=TEST_SIZE, \n",
                "    random_state=RANDOM_STATE\n",
                ")\n",
                "\n",
                "print(f\"\\nüìä Split Summary:\")\n",
                "print(f\"   Training Set: {len(X_train):,} samples ({100-TEST_SIZE*100:.0f}%)\")\n",
                "print(f\"   Test Set:     {len(X_test):,} samples ({TEST_SIZE*100:.0f}%)\")\n",
                "print(f\"\\n   Feature Dimensions: {X_train.shape[1]} features\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Preprocessing Pipeline\n",
                "\n",
                "We use scikit-learn's `ColumnTransformer` to apply different transformations to different column types:\n",
                "\n",
                "- **Numerical Features**: StandardScaler (z-score normalization)\n",
                "- **Categorical Features**: OneHotEncoder (binary dummies)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build preprocessing pipeline\n",
                "print(\"=\" * 80)\n",
                "print(\"PREPROCESSING PIPELINE\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Recalculate feature types from training data\n",
                "final_numeric = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                "final_categorical = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
                "\n",
                "print(f\"\\nüîß Building ColumnTransformer:\")\n",
                "print(f\"   Numerical ({len(final_numeric)}): StandardScaler\")\n",
                "print(f\"   Categorical ({len(final_categorical)}): OneHotEncoder\")\n",
                "\n",
                "# Create the preprocessor\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', StandardScaler(), final_numeric),\n",
                "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), final_categorical)\n",
                "    ],\n",
                "    remainder='passthrough'  # Keep any unspecified columns as-is\n",
                ")\n",
                "\n",
                "# Fit on training data only (to prevent data leakage)\n",
                "print(\"\\n‚è≥ Fitting preprocessor on training data...\")\n",
                "X_train_processed = preprocessor.fit_transform(X_train)\n",
                "X_test_processed = preprocessor.transform(X_test)\n",
                "\n",
                "print(f\"\\n‚úÖ Preprocessing Complete!\")\n",
                "print(f\"   Original features: {X_train.shape[1]}\")\n",
                "print(f\"   Transformed features: {X_train_processed.shape[1]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get feature names after transformation\n",
                "feature_names = preprocessor.get_feature_names_out()\n",
                "\n",
                "print(f\"\\nüìã Processed Feature Names (first 20):\")\n",
                "for i, name in enumerate(feature_names[:20]):\n",
                "    print(f\"   {i+1}. {name}\")\n",
                "if len(feature_names) > 20:\n",
                "    print(f\"   ... and {len(feature_names) - 20} more features\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert to DataFrames for easier inspection\n",
                "X_train_df = pd.DataFrame(X_train_processed, columns=feature_names)\n",
                "X_test_df = pd.DataFrame(X_test_processed, columns=feature_names)\n",
                "\n",
                "print(\"\\nüìä Processed Training Data Summary:\")\n",
                "X_train_df.describe().T.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8. Save Artifacts\n",
                "\n",
                "We save the preprocessed data and fitted preprocessor for use in modeling."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save processed data\n",
                "print(\"=\" * 80)\n",
                "print(\"SAVING ARTIFACTS\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Save processed feature matrices\n",
                "X_train_df.to_csv(os.path.join(DATA_PROCESSED_DIR, 'X_train_processed.csv'), index=False)\n",
                "X_test_df.to_csv(os.path.join(DATA_PROCESSED_DIR, 'X_test_processed.csv'), index=False)\n",
                "\n",
                "# Save target variables\n",
                "pd.Series(y_train).to_csv(os.path.join(DATA_PROCESSED_DIR, 'y_train.csv'), index=False)\n",
                "pd.Series(y_test).to_csv(os.path.join(DATA_PROCESSED_DIR, 'y_test.csv'), index=False)\n",
                "\n",
                "print(f\"\\n‚úÖ Saved processed data:\")\n",
                "print(f\"   ‚Ä¢ X_train_processed.csv ({X_train_df.shape[0]:,} √ó {X_train_df.shape[1]})\")\n",
                "print(f\"   ‚Ä¢ X_test_processed.csv ({X_test_df.shape[0]:,} √ó {X_test_df.shape[1]})\")\n",
                "print(f\"   ‚Ä¢ y_train.csv ({len(y_train):,} samples)\")\n",
                "print(f\"   ‚Ä¢ y_test.csv ({len(y_test):,} samples)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the fitted preprocessor\n",
                "preprocessor_path = os.path.join(MODELS_DIR, 'preprocessor.joblib')\n",
                "joblib.dump(preprocessor, preprocessor_path)\n",
                "\n",
                "print(f\"\\n‚úÖ Saved fitted preprocessor: {preprocessor_path}\")\n",
                "\n",
                "# Save feature names\n",
                "feature_names_path = os.path.join(MODELS_DIR, 'feature_names.txt')\n",
                "with open(feature_names_path, 'w') as f:\n",
                "    for name in feature_names:\n",
                "        f.write(f\"{name}\\n\")\n",
                "\n",
                "print(f\"‚úÖ Saved feature names: {feature_names_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save a complete model-ready dataset (for convenience)\n",
                "model_ready_df = pd.concat([\n",
                "    pd.DataFrame(X_train_processed, columns=feature_names),\n",
                "    pd.DataFrame({'log_clv': y_train.values})\n",
                "], axis=1)\n",
                "\n",
                "model_ready_path = os.path.join(DATA_PROCESSED_DIR, 'model_ready_data.csv')\n",
                "model_ready_df.to_csv(model_ready_path, index=False)\n",
                "\n",
                "print(f\"\\n‚úÖ Saved model-ready dataset: {model_ready_path}\")\n",
                "print(f\"   Shape: {model_ready_df.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 9. Summary\n",
                "\n",
                "### What We Accomplished\n",
                "\n",
                "| Step | Action | Outcome |\n",
                "|------|--------|---------|\n",
                "| 1 | Feature Selection | Identified 21 relevant features |\n",
                "| 2 | Multicollinearity Check | Verified no severe collinearity issues |\n",
                "| 3 | Target Transformation | Applied log1p to reduce skewness |\n",
                "| 4 | Feature Engineering | Created 5 new features |\n",
                "| 5 | Train-Test Split | 80/20 split with random state 42 |\n",
                "| 6 | Preprocessing | StandardScaler + OneHotEncoder pipeline |\n",
                "| 7 | Artifact Export | Saved all processed data and fitted pipeline |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final summary\n",
                "print(\"=\" * 80)\n",
                "print(\"FEATURE ENGINEERING SUMMARY\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(f\"\\nüìä Dataset Overview:\")\n",
                "print(f\"   Original records: {len(df):,}\")\n",
                "print(f\"   Training samples: {len(X_train):,}\")\n",
                "print(f\"   Test samples:     {len(X_test):,}\")\n",
                "\n",
                "print(f\"\\nüîß Feature Engineering:\")\n",
                "print(f\"   Original features:     {len(feature_cols)}\")\n",
                "print(f\"   Engineered features:   {len(engineered_features)}\")\n",
                "print(f\"   Final features (after encoding): {len(feature_names)}\")\n",
                "\n",
                "print(f\"\\nüìÅ Saved Artifacts:\")\n",
                "print(f\"   ‚Ä¢ Processed training data\")\n",
                "print(f\"   ‚Ä¢ Processed test data\")\n",
                "print(f\"   ‚Ä¢ Fitted preprocessor (for inference)\")\n",
                "print(f\"   ‚Ä¢ Feature names list\")\n",
                "print(f\"   ‚Ä¢ Model-ready dataset\")\n",
                "\n",
                "print(f\"\\n‚úÖ Ready for Notebook 04: Predictive Modeling!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Next Steps\n",
                "\n",
                "In **Notebook 04: Predictive Modeling**, we will:\n",
                "\n",
                "1. Establish a baseline model for comparison\n",
                "2. Train multiple algorithms (Linear Regression, Random Forest, Gradient Boosting)\n",
                "3. Perform hyperparameter tuning\n",
                "4. Evaluate models using multiple metrics\n",
                "5. Analyze feature importance\n",
                "6. Select the best model for deployment\n",
                "\n",
                "---\n",
                "\n",
                "**End of Notebook 03**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}