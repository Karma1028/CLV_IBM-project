{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Feature Engineering and Preprocessing\n",
    "## The Alchemy of Data Transformation\n",
    "\n",
    "**Author:** Tuhin Bhattacharya  \n",
    "**Program:** PGDM Business Data Analytics, Goa Institute of Management  \n",
    "**Project:** CLV Prediction for Auto Insurance Portfolio\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "Feature engineering is the **alchemy** of machine learning\u2014transforming raw data into gold. In this notebook, I apply domain knowledge and statistical techniques to create features that will power my CLV prediction model.\n",
    "\n",
    "> **My Philosophy:**  \n",
    "> Great features come from two sources: (1) domain expertise about what drives customer value, and (2) statistical insights from exploratory analysis. I combine both approaches in this notebook.\n",
    "\n",
    "### Transformation Pipeline\n",
    "\n",
    "| Step | Technique | Purpose |\n",
    "|------|-----------|--------|\n",
    "| **1. Target Transformation** | log1p | Reduce CLV skewness from 2.34 to ~0.15 |\n",
    "| **2. Feature Selection** | Domain + VIF | Remove irrelevant/collinear features |\n",
    "| **3. Categorical Encoding** | One-Hot + Target | Handle 5 categorical variables |\n",
    "| **4. Numerical Scaling** | StandardScaler | Normalize feature ranges |\n",
    "| **5. Interaction Features** | Premium \u00d7 Coverage, etc. | Capture combined effects |\n",
    "\n",
    "### Why Log Transformation Matters\n",
    "\n",
    "From my EDA, I found CLV has severe right-skew (2.34). The log1p transformation:\n",
    "\n",
    "```\n",
    "Before: Skewness = 2.34, Range = $1,898 to $83,325\n",
    "After:  Skewness = 0.15, Range = 7.55 to 11.33 (log scale)\n",
    "```\n",
    "\n",
    "This makes the target more Gaussian-like, improving linear model performance and stabilizing gradient-based optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Environment Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Statistical Libraries\n",
    "from scipy import stats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# System\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"\u2705 Environment configured successfully\")\n",
    "print(f\"   Random State: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path Configuration\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_PROCESSED_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
    "FIGURES_DIR = os.path.join(BASE_DIR, 'report', 'figures')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
    "\n",
    "# Ensure directories exist\n",
    "for directory in [DATA_PROCESSED_DIR, FIGURES_DIR, MODELS_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Load cleaned data\n",
    "DATA_PATH = os.path.join(DATA_PROCESSED_DIR, 'cleaned_data.csv')\n",
    "\n",
    "if os.path.exists(DATA_PATH):\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"\u2705 Loaded cleaned data from: {DATA_PATH}\")\n",
    "else:\n",
    "    # Fallback\n",
    "    RAW_PATH = os.path.join(BASE_DIR, 'data', 'raw', 'WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv')\n",
    "    df = pd.read_csv(RAW_PATH)\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "    print(f\"\u26a0\ufe0f Loaded raw data with basic cleaning.\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Dataset: {len(df):,} rows \u00d7 {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Feature Selection Rationale\n",
    "\n",
    "Before engineering features, we must decide **which raw features to use**. Not all available features are appropriate for modeling.\n",
    "\n",
    "### 2.1 Feature Categories\n",
    "\n",
    "| Category | Include? | Reason |\n",
    "|----------|----------|--------|\n",
    "| **Identifier (Customer ID)** | \u274c No | Not predictive; would cause overfitting |\n",
    "| **Target Variable (CLV)** | Target | What we're predicting |\n",
    "| **Dates** | \u274c No | Need special handling; extract features instead |\n",
    "| **Demographics** | \u2705 Yes | Predictive of customer behavior |\n",
    "| **Policy Details** | \u2705 Yes | Core business features |\n",
    "| **Behavioral Data** | \u2705 Yes | Strong signals of customer value |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature categorization\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE CATEGORIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Target variable\n",
    "TARGET = 'customer_lifetime_value'\n",
    "\n",
    "# Columns to exclude from features\n",
    "EXCLUDE_COLS = [\n",
    "    'customer',              # Identifier - not predictive\n",
    "    'customer_lifetime_value',  # Target variable\n",
    "    'effective_to_date',     # Date - needs special handling\n",
    "]\n",
    "\n",
    "# Get all feature columns\n",
    "all_cols = df.columns.tolist()\n",
    "feature_cols = [col for col in all_cols if col not in EXCLUDE_COLS]\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Total Columns: {len(all_cols)}\")\n",
    "print(f\"\ud83d\udccb Excluded Columns: {EXCLUDE_COLS}\")\n",
    "print(f\"\ud83d\udccb Feature Columns: {len(feature_cols)}\")\n",
    "print(f\"\\nFeatures to use: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate feature types\n",
    "numeric_features = df[feature_cols].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = df[feature_cols].select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\n\ud83d\udd22 Numerical Features ({len(numeric_features)}):\")\n",
    "for f in numeric_features:\n",
    "    print(f\"   \u2022 {f}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Categorical Features ({len(categorical_features)}):\")\n",
    "for f in categorical_features:\n",
    "    n_unique = df[f].nunique()\n",
    "    print(f\"   \u2022 {f} ({n_unique} categories)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multicollinearity Check (VIF)\n",
    "\n",
    "**Variance Inflation Factor (VIF)** measures how much the variance of a regression coefficient is inflated due to collinearity. \n",
    "\n",
    "- VIF = 1: No correlation with other features\n",
    "- VIF > 5: Moderate multicollinearity (concerning)\n",
    "- VIF > 10: High multicollinearity (problematic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VIF for numerical features\n",
    "print(\"=\" * 80)\n",
    "print(\"MULTICOLLINEARITY CHECK (VIF)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def calculate_vif(df, features):\n",
    "    \"\"\"Calculate Variance Inflation Factor for each feature.\"\"\"\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Feature'] = features\n",
    "    \n",
    "    # Handle missing values and create feature matrix\n",
    "    X = df[features].dropna()\n",
    "    \n",
    "    vif_values = []\n",
    "    for i in range(len(features)):\n",
    "        try:\n",
    "            vif = variance_inflation_factor(X.values, i)\n",
    "            vif_values.append(vif)\n",
    "        except:\n",
    "            vif_values.append(np.nan)\n",
    "    \n",
    "    vif_data['VIF'] = vif_values\n",
    "    vif_data['Interpretation'] = vif_data['VIF'].apply(\n",
    "        lambda x: 'Good' if x < 5 else ('Moderate' if x < 10 else 'High')\n",
    "    )\n",
    "    \n",
    "    return vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "vif_results = calculate_vif(df, numeric_features)\n",
    "print(\"\\n\ud83d\udcca VIF Analysis Results:\")\n",
    "vif_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Target Variable Transformation\n",
    "\n",
    "From our EDA, we identified that CLV is **right-skewed**. While tree-based models are robust to skewness, transformation often improves performance and interpretability.\n",
    "\n",
    "### Why Log Transformation?\n",
    "\n",
    "The `log1p` transformation ($\\ln(1+x)$) is preferred over `log` because:\n",
    "1. It handles zero values (log(0) is undefined, but log(1) = 0)\n",
    "2. It compresses the range of extreme values\n",
    "3. It makes the distribution more Gaussian-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target transformation\n",
    "print(\"=\" * 80)\n",
    "print(\"TARGET VARIABLE TRANSFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Original distribution stats\n",
    "print(f\"\\n\ud83d\udcca Original CLV Statistics:\")\n",
    "print(f\"   Mean:     ${df[TARGET].mean():,.2f}\")\n",
    "print(f\"   Median:   ${df[TARGET].median():,.2f}\")\n",
    "print(f\"   Skewness: {df[TARGET].skew():.4f}\")\n",
    "print(f\"   Kurtosis: {df[TARGET].kurtosis():.4f}\")\n",
    "\n",
    "# Apply log1p transformation\n",
    "df['log_clv'] = np.log1p(df[TARGET])\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Log-Transformed CLV Statistics:\")\n",
    "print(f\"   Mean:     {df['log_clv'].mean():.4f}\")\n",
    "print(f\"   Median:   {df['log_clv'].median():.4f}\")\n",
    "print(f\"   Skewness: {df['log_clv'].skew():.4f}\")\n",
    "print(f\"   Kurtosis: {df['log_clv'].kurtosis():.4f}\")\n",
    "\n",
    "print(f\"\\n\u2705 Skewness reduced from {df[TARGET].skew():.2f} to {df['log_clv'].skew():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transformation effect\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original\n",
    "sns.histplot(df[TARGET], kde=True, ax=axes[0], color='steelblue', bins=50)\n",
    "axes[0].set_title('Original CLV Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Customer Lifetime Value ($)')\n",
    "axes[0].axvline(df[TARGET].mean(), color='red', linestyle='--', label=f'Mean')\n",
    "axes[0].axvline(df[TARGET].median(), color='green', linestyle='-.', label=f'Median')\n",
    "axes[0].legend()\n",
    "\n",
    "# Log-transformed\n",
    "sns.histplot(df['log_clv'], kde=True, ax=axes[1], color='seagreen', bins=50)\n",
    "axes[1].set_title('Log-Transformed CLV Distribution', fontweight='bold')\n",
    "axes[1].set_xlabel('log(1 + CLV)')\n",
    "axes[1].axvline(df['log_clv'].mean(), color='red', linestyle='--', label=f'Mean')\n",
    "axes[1].axvline(df['log_clv'].median(), color='green', linestyle='-.', label=f'Median')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '03_target_transformation.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcf8 Saved: 03_target_transformation.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Feature Engineering\n",
    "\n",
    "We create new features based on **domain knowledge** and **statistical relationships** identified during EDA.\n",
    "\n",
    "### 4.1 Interaction Features\n",
    "\n",
    "Interaction features capture the **combined effect** of two or more features that may not be captured by the individual features alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Interaction Feature: Coverage \u00d7 Education\n",
    "# Rationale: Different education levels may have different risk profiles per coverage type\n",
    "if 'coverage' in df.columns and 'education' in df.columns:\n",
    "    df['coverage_education'] = df['coverage'] + '_' + df['education']\n",
    "    print(f\"\\n\u2705 Created: coverage_education (Interaction feature)\")\n",
    "    print(f\"   Unique combinations: {df['coverage_education'].nunique()}\")\n",
    "\n",
    "# 2. Insurance Loss Ratio (ILR)\n",
    "# Rationale: Claims relative to premium is a standard insurance metric\n",
    "if 'total_claim_amount' in df.columns and 'monthly_premium_auto' in df.columns:\n",
    "    # Avoid division by zero\n",
    "    df['insurance_loss_ratio'] = df['total_claim_amount'] / (df['monthly_premium_auto'] + 1)\n",
    "    print(f\"\\n\u2705 Created: insurance_loss_ratio (Claims / Premium)\")\n",
    "    print(f\"   Mean ILR: {df['insurance_loss_ratio'].mean():.2f}\")\n",
    "\n",
    "# 3. Premium per Policy\n",
    "# Rationale: Average premium contribution per policy\n",
    "if 'monthly_premium_auto' in df.columns and 'number_of_policies' in df.columns:\n",
    "    df['premium_per_policy'] = df['monthly_premium_auto'] / (df['number_of_policies'] + 1)\n",
    "    print(f\"\\n\u2705 Created: premium_per_policy (Premium / # Policies)\")\n",
    "\n",
    "# 4. Customer Engagement Score (Complaints + Response)\n",
    "if 'number_of_open_complaints' in df.columns:\n",
    "    # Lower is better (fewer complaints)\n",
    "    df['complaint_flag'] = (df['number_of_open_complaints'] > 0).astype(int)\n",
    "    print(f\"\\n\u2705 Created: complaint_flag (Binary: has complaints)\")\n",
    "    print(f\"   Customers with complaints: {df['complaint_flag'].sum():,} ({df['complaint_flag'].mean()*100:.1f}%)\")\n",
    "\n",
    "# 5. Policy Tenure Category\n",
    "if 'months_since_policy_inception' in df.columns:\n",
    "    df['tenure_category'] = pd.cut(\n",
    "        df['months_since_policy_inception'],\n",
    "        bins=[0, 12, 36, 60, np.inf],\n",
    "        labels=['new', 'established', 'loyal', 'veteran']\n",
    "    )\n",
    "    print(f\"\\n\u2705 Created: tenure_category (Binned tenure)\")\n",
    "    print(df['tenure_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of engineered features\n",
    "engineered_features = ['coverage_education', 'insurance_loss_ratio', 'premium_per_policy', \n",
    "                        'complaint_flag', 'tenure_category']\n",
    "engineered_features = [f for f in engineered_features if f in df.columns]\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Summary of Engineered Features:\")\n",
    "for f in engineered_features:\n",
    "    dtype = df[f].dtype\n",
    "    print(f\"   \u2022 {f}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Prepare Final Feature Set\n",
    "\n",
    "Now we define the final set of features to use for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define final feature set\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL FEATURE SET DEFINITION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Columns to drop from features\n",
    "drop_for_modeling = [\n",
    "    'customer',                    # Identifier\n",
    "    'customer_lifetime_value',     # Original target\n",
    "    'log_clv',                     # Will be our target\n",
    "    'effective_to_date',           # Date column\n",
    "    'policy',                      # Redundant with policy_type\n",
    "]\n",
    "\n",
    "# Get final features\n",
    "final_feature_cols = [col for col in df.columns if col not in drop_for_modeling]\n",
    "\n",
    "# Separate by type\n",
    "final_numeric = df[final_feature_cols].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "final_categorical = df[final_feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Final Feature Count: {len(final_feature_cols)}\")\n",
    "print(f\"   Numerical: {len(final_numeric)}\")\n",
    "print(f\"   Categorical: {len(final_categorical)}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udd22 Numerical Features:\")\n",
    "for f in final_numeric:\n",
    "    print(f\"   \u2022 {f}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Categorical Features:\")\n",
    "for f in final_categorical:\n",
    "    print(f\"   \u2022 {f} ({df[f].nunique()} categories)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Train-Test Split\n",
    "\n",
    "Before preprocessing, we split the data to prevent **data leakage**. The preprocessing (scaling, encoding) must be fit only on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAIN-TEST SPLIT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Target\n",
    "y = df['log_clv']\n",
    "\n",
    "# Features\n",
    "X = df[final_feature_cols].copy()\n",
    "\n",
    "# Handle any remaining missing values in engineered features\n",
    "for col in X.columns:\n",
    "    if X[col].dtype in ['object', 'category']:\n",
    "        X[col] = X[col].fillna('unknown')\n",
    "    else:\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "# Split: 80% train, 20% test\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Split Summary:\")\n",
    "print(f\"   Training Set: {len(X_train):,} samples ({100-TEST_SIZE*100:.0f}%)\")\n",
    "print(f\"   Test Set:     {len(X_test):,} samples ({TEST_SIZE*100:.0f}%)\")\n",
    "print(f\"\\n   Feature Dimensions: {X_train.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Preprocessing Pipeline\n",
    "\n",
    "We use scikit-learn's `ColumnTransformer` to apply different transformations to different column types:\n",
    "\n",
    "- **Numerical Features**: StandardScaler (z-score normalization)\n",
    "- **Categorical Features**: OneHotEncoder (binary dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build preprocessing pipeline\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESSING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Recalculate feature types from training data\n",
    "final_numeric = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "final_categorical = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"\\n\ud83d\udd27 Building ColumnTransformer:\")\n",
    "print(f\"   Numerical ({len(final_numeric)}): StandardScaler\")\n",
    "print(f\"   Categorical ({len(final_categorical)}): OneHotEncoder\")\n",
    "\n",
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), final_numeric),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), final_categorical)\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep any unspecified columns as-is\n",
    ")\n",
    "\n",
    "# Fit on training data only (to prevent data leakage)\n",
    "print(\"\\n\u23f3 Fitting preprocessor on training data...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"\\n\u2705 Preprocessing Complete!\")\n",
    "print(f\"   Original features: {X_train.shape[1]}\")\n",
    "print(f\"   Transformed features: {X_train_processed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after transformation\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Processed Feature Names (first 20):\")\n",
    "for i, name in enumerate(feature_names[:20]):\n",
    "    print(f\"   {i+1}. {name}\")\n",
    "if len(feature_names) > 20:\n",
    "    print(f\"   ... and {len(feature_names) - 20} more features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrames for easier inspection\n",
    "X_train_df = pd.DataFrame(X_train_processed, columns=feature_names)\n",
    "X_test_df = pd.DataFrame(X_test_processed, columns=feature_names)\n",
    "\n",
    "print(\"\\n\ud83d\udcca Processed Training Data Summary:\")\n",
    "X_train_df.describe().T.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Save Artifacts\n",
    "\n",
    "We save the preprocessed data and fitted preprocessor for use in modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING ARTIFACTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save processed feature matrices\n",
    "X_train_df.to_csv(os.path.join(DATA_PROCESSED_DIR, 'X_train_processed.csv'), index=False)\n",
    "X_test_df.to_csv(os.path.join(DATA_PROCESSED_DIR, 'X_test_processed.csv'), index=False)\n",
    "\n",
    "# Save target variables\n",
    "pd.Series(y_train).to_csv(os.path.join(DATA_PROCESSED_DIR, 'y_train.csv'), index=False)\n",
    "pd.Series(y_test).to_csv(os.path.join(DATA_PROCESSED_DIR, 'y_test.csv'), index=False)\n",
    "\n",
    "print(f\"\\n\u2705 Saved processed data:\")\n",
    "print(f\"   \u2022 X_train_processed.csv ({X_train_df.shape[0]:,} \u00d7 {X_train_df.shape[1]})\")\n",
    "print(f\"   \u2022 X_test_processed.csv ({X_test_df.shape[0]:,} \u00d7 {X_test_df.shape[1]})\")\n",
    "print(f\"   \u2022 y_train.csv ({len(y_train):,} samples)\")\n",
    "print(f\"   \u2022 y_test.csv ({len(y_test):,} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fitted preprocessor\n",
    "preprocessor_path = os.path.join(MODELS_DIR, 'preprocessor.joblib')\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "\n",
    "print(f\"\\n\u2705 Saved fitted preprocessor: {preprocessor_path}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names_path = os.path.join(MODELS_DIR, 'feature_names.txt')\n",
    "with open(feature_names_path, 'w') as f:\n",
    "    for name in feature_names:\n",
    "        f.write(f\"{name}\\n\")\n",
    "\n",
    "print(f\"\u2705 Saved feature names: {feature_names_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a complete model-ready dataset (for convenience)\n",
    "model_ready_df = pd.concat([\n",
    "    pd.DataFrame(X_train_processed, columns=feature_names),\n",
    "    pd.DataFrame({'log_clv': y_train.values})\n",
    "], axis=1)\n",
    "\n",
    "model_ready_path = os.path.join(DATA_PROCESSED_DIR, 'model_ready_data.csv')\n",
    "model_ready_df.to_csv(model_ready_path, index=False)\n",
    "\n",
    "print(f\"\\n\u2705 Saved model-ready dataset: {model_ready_path}\")\n",
    "print(f\"   Shape: {model_ready_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Summary\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "| Step | Action | Outcome |\n",
    "|------|--------|---------|\n",
    "| 1 | Feature Selection | Identified 21 relevant features |\n",
    "| 2 | Multicollinearity Check | Verified no severe collinearity issues |\n",
    "| 3 | Target Transformation | Applied log1p to reduce skewness |\n",
    "| 4 | Feature Engineering | Created 5 new features |\n",
    "| 5 | Train-Test Split | 80/20 split with random state 42 |\n",
    "| 6 | Preprocessing | StandardScaler + OneHotEncoder pipeline |\n",
    "| 7 | Artifact Export | Saved all processed data and fitted pipeline |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Dataset Overview:\")\n",
    "print(f\"   Original records: {len(df):,}\")\n",
    "print(f\"   Training samples: {len(X_train):,}\")\n",
    "print(f\"   Test samples:     {len(X_test):,}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udd27 Feature Engineering:\")\n",
    "print(f\"   Original features:     {len(feature_cols)}\")\n",
    "print(f\"   Engineered features:   {len(engineered_features)}\")\n",
    "print(f\"   Final features (after encoding): {len(feature_names)}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcc1 Saved Artifacts:\")\n",
    "print(f\"   \u2022 Processed training data\")\n",
    "print(f\"   \u2022 Processed test data\")\n",
    "print(f\"   \u2022 Fitted preprocessor (for inference)\")\n",
    "print(f\"   \u2022 Feature names list\")\n",
    "print(f\"   \u2022 Model-ready dataset\")\n",
    "\n",
    "print(f\"\\n\u2705 Ready for Notebook 04: Predictive Modeling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dca731",
   "metadata": {},
   "source": [
    "\n",
    "### 3.5 Advanced Transformation Comparison (Masterclass Update)\n",
    "In this section, we compare **Log Transformation** against **Yeo-Johnson (Power Transform)** to better handle zero-inflated data (like Income). We also compare **StandardScaler** vs **RobustScaler**.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644cf912",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler, RobustScaler, MinMaxScaler\n",
    "\n",
    "# 1. Income Transformation Comparison\n",
    "if 'income' in df.columns:\n",
    "    # Log\n",
    "    df['inc_log'] = np.log1p(df['income'])\n",
    "    \n",
    "    # Yeo-Johnson\n",
    "    pt = PowerTransformer(method='yeo-johnson')\n",
    "    df['inc_yj'] = pt.fit_transform(df[['income']])\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    sns.histplot(df['income'], ax=axes[0], color='gray').set_title('Original Income')\n",
    "    sns.histplot(df['inc_log'], ax=axes[1], color='blue').set_title('Log Transform')\n",
    "    sns.histplot(df['inc_yj'], ax=axes[2], color='green').set_title('Yeo-Johnson Transform')\n",
    "    plt.tight_layout()\n",
    "    plt.show() # Masterclass Insight: Yeo-Johnson handles zero-inflation better.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d4960",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Scaling Comparison (Monthly Premium Auto)\n",
    "if 'monthly_premium_auto' in df.columns:\n",
    "    data = df[['monthly_premium_auto']].values\n",
    "    \n",
    "    # Scalers\n",
    "    std = StandardScaler().fit_transform(data)\n",
    "    rob = RobustScaler().fit_transform(data)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    sns.boxplot(y=std, ax=axes[0], color='red').set_title('Standard Scaler (Sensitive to Outliers)')\n",
    "    sns.boxplot(y=rob, ax=axes[1], color='purple').set_title('Robust Scaler (IQR Based)')\n",
    "    plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In **Notebook 04: Predictive Modeling**, we will:\n",
    "\n",
    "1. Establish a baseline model for comparison\n",
    "2. Train multiple algorithms (Linear Regression, Random Forest, Gradient Boosting)\n",
    "3. Perform hyperparameter tuning\n",
    "4. Evaluate models using multiple metrics\n",
    "5. Analyze feature importance\n",
    "6. Select the best model for deployment\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook 03**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}