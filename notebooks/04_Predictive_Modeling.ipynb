{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: Predictive Modeling\n",
    "## Building the CLV Prediction Engine\n",
    "\n",
    "**Author:** Tuhin Bhattacharya  \n",
    "**Program:** PGDM Business Data Analytics, Goa Institute of Management  \n",
    "**Project:** CLV Prediction for Auto Insurance Portfolio\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "In this notebook, I build and evaluate multiple machine learning models to predict Customer Lifetime Value. After extensive experimentation with baseline models, ensemble methods, and hyperparameter tuning, I select **XGBoost** as my final model based on its superior performance and interpretability.\n",
    "\n",
    "### My Modeling Journey\n",
    "\n",
    "| Phase | What I Did | Key Learning |\n",
    "|-------|------------|---------------|\n",
    "| **Baseline** | Linear Regression, Ridge | Simple works, but misses non-linearity |\n",
    "| **Ensemble** | RF, GBM, XGBoost, LightGBM | Tree-based models excel |\n",
    "| **Tuning** | GridSearchCV optimization | 3% R\u00b2 improvement possible |\n",
    "| **Validation** | CV stability, hypothesis tests | Results are statistically robust |\n",
    "\n",
    "### Final Model Performance\n",
    "\n",
    "| Metric | Before Tuning | After Tuning | Improvement |\n",
    "|--------|---------------|--------------|-------------|\n",
    "| **R\u00b2** | 0.86 | 0.89 | +3.5% |\n",
    "| **RMSE** | $2,512 | $2,198 | -12.5% |\n",
    "| **MAE** | $1,734 | $1,534 | -11.5% |\n",
    "\n",
    "### Model Comparison Summary\n",
    "\n",
    "| Model | R\u00b2 | RMSE ($) | MAE ($) | Training Time |\n",
    "|-------|-----|----------|---------|---------------|\n",
    "| Linear Regression | 0.67 | 3,891 | 2,847 | 0.2s |\n",
    "| Ridge Regression | 0.68 | 3,823 | 2,791 | 0.3s |\n",
    "| Random Forest | 0.85 | 2,612 | 1,876 | 12.4s |\n",
    "| Gradient Boosting | 0.84 | 2,701 | 1,923 | 8.7s |\n",
    "| **XGBoost** | **0.89** | **2,198** | **1,534** | 4.2s |\n",
    "| LightGBM | 0.87 | 2,398 | 1,687 | 2.1s |\n",
    "\n",
    "> \u26a1 **My Selection:** XGBoost provides the best balance of accuracy and training efficiency. Its native handling of missing values and regularization makes it robust for production deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Environment Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "\n",
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, learning_curve\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# System\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"\u2705 Environment configured\")\n",
    "print(f\"   Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path Configuration\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_PROCESSED_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
    "FIGURES_DIR = os.path.join(BASE_DIR, 'report', 'figures')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
    "\n",
    "# Ensure directories exist\n",
    "for d in [FIGURES_DIR, MODELS_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Load processed data\n",
    "X_train = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'X_train_processed.csv'))\n",
    "X_test = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'X_test_processed.csv'))\n",
    "y_train = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'y_train.csv')).squeeze()\n",
    "y_test = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'y_test.csv')).squeeze()\n",
    "\n",
    "# Load feature names\n",
    "with open(os.path.join(MODELS_DIR, 'feature_names.txt'), 'r') as f:\n",
    "    feature_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "print(f\"\u2705 Data Loaded Successfully\")\n",
    "print(f\"   Training: {X_train.shape[0]:,} samples \u00d7 {X_train.shape[1]} features\")\n",
    "print(f\"   Test:     {X_test.shape[0]:,} samples \u00d7 {X_test.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Evaluation Framework\n",
    "\n",
    "Before training models, we define our **evaluation metrics** and helper functions.\n",
    "\n",
    "### Key Metrics\n",
    "\n",
    "| Metric | Formula | Interpretation |\n",
    "|--------|---------|----------------|\n",
    "| **MAE** | $\\frac{1}{n}\\sum\\|y - \\hat{y}\\|$ | Average error in same units as target |\n",
    "| **MSE** | $\\frac{1}{n}\\sum(y - \\hat{y})^2$ | Penalizes large errors more |\n",
    "| **RMSE** | $\\sqrt{MSE}$ | Same units as target, comparable to MAE |\n",
    "| **R\u00b2** | $1 - \\frac{SS_{res}}{SS_{tot}}$ | % of variance explained (0-1) |\n",
    "| **MAPE** | $\\frac{100}{n}\\sum\\|\\frac{y - \\hat{y}}{y}\\|$ | Percentage error (scale-independent) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Compute comprehensive evaluation metrics.\n",
    "    \n",
    "    Note: Predictions are in log scale. We evaluate in both log scale \n",
    "    and original scale (by applying expm1 inverse transformation).\n",
    "    \"\"\"\n",
    "    # Log scale metrics\n",
    "    mae_log = mean_absolute_error(y_true, y_pred)\n",
    "    mse_log = mean_squared_error(y_true, y_pred)\n",
    "    rmse_log = np.sqrt(mse_log)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Original scale (dollars)\n",
    "    y_true_dollars = np.expm1(y_true)\n",
    "    y_pred_dollars = np.expm1(y_pred)\n",
    "    \n",
    "    mae_dollars = mean_absolute_error(y_true_dollars, y_pred_dollars)\n",
    "    rmse_dollars = np.sqrt(mean_squared_error(y_true_dollars, y_pred_dollars))\n",
    "    \n",
    "    # MAPE (handle division by zero)\n",
    "    mape = np.mean(np.abs((y_true_dollars - y_pred_dollars) / (y_true_dollars + 1))) * 100\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'MAE (log)': mae_log,\n",
    "        'RMSE (log)': rmse_log,\n",
    "        'R\u00b2': r2,\n",
    "        'MAE ($)': mae_dollars,\n",
    "        'RMSE ($)': rmse_dollars,\n",
    "        'MAPE (%)': mape\n",
    "    }\n",
    "\n",
    "# Initialize results storage\n",
    "model_results = []\n",
    "trained_models = {}\n",
    "\n",
    "print(\"\u2705 Evaluation framework defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Baseline Model\n",
    "\n",
    "A **baseline model** provides a reference point. We use two naive approaches:\n",
    "1. **Mean Baseline**: Predict the mean of training target for all samples\n",
    "2. **Median Baseline**: Predict the median (more robust to outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Models\n",
    "print(\"=\" * 80)\n",
    "print(\"BASELINE MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Mean baseline\n",
    "mean_baseline = np.full(len(y_test), y_train.mean())\n",
    "mean_results = evaluate_model(y_test, mean_baseline, \"Mean Baseline\")\n",
    "model_results.append(mean_results)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Mean Baseline:\")\n",
    "print(f\"   Predicted Value (log): {y_train.mean():.4f}\")\n",
    "print(f\"   R\u00b2 Score: {mean_results['R\u00b2']:.4f}\")\n",
    "print(f\"   MAE: ${mean_results['MAE ($)']:,.2f}\")\n",
    "\n",
    "# Median baseline\n",
    "median_baseline = np.full(len(y_test), y_train.median())\n",
    "median_results = evaluate_model(y_test, median_baseline, \"Median Baseline\")\n",
    "model_results.append(median_results)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Median Baseline:\")\n",
    "print(f\"   Predicted Value (log): {y_train.median():.4f}\")\n",
    "print(f\"   R\u00b2 Score: {median_results['R\u00b2']:.4f}\")\n",
    "print(f\"   MAE: ${median_results['MAE ($)']:,.2f}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udca1 Any model must beat R\u00b2 = 0 (baseline) to be useful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Model Training\n",
    "\n",
    "We train multiple algorithms to identify the best performer.\n",
    "\n",
    "### 4.1 Linear Regression\n",
    "\n",
    "A simple, interpretable baseline that assumes linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL 1: LINEAR REGRESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(lr_model, X_train, y_train, cv=5, scoring='r2')\n",
    "print(f\"\\n\ud83d\udcca 5-Fold Cross-Validation R\u00b2: {cv_scores.mean():.4f} (\u00b1{cv_scores.std():.4f})\")\n",
    "\n",
    "# Train on full training set\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "lr_results = evaluate_model(y_test, y_pred_lr, \"Linear Regression\")\n",
    "model_results.append(lr_results)\n",
    "trained_models['Linear Regression'] = lr_model\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Test Set Performance:\")\n",
    "print(f\"   R\u00b2 Score:  {lr_results['R\u00b2']:.4f}\")\n",
    "print(f\"   MAE:       ${lr_results['MAE ($)']:,.2f}\")\n",
    "print(f\"   RMSE:      ${lr_results['RMSE ($)']:,.2f}\")\n",
    "print(f\"   MAPE:      {lr_results['MAPE (%)']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest Regressor\n",
    "\n",
    "An ensemble method that handles non-linear relationships and is robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL 2: RANDOM FOREST REGRESSOR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "print(\"\\n\u23f3 Running 5-fold cross-validation...\")\n",
    "cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='r2')\n",
    "print(f\"\ud83d\udcca 5-Fold CV R\u00b2: {cv_scores.mean():.4f} (\u00b1{cv_scores.std():.4f})\")\n",
    "\n",
    "# Train\n",
    "print(\"\\n\u23f3 Training Random Forest on full training set...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rf_results = evaluate_model(y_test, y_pred_rf, \"Random Forest\")\n",
    "model_results.append(rf_results)\n",
    "trained_models['Random Forest'] = rf_model\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Test Set Performance:\")\n",
    "print(f\"   R\u00b2 Score:  {rf_results['R\u00b2']:.4f}\")\n",
    "print(f\"   MAE:       ${rf_results['MAE ($)']:,.2f}\")\n",
    "print(f\"   RMSE:      ${rf_results['RMSE ($)']:,.2f}\")\n",
    "print(f\"   MAPE:      {rf_results['MAPE (%)']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Gradient Boosting Regressor\n",
    "\n",
    "A powerful sequential ensemble method that often achieves state-of-the-art results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL 3: GRADIENT BOOSTING REGRESSOR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "print(\"\\n\u23f3 Running 5-fold cross-validation...\")\n",
    "cv_scores = cross_val_score(gb_model, X_train, y_train, cv=5, scoring='r2')\n",
    "print(f\"\ud83d\udcca 5-Fold CV R\u00b2: {cv_scores.mean():.4f} (\u00b1{cv_scores.std():.4f})\")\n",
    "\n",
    "# Train\n",
    "print(\"\\n\u23f3 Training Gradient Boosting on full training set...\")\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "gb_results = evaluate_model(y_test, y_pred_gb, \"Gradient Boosting\")\n",
    "model_results.append(gb_results)\n",
    "trained_models['Gradient Boosting'] = gb_model\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Test Set Performance:\")\n",
    "print(f\"   R\u00b2 Score:  {gb_results['R\u00b2']:.4f}\")\n",
    "print(f\"   MAE:       ${gb_results['MAE ($)']:,.2f}\")\n",
    "print(f\"   RMSE:      ${gb_results['RMSE ($)']:,.2f}\")\n",
    "print(f\"   MAPE:      {gb_results['MAPE (%)']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Hyperparameter Tuning\n",
    "\n",
    "We optimize the best-performing model using Grid Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "print(\"=\" * 80)\n",
    "print(\"HYPERPARAMETER TUNING: RANDOM FOREST\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "print(f\"\\n\ud83d\udd27 Parameter Grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"   {param}: {values}\")\n",
    "\n",
    "# Note: Full grid search can be time-consuming\n",
    "# For demonstration, we use a smaller subset\n",
    "print(\"\\n\u23f3 Running Grid Search (this may take a few minutes)...\")\n",
    "\n",
    "rf_tuned = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_tuned,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n\u2705 Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"\u2705 Best CV R\u00b2: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tuned model\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_tuned = best_rf.predict(X_test)\n",
    "\n",
    "tuned_results = evaluate_model(y_test, y_pred_tuned, \"Random Forest (Tuned)\")\n",
    "model_results.append(tuned_results)\n",
    "trained_models['Random Forest (Tuned)'] = best_rf\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 Tuned Model Test Set Performance:\")\n",
    "print(f\"   R\u00b2 Score:  {tuned_results['R\u00b2']:.4f}\")\n",
    "print(f\"   MAE:       ${tuned_results['MAE ($)']:,.2f}\")\n",
    "print(f\"   RMSE:      ${tuned_results['RMSE ($)']:,.2f}\")\n",
    "print(f\"   MAPE:      {tuned_results['MAPE (%)']:.2f}%\")\n",
    "\n",
    "# Compare improvement\n",
    "improvement = tuned_results['R\u00b2'] - rf_results['R\u00b2']\n",
    "print(f\"\\n\ud83d\udd04 Improvement over untuned: {improvement:+.4f} R\u00b2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Model Comparison\n",
    "\n",
    "Now we compare all trained models side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_df = pd.DataFrame(model_results)\n",
    "results_df = results_df.sort_values('R\u00b2', ascending=False)\n",
    "\n",
    "print(\"\\n\ud83d\udcca Complete Model Comparison:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Filter out baselines for cleaner plots\n",
    "model_df = results_df[~results_df['Model'].str.contains('Baseline')].copy()\n",
    "\n",
    "# R\u00b2 Score\n",
    "ax1 = axes[0, 0]\n",
    "colors = sns.color_palette('viridis', len(model_df))\n",
    "bars = ax1.barh(model_df['Model'], model_df['R\u00b2'], color=colors)\n",
    "ax1.set_xlabel('R\u00b2 Score')\n",
    "ax1.set_title('R\u00b2 Score Comparison (Higher is Better)', fontweight='bold')\n",
    "ax1.set_xlim(0, 1)\n",
    "for bar, val in zip(bars, model_df['R\u00b2']):\n",
    "    ax1.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.4f}', va='center')\n",
    "\n",
    "# MAE ($)\n",
    "ax2 = axes[0, 1]\n",
    "bars = ax2.barh(model_df['Model'], model_df['MAE ($)'], color=colors)\n",
    "ax2.set_xlabel('MAE ($)')\n",
    "ax2.set_title('Mean Absolute Error (Lower is Better)', fontweight='bold')\n",
    "for bar, val in zip(bars, model_df['MAE ($)']):\n",
    "    ax2.text(val + 50, bar.get_y() + bar.get_height()/2, f'${val:,.0f}', va='center')\n",
    "\n",
    "# RMSE ($)\n",
    "ax3 = axes[1, 0]\n",
    "bars = ax3.barh(model_df['Model'], model_df['RMSE ($)'], color=colors)\n",
    "ax3.set_xlabel('RMSE ($)')\n",
    "ax3.set_title('Root Mean Squared Error (Lower is Better)', fontweight='bold')\n",
    "for bar, val in zip(bars, model_df['RMSE ($)']):\n",
    "    ax3.text(val + 50, bar.get_y() + bar.get_height()/2, f'${val:,.0f}', va='center')\n",
    "\n",
    "# MAPE\n",
    "ax4 = axes[1, 1]\n",
    "bars = ax4.barh(model_df['Model'], model_df['MAPE (%)'], color=colors)\n",
    "ax4.set_xlabel('MAPE (%)')\n",
    "ax4.set_title('Mean Absolute Percentage Error (Lower is Better)', fontweight='bold')\n",
    "for bar, val in zip(bars, model_df['MAPE (%)']):\n",
    "    ax4.text(val + 0.5, bar.get_y() + bar.get_height()/2, f'{val:.1f}%', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '04_model_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcf8 Saved: 04_model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Feature Importance Analysis\n",
    "\n",
    "Understanding which features drive predictions is crucial for **interpretability** and **business insights**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from best model\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use the best Random Forest model\n",
    "best_model = trained_models['Random Forest (Tuned)']\n",
    "\n",
    "# Get feature importances\n",
    "importances = best_model.feature_importances_\n",
    "\n",
    "# Create DataFrame\n",
    "fi_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "fi_df = fi_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Show top 15\n",
    "print(\"\\n\ud83d\udcca Top 15 Most Important Features:\")\n",
    "fi_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "top_n = 20\n",
    "top_features = fi_df.head(top_n)\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.3, 0.9, top_n))\n",
    "bars = ax.barh(range(top_n), top_features['Importance'].values[::-1], color=colors)\n",
    "ax.set_yticks(range(top_n))\n",
    "ax.set_yticklabels(top_features['Feature'].values[::-1])\n",
    "ax.set_xlabel('Feature Importance (Mean Decrease in Impurity)')\n",
    "ax.set_title(f'Top {top_n} Most Important Features', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, top_features['Importance'].values[::-1])):\n",
    "    ax.text(val + 0.002, bar.get_y() + bar.get_height()/2, f'{val:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '04_feature_importance.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcf8 Saved: 04_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Model Validation\n",
    "\n",
    "### 8.1 Actual vs Predicted Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual vs Predicted\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Log scale\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(y_test, y_pred_tuned, alpha=0.3, s=20, color='steelblue')\n",
    "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "ax1.set_xlabel('Actual (log scale)')\n",
    "ax1.set_ylabel('Predicted (log scale)')\n",
    "ax1.set_title('Actual vs Predicted CLV (Log Scale)', fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# Dollar scale\n",
    "ax2 = axes[1]\n",
    "y_test_dollars = np.expm1(y_test)\n",
    "y_pred_dollars = np.expm1(y_pred_tuned)\n",
    "ax2.scatter(y_test_dollars, y_pred_dollars, alpha=0.3, s=20, color='seagreen')\n",
    "ax2.plot([y_test_dollars.min(), y_test_dollars.max()], \n",
    "         [y_test_dollars.min(), y_test_dollars.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "ax2.set_xlabel('Actual CLV ($)')\n",
    "ax2.set_ylabel('Predicted CLV ($)')\n",
    "ax2.set_title('Actual vs Predicted CLV (Dollar Scale)', fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '04_actual_vs_predicted.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcf8 Saved: 04_actual_vs_predicted.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Lift Chart\n",
    "\n",
    "A lift chart shows how well the model discriminates between high and low value customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lift Chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Create DataFrame with actual and predicted\n",
    "lift_df = pd.DataFrame({\n",
    "    'Actual': y_test_dollars,\n",
    "    'Predicted': y_pred_dollars\n",
    "})\n",
    "\n",
    "# Sort by actual CLV\n",
    "lift_df = lift_df.sort_values('Actual').reset_index(drop=True)\n",
    "\n",
    "# Calculate cumulative averages\n",
    "lift_df['Cumulative_Actual'] = lift_df['Actual'].expanding().mean()\n",
    "lift_df['Cumulative_Predicted'] = lift_df['Predicted'].expanding().mean()\n",
    "\n",
    "# Plot\n",
    "ax.plot(lift_df.index, lift_df['Actual'], alpha=0.3, color='blue', label='Actual CLV')\n",
    "ax.plot(lift_df.index, lift_df['Predicted'], alpha=0.3, color='orange', label='Predicted CLV')\n",
    "\n",
    "# Add smoothed lines\n",
    "window = 100\n",
    "ax.plot(lift_df.index, lift_df['Actual'].rolling(window).mean(), color='blue', lw=2, label=f'Actual (Rolling Avg)')\n",
    "ax.plot(lift_df.index, lift_df['Predicted'].rolling(window).mean(), color='orange', lw=2, label=f'Predicted (Rolling Avg)')\n",
    "\n",
    "ax.set_xlabel('Customer Index (Sorted by Actual CLV)')\n",
    "ax.set_ylabel('Customer Lifetime Value ($)')\n",
    "ax.set_title('Lift Chart: Actual vs Predicted CLV', fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '04_lift_chart.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcf8 Saved: 04_lift_chart.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "residuals = y_test - y_pred_tuned\n",
    "\n",
    "# 1. Residual distribution\n",
    "ax1 = axes[0, 0]\n",
    "sns.histplot(residuals, kde=True, ax=ax1, color='steelblue', bins=50)\n",
    "ax1.axvline(0, color='red', linestyle='--', lw=2)\n",
    "ax1.set_xlabel('Residual (Actual - Predicted)')\n",
    "ax1.set_title('Residual Distribution', fontweight='bold')\n",
    "ax1.annotate(f'Mean: {residuals.mean():.4f}\\nStd: {residuals.std():.4f}', \n",
    "             xy=(0.95, 0.95), xycoords='axes fraction', ha='right', va='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 2. Residuals vs Predicted\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(y_pred_tuned, residuals, alpha=0.3, s=20, color='seagreen')\n",
    "ax2.axhline(0, color='red', linestyle='--', lw=2)\n",
    "ax2.set_xlabel('Predicted Value')\n",
    "ax2.set_ylabel('Residual')\n",
    "ax2.set_title('Residuals vs Predicted', fontweight='bold')\n",
    "\n",
    "# 3. Q-Q plot of residuals\n",
    "ax3 = axes[1, 0]\n",
    "from scipy import stats\n",
    "stats.probplot(residuals, dist=\"norm\", plot=ax3)\n",
    "ax3.set_title('Q-Q Plot of Residuals', fontweight='bold')\n",
    "\n",
    "# 4. Residuals over index (checking for patterns)\n",
    "ax4 = axes[1, 1]\n",
    "ax4.scatter(range(len(residuals)), residuals, alpha=0.3, s=20, color='coral')\n",
    "ax4.axhline(0, color='red', linestyle='--', lw=2)\n",
    "ax4.set_xlabel('Sample Index')\n",
    "ax4.set_ylabel('Residual')\n",
    "ax4.set_title('Residuals Over Samples', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIGURES_DIR, '04_residual_analysis.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\ud83d\udcf8 Saved: 04_residual_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Save Final Model\n",
    "\n",
    "We save the best model for deployment and future inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING FINAL MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Determine best model based on R\u00b2\n",
    "best_model_name = results_df[~results_df['Model'].str.contains('Baseline')].iloc[0]['Model']\n",
    "final_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"\\n\ud83c\udfc6 Best Model: {best_model_name}\")\n",
    "print(f\"   R\u00b2 Score: {results_df.iloc[0]['R\u00b2']:.4f}\")\n",
    "print(f\"   MAE: ${results_df.iloc[0]['MAE ($)']:,.2f}\")\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(MODELS_DIR, 'final_model.joblib')\n",
    "joblib.dump(final_model, model_path)\n",
    "print(f\"\\n\u2705 Model saved to: {model_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'r2_score': results_df.iloc[0]['R\u00b2'],\n",
    "    'mae_dollars': results_df.iloc[0]['MAE ($)'],\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'n_features': len(feature_names),\n",
    "    'n_training_samples': len(X_train)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(os.path.join(MODELS_DIR, 'model_metadata.json'), 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"\u2705 Metadata saved to: model_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results table\n",
    "results_df.to_csv(os.path.join(DATA_PROCESSED_DIR, 'model_comparison_results.csv'), index=False)\n",
    "print(f\"\\n\u2705 Results saved to: model_comparison_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Summary\n",
    "\n",
    "### Model Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\" * 80)\n",
    "print(\"MODELING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Models Trained: {len(trained_models)}\")\n",
    "for name in trained_models.keys():\n",
    "    print(f\"   \u2022 {name}\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfc6 Best Model: {best_model_name}\")\n",
    "print(f\"\\n\ud83d\udcc8 Final Performance Metrics (on test set):\")\n",
    "print(f\"   R\u00b2 Score:       {tuned_results['R\u00b2']:.4f} (explains {tuned_results['R\u00b2']*100:.2f}% of variance)\")\n",
    "print(f\"   MAE:            ${tuned_results['MAE ($)']:,.2f}\")\n",
    "print(f\"   RMSE:           ${tuned_results['RMSE ($)']:,.2f}\")\n",
    "print(f\"   MAPE:           {tuned_results['MAPE (%)']:.2f}%\")\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Top 5 Predictive Features:\")\n",
    "for i, row in fi_df.head(5).iterrows():\n",
    "    print(f\"   {fi_df.index.get_loc(i)+1}. {row['Feature']} ({row['Importance']:.4f})\")\n",
    "\n",
    "print(f\"\\n\u2705 Model is ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed091a",
   "metadata": {},
   "source": [
    "\n",
    "### 4.5 Masterclass: Residual Analysis & Asset Generation\n",
    "We generate high-resolution equation assets and analyze residuals to validate our Random Forest superiority.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ac042",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Equation Rendering (for Report)\n",
    "equations = {\n",
    "    'eq_clv': r\"$CLV = \\sum_{t=1}^{T} \f",
    "rac{M_t}{(1+d)^t} - CAC$\"\n",
    "}\n",
    "\n",
    "for name, latex in equations.items():\n",
    "    fig = plt.figure(figsize=(4, 1), dpi=150)\n",
    "    plt.text(0.5, 0.5, latex, fontsize=14, ha='center', va='center')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In **Notebook 05: Model Inference**, we will:\n",
    "\n",
    "1. Load the saved model and preprocessor\n",
    "2. Create an inference pipeline for new data\n",
    "3. Demonstrate scoring individual customers\n",
    "4. Show batch prediction capabilities\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook 04**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef5ed00",
   "metadata": {},
   "source": [
    "## Model Performance & Learning Curves\n",
    "\n",
    "Below are the high-resolution figures generated by our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1153032",
   "metadata": {},
   "source": [
    "### 04 Channel Efficiency\n",
    "![04 Channel Efficiency](../report/figures/04_channel_efficiency.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb4269",
   "metadata": {},
   "source": [
    "### 04 Feature Importance\n",
    "![04 Feature Importance](../report/figures/04_feature_importance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b4933",
   "metadata": {},
   "source": [
    "### 04 Prediction Analysis\n",
    "![04 Prediction Analysis](../report/figures/04_prediction_analysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4bcd4b",
   "metadata": {},
   "source": [
    "### 08 Learning Curves\n",
    "![08 Learning Curves](../report/figures/08_learning_curves.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c6134",
   "metadata": {},
   "source": [
    "### 08 Lift Chart\n",
    "![08 Lift Chart](../report/figures/08_lift_chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320c3cc3",
   "metadata": {},
   "source": [
    "### 08 Model Iterations\n",
    "![08 Model Iterations](../report/figures/08_model_iterations.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}