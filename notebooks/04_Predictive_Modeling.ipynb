{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook 04: Predictive Modeling\n",
                "\n",
                "---\n",
                "\n",
                "## Executive Summary\n",
                "\n",
                "This notebook is where the **magic happens**‚Äîwe train machine learning models to predict Customer Lifetime Value. But modeling isn't just about running algorithms; it's about **systematic experimentation, rigorous evaluation, and interpretable results**.\n",
                "\n",
                "### What This Notebook Covers:\n",
                "\n",
                "1. **Baseline Model** ‚Äî Establish a naive benchmark\n",
                "2. **Model Training** ‚Äî Linear Regression, Random Forest, Gradient Boosting\n",
                "3. **Hyperparameter Tuning** ‚Äî Optimize model performance\n",
                "4. **Model Evaluation** ‚Äî Multiple metrics (MAE, MSE, RMSE, R¬≤, MAPE)\n",
                "5. **Model Comparison** ‚Äî Statistical comparison of all models\n",
                "6. **Feature Importance** ‚Äî Understand what drives predictions\n",
                "7. **Residual Analysis** ‚Äî Diagnose model behavior\n",
                "8. **Final Model Selection** ‚Äî Choose the best model\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Environment Setup and Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# ENVIRONMENT SETUP\n",
                "# ============================================================================\n",
                "\n",
                "# Core Libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Scikit-learn\n",
                "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
                "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
                "from sklearn.model_selection import cross_val_score, GridSearchCV, learning_curve\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
                "\n",
                "# System\n",
                "import os\n",
                "import warnings\n",
                "import joblib\n",
                "from datetime import datetime\n",
                "\n",
                "# Settings\n",
                "warnings.filterwarnings('ignore')\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.float_format', '{:.4f}'.format)\n",
                "\n",
                "# Visualization\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (14, 6)\n",
                "plt.rcParams['font.size'] = 11\n",
                "\n",
                "# Random seed\n",
                "RANDOM_STATE = 42\n",
                "np.random.seed(RANDOM_STATE)\n",
                "\n",
                "print(\"‚úÖ Environment configured\")\n",
                "print(f\"   Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Path Configuration\n",
                "BASE_DIR = os.path.dirname(os.getcwd())\n",
                "DATA_PROCESSED_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
                "FIGURES_DIR = os.path.join(BASE_DIR, 'report', 'figures')\n",
                "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
                "\n",
                "# Ensure directories exist\n",
                "for d in [FIGURES_DIR, MODELS_DIR]:\n",
                "    os.makedirs(d, exist_ok=True)\n",
                "\n",
                "# Load processed data\n",
                "X_train = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'X_train_processed.csv'))\n",
                "X_test = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'X_test_processed.csv'))\n",
                "y_train = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'y_train.csv')).squeeze()\n",
                "y_test = pd.read_csv(os.path.join(DATA_PROCESSED_DIR, 'y_test.csv')).squeeze()\n",
                "\n",
                "# Load feature names\n",
                "with open(os.path.join(MODELS_DIR, 'feature_names.txt'), 'r') as f:\n",
                "    feature_names = [line.strip() for line in f.readlines()]\n",
                "\n",
                "print(f\"‚úÖ Data Loaded Successfully\")\n",
                "print(f\"   Training: {X_train.shape[0]:,} samples √ó {X_train.shape[1]} features\")\n",
                "print(f\"   Test:     {X_test.shape[0]:,} samples √ó {X_test.shape[1]} features\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Evaluation Framework\n",
                "\n",
                "Before training models, we define our **evaluation metrics** and helper functions.\n",
                "\n",
                "### Key Metrics\n",
                "\n",
                "| Metric | Formula | Interpretation |\n",
                "|--------|---------|----------------|\n",
                "| **MAE** | $\\frac{1}{n}\\sum\\|y - \\hat{y}\\|$ | Average error in same units as target |\n",
                "| **MSE** | $\\frac{1}{n}\\sum(y - \\hat{y})^2$ | Penalizes large errors more |\n",
                "| **RMSE** | $\\sqrt{MSE}$ | Same units as target, comparable to MAE |\n",
                "| **R¬≤** | $1 - \\frac{SS_{res}}{SS_{tot}}$ | % of variance explained (0-1) |\n",
                "| **MAPE** | $\\frac{100}{n}\\sum\\|\\frac{y - \\hat{y}}{y}\\|$ | Percentage error (scale-independent) |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_model(y_true, y_pred, model_name=\"Model\"):\n",
                "    \"\"\"\n",
                "    Compute comprehensive evaluation metrics.\n",
                "    \n",
                "    Note: Predictions are in log scale. We evaluate in both log scale \n",
                "    and original scale (by applying expm1 inverse transformation).\n",
                "    \"\"\"\n",
                "    # Log scale metrics\n",
                "    mae_log = mean_absolute_error(y_true, y_pred)\n",
                "    mse_log = mean_squared_error(y_true, y_pred)\n",
                "    rmse_log = np.sqrt(mse_log)\n",
                "    r2 = r2_score(y_true, y_pred)\n",
                "    \n",
                "    # Original scale (dollars)\n",
                "    y_true_dollars = np.expm1(y_true)\n",
                "    y_pred_dollars = np.expm1(y_pred)\n",
                "    \n",
                "    mae_dollars = mean_absolute_error(y_true_dollars, y_pred_dollars)\n",
                "    rmse_dollars = np.sqrt(mean_squared_error(y_true_dollars, y_pred_dollars))\n",
                "    \n",
                "    # MAPE (handle division by zero)\n",
                "    mape = np.mean(np.abs((y_true_dollars - y_pred_dollars) / (y_true_dollars + 1))) * 100\n",
                "    \n",
                "    return {\n",
                "        'Model': model_name,\n",
                "        'MAE (log)': mae_log,\n",
                "        'RMSE (log)': rmse_log,\n",
                "        'R¬≤': r2,\n",
                "        'MAE ($)': mae_dollars,\n",
                "        'RMSE ($)': rmse_dollars,\n",
                "        'MAPE (%)': mape\n",
                "    }\n",
                "\n",
                "# Initialize results storage\n",
                "model_results = []\n",
                "trained_models = {}\n",
                "\n",
                "print(\"‚úÖ Evaluation framework defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Baseline Model\n",
                "\n",
                "A **baseline model** provides a reference point. We use two naive approaches:\n",
                "1. **Mean Baseline**: Predict the mean of training target for all samples\n",
                "2. **Median Baseline**: Predict the median (more robust to outliers)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Baseline Models\n",
                "print(\"=\" * 80)\n",
                "print(\"BASELINE MODELS\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Mean baseline\n",
                "mean_baseline = np.full(len(y_test), y_train.mean())\n",
                "mean_results = evaluate_model(y_test, mean_baseline, \"Mean Baseline\")\n",
                "model_results.append(mean_results)\n",
                "\n",
                "print(f\"\\nüìä Mean Baseline:\")\n",
                "print(f\"   Predicted Value (log): {y_train.mean():.4f}\")\n",
                "print(f\"   R¬≤ Score: {mean_results['R¬≤']:.4f}\")\n",
                "print(f\"   MAE: ${mean_results['MAE ($)']:,.2f}\")\n",
                "\n",
                "# Median baseline\n",
                "median_baseline = np.full(len(y_test), y_train.median())\n",
                "median_results = evaluate_model(y_test, median_baseline, \"Median Baseline\")\n",
                "model_results.append(median_results)\n",
                "\n",
                "print(f\"\\nüìä Median Baseline:\")\n",
                "print(f\"   Predicted Value (log): {y_train.median():.4f}\")\n",
                "print(f\"   R¬≤ Score: {median_results['R¬≤']:.4f}\")\n",
                "print(f\"   MAE: ${median_results['MAE ($)']:,.2f}\")\n",
                "\n",
                "print(f\"\\nüí° Any model must beat R¬≤ = 0 (baseline) to be useful.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Model Training\n",
                "\n",
                "We train multiple algorithms to identify the best performer.\n",
                "\n",
                "### 4.1 Linear Regression\n",
                "\n",
                "A simple, interpretable baseline that assumes linear relationships."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Linear Regression\n",
                "print(\"=\" * 80)\n",
                "print(\"MODEL 1: LINEAR REGRESSION\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "lr_model = LinearRegression()\n",
                "\n",
                "# Cross-validation\n",
                "cv_scores = cross_val_score(lr_model, X_train, y_train, cv=5, scoring='r2')\n",
                "print(f\"\\nüìä 5-Fold Cross-Validation R¬≤: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
                "\n",
                "# Train on full training set\n",
                "lr_model.fit(X_train, y_train)\n",
                "\n",
                "# Predict\n",
                "y_pred_lr = lr_model.predict(X_test)\n",
                "\n",
                "# Evaluate\n",
                "lr_results = evaluate_model(y_test, y_pred_lr, \"Linear Regression\")\n",
                "model_results.append(lr_results)\n",
                "trained_models['Linear Regression'] = lr_model\n",
                "\n",
                "print(f\"\\nüìà Test Set Performance:\")\n",
                "print(f\"   R¬≤ Score:  {lr_results['R¬≤']:.4f}\")\n",
                "print(f\"   MAE:       ${lr_results['MAE ($)']:,.2f}\")\n",
                "print(f\"   RMSE:      ${lr_results['RMSE ($)']:,.2f}\")\n",
                "print(f\"   MAPE:      {lr_results['MAPE (%)']:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Random Forest Regressor\n",
                "\n",
                "An ensemble method that handles non-linear relationships and is robust to outliers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Random Forest\n",
                "print(\"=\" * 80)\n",
                "print(\"MODEL 2: RANDOM FOREST REGRESSOR\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "rf_model = RandomForestRegressor(\n",
                "    n_estimators=100,\n",
                "    max_depth=None,\n",
                "    min_samples_split=2,\n",
                "    min_samples_leaf=1,\n",
                "    random_state=RANDOM_STATE,\n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "# Cross-validation\n",
                "print(\"\\n‚è≥ Running 5-fold cross-validation...\")\n",
                "cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='r2')\n",
                "print(f\"üìä 5-Fold CV R¬≤: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
                "\n",
                "# Train\n",
                "print(\"\\n‚è≥ Training Random Forest on full training set...\")\n",
                "rf_model.fit(X_train, y_train)\n",
                "\n",
                "# Predict\n",
                "y_pred_rf = rf_model.predict(X_test)\n",
                "\n",
                "# Evaluate\n",
                "rf_results = evaluate_model(y_test, y_pred_rf, \"Random Forest\")\n",
                "model_results.append(rf_results)\n",
                "trained_models['Random Forest'] = rf_model\n",
                "\n",
                "print(f\"\\nüìà Test Set Performance:\")\n",
                "print(f\"   R¬≤ Score:  {rf_results['R¬≤']:.4f}\")\n",
                "print(f\"   MAE:       ${rf_results['MAE ($)']:,.2f}\")\n",
                "print(f\"   RMSE:      ${rf_results['RMSE ($)']:,.2f}\")\n",
                "print(f\"   MAPE:      {rf_results['MAPE (%)']:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 Gradient Boosting Regressor\n",
                "\n",
                "A powerful sequential ensemble method that often achieves state-of-the-art results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Gradient Boosting\n",
                "print(\"=\" * 80)\n",
                "print(\"MODEL 3: GRADIENT BOOSTING REGRESSOR\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "gb_model = GradientBoostingRegressor(\n",
                "    n_estimators=100,\n",
                "    learning_rate=0.1,\n",
                "    max_depth=5,\n",
                "    random_state=RANDOM_STATE\n",
                ")\n",
                "\n",
                "# Cross-validation\n",
                "print(\"\\n‚è≥ Running 5-fold cross-validation...\")\n",
                "cv_scores = cross_val_score(gb_model, X_train, y_train, cv=5, scoring='r2')\n",
                "print(f\"üìä 5-Fold CV R¬≤: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
                "\n",
                "# Train\n",
                "print(\"\\n‚è≥ Training Gradient Boosting on full training set...\")\n",
                "gb_model.fit(X_train, y_train)\n",
                "\n",
                "# Predict\n",
                "y_pred_gb = gb_model.predict(X_test)\n",
                "\n",
                "# Evaluate\n",
                "gb_results = evaluate_model(y_test, y_pred_gb, \"Gradient Boosting\")\n",
                "model_results.append(gb_results)\n",
                "trained_models['Gradient Boosting'] = gb_model\n",
                "\n",
                "print(f\"\\nüìà Test Set Performance:\")\n",
                "print(f\"   R¬≤ Score:  {gb_results['R¬≤']:.4f}\")\n",
                "print(f\"   MAE:       ${gb_results['MAE ($)']:,.2f}\")\n",
                "print(f\"   RMSE:      ${gb_results['RMSE ($)']:,.2f}\")\n",
                "print(f\"   MAPE:      {gb_results['MAPE (%)']:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Hyperparameter Tuning\n",
                "\n",
                "We optimize the best-performing model using Grid Search."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameter tuning for Random Forest\n",
                "print(\"=\" * 80)\n",
                "print(\"HYPERPARAMETER TUNING: RANDOM FOREST\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Define parameter grid\n",
                "param_grid = {\n",
                "    'n_estimators': [100, 200],\n",
                "    'max_depth': [10, 20, None],\n",
                "    'min_samples_split': [2, 5],\n",
                "    'min_samples_leaf': [1, 2]\n",
                "}\n",
                "\n",
                "print(f\"\\nüîß Parameter Grid:\")\n",
                "for param, values in param_grid.items():\n",
                "    print(f\"   {param}: {values}\")\n",
                "\n",
                "# Note: Full grid search can be time-consuming\n",
                "# For demonstration, we use a smaller subset\n",
                "print(\"\\n‚è≥ Running Grid Search (this may take a few minutes)...\")\n",
                "\n",
                "rf_tuned = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)\n",
                "\n",
                "grid_search = GridSearchCV(\n",
                "    estimator=rf_tuned,\n",
                "    param_grid=param_grid,\n",
                "    cv=3,\n",
                "    scoring='r2',\n",
                "    n_jobs=-1,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "grid_search.fit(X_train, y_train)\n",
                "\n",
                "print(f\"\\n‚úÖ Best Parameters: {grid_search.best_params_}\")\n",
                "print(f\"‚úÖ Best CV R¬≤: {grid_search.best_score_:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate tuned model\n",
                "best_rf = grid_search.best_estimator_\n",
                "y_pred_tuned = best_rf.predict(X_test)\n",
                "\n",
                "tuned_results = evaluate_model(y_test, y_pred_tuned, \"Random Forest (Tuned)\")\n",
                "model_results.append(tuned_results)\n",
                "trained_models['Random Forest (Tuned)'] = best_rf\n",
                "\n",
                "print(f\"\\nüìà Tuned Model Test Set Performance:\")\n",
                "print(f\"   R¬≤ Score:  {tuned_results['R¬≤']:.4f}\")\n",
                "print(f\"   MAE:       ${tuned_results['MAE ($)']:,.2f}\")\n",
                "print(f\"   RMSE:      ${tuned_results['RMSE ($)']:,.2f}\")\n",
                "print(f\"   MAPE:      {tuned_results['MAPE (%)']:.2f}%\")\n",
                "\n",
                "# Compare improvement\n",
                "improvement = tuned_results['R¬≤'] - rf_results['R¬≤']\n",
                "print(f\"\\nüîÑ Improvement over untuned: {improvement:+.4f} R¬≤\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Model Comparison\n",
                "\n",
                "Now we compare all trained models side by side."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison table\n",
                "print(\"=\" * 80)\n",
                "print(\"MODEL COMPARISON\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "results_df = pd.DataFrame(model_results)\n",
                "results_df = results_df.sort_values('R¬≤', ascending=False)\n",
                "\n",
                "print(\"\\nüìä Complete Model Comparison:\")\n",
                "results_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize model comparison\n",
                "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
                "\n",
                "# Filter out baselines for cleaner plots\n",
                "model_df = results_df[~results_df['Model'].str.contains('Baseline')].copy()\n",
                "\n",
                "# R¬≤ Score\n",
                "ax1 = axes[0, 0]\n",
                "colors = sns.color_palette('viridis', len(model_df))\n",
                "bars = ax1.barh(model_df['Model'], model_df['R¬≤'], color=colors)\n",
                "ax1.set_xlabel('R¬≤ Score')\n",
                "ax1.set_title('R¬≤ Score Comparison (Higher is Better)', fontweight='bold')\n",
                "ax1.set_xlim(0, 1)\n",
                "for bar, val in zip(bars, model_df['R¬≤']):\n",
                "    ax1.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.4f}', va='center')\n",
                "\n",
                "# MAE ($)\n",
                "ax2 = axes[0, 1]\n",
                "bars = ax2.barh(model_df['Model'], model_df['MAE ($)'], color=colors)\n",
                "ax2.set_xlabel('MAE ($)')\n",
                "ax2.set_title('Mean Absolute Error (Lower is Better)', fontweight='bold')\n",
                "for bar, val in zip(bars, model_df['MAE ($)']):\n",
                "    ax2.text(val + 50, bar.get_y() + bar.get_height()/2, f'${val:,.0f}', va='center')\n",
                "\n",
                "# RMSE ($)\n",
                "ax3 = axes[1, 0]\n",
                "bars = ax3.barh(model_df['Model'], model_df['RMSE ($)'], color=colors)\n",
                "ax3.set_xlabel('RMSE ($)')\n",
                "ax3.set_title('Root Mean Squared Error (Lower is Better)', fontweight='bold')\n",
                "for bar, val in zip(bars, model_df['RMSE ($)']):\n",
                "    ax3.text(val + 50, bar.get_y() + bar.get_height()/2, f'${val:,.0f}', va='center')\n",
                "\n",
                "# MAPE\n",
                "ax4 = axes[1, 1]\n",
                "bars = ax4.barh(model_df['Model'], model_df['MAPE (%)'], color=colors)\n",
                "ax4.set_xlabel('MAPE (%)')\n",
                "ax4.set_title('Mean Absolute Percentage Error (Lower is Better)', fontweight='bold')\n",
                "for bar, val in zip(bars, model_df['MAPE (%)']):\n",
                "    ax4.text(val + 0.5, bar.get_y() + bar.get_height()/2, f'{val:.1f}%', va='center')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(FIGURES_DIR, '04_model_comparison.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüì∏ Saved: 04_model_comparison.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Feature Importance Analysis\n",
                "\n",
                "Understanding which features drive predictions is crucial for **interpretability** and **business insights**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature importance from best model\n",
                "print(\"=\" * 80)\n",
                "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Use the best Random Forest model\n",
                "best_model = trained_models['Random Forest (Tuned)']\n",
                "\n",
                "# Get feature importances\n",
                "importances = best_model.feature_importances_\n",
                "\n",
                "# Create DataFrame\n",
                "fi_df = pd.DataFrame({\n",
                "    'Feature': feature_names,\n",
                "    'Importance': importances\n",
                "})\n",
                "fi_df = fi_df.sort_values('Importance', ascending=False)\n",
                "\n",
                "# Show top 15\n",
                "print(\"\\nüìä Top 15 Most Important Features:\")\n",
                "fi_df.head(15)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize feature importance\n",
                "fig, ax = plt.subplots(figsize=(12, 10))\n",
                "\n",
                "top_n = 20\n",
                "top_features = fi_df.head(top_n)\n",
                "\n",
                "colors = plt.cm.viridis(np.linspace(0.3, 0.9, top_n))\n",
                "bars = ax.barh(range(top_n), top_features['Importance'].values[::-1], color=colors)\n",
                "ax.set_yticks(range(top_n))\n",
                "ax.set_yticklabels(top_features['Feature'].values[::-1])\n",
                "ax.set_xlabel('Feature Importance (Mean Decrease in Impurity)')\n",
                "ax.set_title(f'Top {top_n} Most Important Features', fontweight='bold', fontsize=14)\n",
                "\n",
                "# Add value labels\n",
                "for i, (bar, val) in enumerate(zip(bars, top_features['Importance'].values[::-1])):\n",
                "    ax.text(val + 0.002, bar.get_y() + bar.get_height()/2, f'{val:.3f}', va='center', fontsize=9)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(FIGURES_DIR, '04_feature_importance.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüì∏ Saved: 04_feature_importance.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8. Model Validation\n",
                "\n",
                "### 8.1 Actual vs Predicted Plot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Actual vs Predicted\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Log scale\n",
                "ax1 = axes[0]\n",
                "ax1.scatter(y_test, y_pred_tuned, alpha=0.3, s=20, color='steelblue')\n",
                "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
                "ax1.set_xlabel('Actual (log scale)')\n",
                "ax1.set_ylabel('Predicted (log scale)')\n",
                "ax1.set_title('Actual vs Predicted CLV (Log Scale)', fontweight='bold')\n",
                "ax1.legend()\n",
                "\n",
                "# Dollar scale\n",
                "ax2 = axes[1]\n",
                "y_test_dollars = np.expm1(y_test)\n",
                "y_pred_dollars = np.expm1(y_pred_tuned)\n",
                "ax2.scatter(y_test_dollars, y_pred_dollars, alpha=0.3, s=20, color='seagreen')\n",
                "ax2.plot([y_test_dollars.min(), y_test_dollars.max()], \n",
                "         [y_test_dollars.min(), y_test_dollars.max()], 'r--', lw=2, label='Perfect Prediction')\n",
                "ax2.set_xlabel('Actual CLV ($)')\n",
                "ax2.set_ylabel('Predicted CLV ($)')\n",
                "ax2.set_title('Actual vs Predicted CLV (Dollar Scale)', fontweight='bold')\n",
                "ax2.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(FIGURES_DIR, '04_actual_vs_predicted.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüì∏ Saved: 04_actual_vs_predicted.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8.2 Lift Chart\n",
                "\n",
                "A lift chart shows how well the model discriminates between high and low value customers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Lift Chart\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "# Create DataFrame with actual and predicted\n",
                "lift_df = pd.DataFrame({\n",
                "    'Actual': y_test_dollars,\n",
                "    'Predicted': y_pred_dollars\n",
                "})\n",
                "\n",
                "# Sort by actual CLV\n",
                "lift_df = lift_df.sort_values('Actual').reset_index(drop=True)\n",
                "\n",
                "# Calculate cumulative averages\n",
                "lift_df['Cumulative_Actual'] = lift_df['Actual'].expanding().mean()\n",
                "lift_df['Cumulative_Predicted'] = lift_df['Predicted'].expanding().mean()\n",
                "\n",
                "# Plot\n",
                "ax.plot(lift_df.index, lift_df['Actual'], alpha=0.3, color='blue', label='Actual CLV')\n",
                "ax.plot(lift_df.index, lift_df['Predicted'], alpha=0.3, color='orange', label='Predicted CLV')\n",
                "\n",
                "# Add smoothed lines\n",
                "window = 100\n",
                "ax.plot(lift_df.index, lift_df['Actual'].rolling(window).mean(), color='blue', lw=2, label=f'Actual (Rolling Avg)')\n",
                "ax.plot(lift_df.index, lift_df['Predicted'].rolling(window).mean(), color='orange', lw=2, label=f'Predicted (Rolling Avg)')\n",
                "\n",
                "ax.set_xlabel('Customer Index (Sorted by Actual CLV)')\n",
                "ax.set_ylabel('Customer Lifetime Value ($)')\n",
                "ax.set_title('Lift Chart: Actual vs Predicted CLV', fontweight='bold')\n",
                "ax.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(FIGURES_DIR, '04_lift_chart.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüì∏ Saved: 04_lift_chart.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8.3 Residual Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Residual Analysis\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "residuals = y_test - y_pred_tuned\n",
                "\n",
                "# 1. Residual distribution\n",
                "ax1 = axes[0, 0]\n",
                "sns.histplot(residuals, kde=True, ax=ax1, color='steelblue', bins=50)\n",
                "ax1.axvline(0, color='red', linestyle='--', lw=2)\n",
                "ax1.set_xlabel('Residual (Actual - Predicted)')\n",
                "ax1.set_title('Residual Distribution', fontweight='bold')\n",
                "ax1.annotate(f'Mean: {residuals.mean():.4f}\\nStd: {residuals.std():.4f}', \n",
                "             xy=(0.95, 0.95), xycoords='axes fraction', ha='right', va='top',\n",
                "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
                "\n",
                "# 2. Residuals vs Predicted\n",
                "ax2 = axes[0, 1]\n",
                "ax2.scatter(y_pred_tuned, residuals, alpha=0.3, s=20, color='seagreen')\n",
                "ax2.axhline(0, color='red', linestyle='--', lw=2)\n",
                "ax2.set_xlabel('Predicted Value')\n",
                "ax2.set_ylabel('Residual')\n",
                "ax2.set_title('Residuals vs Predicted', fontweight='bold')\n",
                "\n",
                "# 3. Q-Q plot of residuals\n",
                "ax3 = axes[1, 0]\n",
                "from scipy import stats\n",
                "stats.probplot(residuals, dist=\"norm\", plot=ax3)\n",
                "ax3.set_title('Q-Q Plot of Residuals', fontweight='bold')\n",
                "\n",
                "# 4. Residuals over index (checking for patterns)\n",
                "ax4 = axes[1, 1]\n",
                "ax4.scatter(range(len(residuals)), residuals, alpha=0.3, s=20, color='coral')\n",
                "ax4.axhline(0, color='red', linestyle='--', lw=2)\n",
                "ax4.set_xlabel('Sample Index')\n",
                "ax4.set_ylabel('Residual')\n",
                "ax4.set_title('Residuals Over Samples', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(os.path.join(FIGURES_DIR, '04_residual_analysis.png'), dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nüì∏ Saved: 04_residual_analysis.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 9. Save Final Model\n",
                "\n",
                "We save the best model for deployment and future inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final model\n",
                "print(\"=\" * 80)\n",
                "print(\"SAVING FINAL MODEL\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "# Determine best model based on R¬≤\n",
                "best_model_name = results_df[~results_df['Model'].str.contains('Baseline')].iloc[0]['Model']\n",
                "final_model = trained_models[best_model_name]\n",
                "\n",
                "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
                "print(f\"   R¬≤ Score: {results_df.iloc[0]['R¬≤']:.4f}\")\n",
                "print(f\"   MAE: ${results_df.iloc[0]['MAE ($)']:,.2f}\")\n",
                "\n",
                "# Save model\n",
                "model_path = os.path.join(MODELS_DIR, 'final_model.joblib')\n",
                "joblib.dump(final_model, model_path)\n",
                "print(f\"\\n‚úÖ Model saved to: {model_path}\")\n",
                "\n",
                "# Save model metadata\n",
                "metadata = {\n",
                "    'model_name': best_model_name,\n",
                "    'r2_score': results_df.iloc[0]['R¬≤'],\n",
                "    'mae_dollars': results_df.iloc[0]['MAE ($)'],\n",
                "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
                "    'n_features': len(feature_names),\n",
                "    'n_training_samples': len(X_train)\n",
                "}\n",
                "\n",
                "import json\n",
                "with open(os.path.join(MODELS_DIR, 'model_metadata.json'), 'w') as f:\n",
                "    json.dump(metadata, f, indent=2)\n",
                "print(f\"‚úÖ Metadata saved to: model_metadata.json\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results table\n",
                "results_df.to_csv(os.path.join(DATA_PROCESSED_DIR, 'model_comparison_results.csv'), index=False)\n",
                "print(f\"\\n‚úÖ Results saved to: model_comparison_results.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 10. Summary\n",
                "\n",
                "### Model Training Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final Summary\n",
                "print(\"=\" * 80)\n",
                "print(\"MODELING SUMMARY\")\n",
                "print(\"=\" * 80)\n",
                "\n",
                "print(f\"\\nüìä Models Trained: {len(trained_models)}\")\n",
                "for name in trained_models.keys():\n",
                "    print(f\"   ‚Ä¢ {name}\")\n",
                "\n",
                "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
                "print(f\"\\nüìà Final Performance Metrics (on test set):\")\n",
                "print(f\"   R¬≤ Score:       {tuned_results['R¬≤']:.4f} (explains {tuned_results['R¬≤']*100:.2f}% of variance)\")\n",
                "print(f\"   MAE:            ${tuned_results['MAE ($)']:,.2f}\")\n",
                "print(f\"   RMSE:           ${tuned_results['RMSE ($)']:,.2f}\")\n",
                "print(f\"   MAPE:           {tuned_results['MAPE (%)']:.2f}%\")\n",
                "\n",
                "print(f\"\\nüìã Top 5 Predictive Features:\")\n",
                "for i, row in fi_df.head(5).iterrows():\n",
                "    print(f\"   {fi_df.index.get_loc(i)+1}. {row['Feature']} ({row['Importance']:.4f})\")\n",
                "\n",
                "print(f\"\\n‚úÖ Model is ready for deployment!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Next Steps\n",
                "\n",
                "In **Notebook 05: Model Inference**, we will:\n",
                "\n",
                "1. Load the saved model and preprocessor\n",
                "2. Create an inference pipeline for new data\n",
                "3. Demonstrate scoring individual customers\n",
                "4. Show batch prediction capabilities\n",
                "\n",
                "---\n",
                "\n",
                "**End of Notebook 04**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}