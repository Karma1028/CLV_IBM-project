{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: Model Inference\n",
    "## Putting the Model to Work\n",
    "\n",
    "**Author:** Tuhin Bhattacharya  \n",
    "**Program:** PGDM Business Data Analytics, Goa Institute of Management  \n",
    "**Project:** CLV Prediction for Auto Insurance Portfolio\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "In this notebook, I demonstrate how to use my trained CLV prediction model for real-world inference. This is where theory meets practice\u2014the model I built in previous notebooks now generates actionable predictions for business decisions.\n",
    "\n",
    "### What This Notebook Covers\n",
    "\n",
    "| Section | Purpose |\n",
    "|---------|--------|\n",
    "| **Loading Artifacts** | Model, preprocessor, and metadata |\n",
    "| **Single Prediction** | Predict CLV for one customer |\n",
    "| **Batch Prediction** | Score multiple customers at once |\n",
    "| **Segmentation** | Categorize customers by predicted value |\n",
    "| **Business Use Cases** | Practical applications |\n",
    "\n",
    "### Key Business Applications\n",
    "\n",
    "1. **New Customer Valuation**: Estimate expected value at acquisition\n",
    "2. **Retention Prioritization**: Focus resources on high-potential customers\n",
    "3. **Marketing Optimization**: Allocate budget based on predicted returns\n",
    "4. **Risk Assessment**: Identify customers likely to churn (low CLV)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Path Configuration\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "DATA_RAW_DIR = os.path.join(BASE_DIR, 'data', 'raw')\n",
    "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
    "\n",
    "print(\"\u2705 Environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Load Trained Artifacts\n",
    "\n",
    "We need three components for inference:\n",
    "1. **Trained Model** \u2014 The Random Forest regressor\n",
    "2. **Preprocessor** \u2014 The fitted ColumnTransformer\n",
    "3. **Feature Names** \u2014 To ensure correct column ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING TRAINED ARTIFACTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = joblib.load(os.path.join(MODELS_DIR, 'final_model.joblib'))\n",
    "print(f\"\\n\u2705 Model loaded: {type(model).__name__}\")\n",
    "\n",
    "# Load preprocessor\n",
    "preprocessor = joblib.load(os.path.join(MODELS_DIR, 'preprocessor.joblib'))\n",
    "print(f\"\u2705 Preprocessor loaded\")\n",
    "\n",
    "# Load model metadata\n",
    "with open(os.path.join(MODELS_DIR, 'model_metadata.json'), 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Model Metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Create Inference Function\n",
    "\n",
    "We create a reusable function that handles the complete prediction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_clv(input_data, model, preprocessor):\n",
    "    \"\"\"\n",
    "    Predict Customer Lifetime Value for new data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_data : pd.DataFrame\n",
    "        Raw customer data with required features\n",
    "    model : sklearn estimator\n",
    "        Trained prediction model\n",
    "    preprocessor : ColumnTransformer\n",
    "        Fitted preprocessing pipeline\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Input data with predicted CLV columns added\n",
    "    \"\"\"\n",
    "    # Create working copy\n",
    "    df = input_data.copy()\n",
    "    \n",
    "    # Standardize column names\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "    \n",
    "    # Normalize string columns\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if col != 'customer':\n",
    "            df[col] = df[col].astype(str).str.strip().str.lower()\n",
    "    \n",
    "    # Feature engineering (must match training)\n",
    "    if 'coverage' in df.columns and 'education' in df.columns:\n",
    "        df['coverage_education'] = df['coverage'] + '_' + df['education']\n",
    "    \n",
    "    if 'total_claim_amount' in df.columns and 'monthly_premium_auto' in df.columns:\n",
    "        df['insurance_loss_ratio'] = df['total_claim_amount'] / (df['monthly_premium_auto'] + 1)\n",
    "    \n",
    "    if 'monthly_premium_auto' in df.columns and 'number_of_policies' in df.columns:\n",
    "        df['premium_per_policy'] = df['monthly_premium_auto'] / (df['number_of_policies'] + 1)\n",
    "    \n",
    "    if 'number_of_open_complaints' in df.columns:\n",
    "        df['complaint_flag'] = (df['number_of_open_complaints'] > 0).astype(int)\n",
    "    \n",
    "    if 'months_since_policy_inception' in df.columns:\n",
    "        df['tenure_category'] = pd.cut(\n",
    "            df['months_since_policy_inception'],\n",
    "            bins=[0, 12, 36, 60, np.inf],\n",
    "            labels=['new', 'established', 'loyal', 'veteran']\n",
    "        )\n",
    "    \n",
    "    # Drop columns not needed for prediction\n",
    "    drop_cols = ['customer', 'customer_lifetime_value', 'effective_to_date', 'policy']\n",
    "    feature_df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors='ignore')\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in feature_df.columns:\n",
    "        if feature_df[col].dtype in ['object', 'category']:\n",
    "            feature_df[col] = feature_df[col].fillna('unknown')\n",
    "        else:\n",
    "            feature_df[col] = feature_df[col].fillna(feature_df[col].median())\n",
    "    \n",
    "    # Preprocess\n",
    "    X_processed = preprocessor.transform(feature_df)\n",
    "    \n",
    "    # Predict (in log scale)\n",
    "    log_predictions = model.predict(X_processed)\n",
    "    \n",
    "    # Convert to dollar scale\n",
    "    dollar_predictions = np.expm1(log_predictions)\n",
    "    \n",
    "    # Add predictions to original data\n",
    "    result_df = input_data.copy()\n",
    "    result_df['Predicted_CLV_Log'] = log_predictions\n",
    "    result_df['Predicted_CLV_Dollars'] = dollar_predictions.round(2)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "print(\"\u2705 Inference function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Single Customer Prediction\n",
    "\n",
    "Let's demonstrate predicting CLV for a single customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample customer\n",
    "print(\"=\" * 60)\n",
    "print(\"SINGLE CUSTOMER PREDICTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sample_customer = pd.DataFrame([{\n",
    "    'state': 'California',\n",
    "    'response': 'No',\n",
    "    'coverage': 'Premium',\n",
    "    'education': 'Master',\n",
    "    'employmentstatus': 'Employed',\n",
    "    'gender': 'M',\n",
    "    'income': 75000,\n",
    "    'location_code': 'Suburban',\n",
    "    'marital_status': 'Married',\n",
    "    'monthly_premium_auto': 200,\n",
    "    'months_since_last_claim': 12,\n",
    "    'months_since_policy_inception': 48,\n",
    "    'number_of_open_complaints': 0,\n",
    "    'number_of_policies': 3,\n",
    "    'policy_type': 'Corporate Auto',\n",
    "    'renew_offer_type': 'Offer1',\n",
    "    'sales_channel': 'Agent',\n",
    "    'total_claim_amount': 500,\n",
    "    'vehicle_class': 'Two-Door Car',\n",
    "    'vehicle_size': 'Medsize'\n",
    "}])\n",
    "\n",
    "print(\"\\n\ud83d\udccb Sample Customer Profile:\")\n",
    "for col, val in sample_customer.iloc[0].items():\n",
    "    print(f\"   {col}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "result = predict_clv(sample_customer, model, preprocessor)\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf PREDICTION RESULT:\")\n",
    "print(f\"   Predicted CLV (Log Scale): {result['Predicted_CLV_Log'].iloc[0]:.4f}\")\n",
    "print(f\"   Predicted CLV (Dollars):   ${result['Predicted_CLV_Dollars'].iloc[0]:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Batch Prediction\n",
    "\n",
    "For production use, we often need to score many customers at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original dataset for batch prediction demo\n",
    "print(\"=\" * 60)\n",
    "print(\"BATCH PREDICTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load raw data\n",
    "raw_data = pd.read_csv(os.path.join(DATA_RAW_DIR, 'WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv'))\n",
    "\n",
    "# Take a sample for demonstration\n",
    "sample_batch = raw_data.sample(100, random_state=42)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Scoring {len(sample_batch)} customers...\")\n",
    "\n",
    "# Make predictions\n",
    "batch_results = predict_clv(sample_batch, model, preprocessor)\n",
    "\n",
    "print(f\"\\n\u2705 Batch prediction complete!\")\n",
    "print(f\"\\n\ud83d\udcca Predicted CLV Statistics:\")\n",
    "print(f\"   Mean:   ${batch_results['Predicted_CLV_Dollars'].mean():,.2f}\")\n",
    "print(f\"   Median: ${batch_results['Predicted_CLV_Dollars'].median():,.2f}\")\n",
    "print(f\"   Min:    ${batch_results['Predicted_CLV_Dollars'].min():,.2f}\")\n",
    "print(f\"   Max:    ${batch_results['Predicted_CLV_Dollars'].max():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample results\n",
    "print(\"\\n\ud83d\udccb Sample Predictions:\")\n",
    "display_cols = ['Customer', 'Customer Lifetime Value', 'Predicted_CLV_Dollars']\n",
    "display_cols = [c for c in display_cols if c in batch_results.columns]\n",
    "batch_results[display_cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Business Use Cases\n",
    "\n",
    "### 6.1 Customer Segmentation by Predicted CLV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment customers by predicted CLV\n",
    "print(\"=\" * 60)\n",
    "print(\"CUSTOMER SEGMENTATION BY CLV\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define CLV segments\n",
    "def assign_segment(clv):\n",
    "    if clv >= 10000:\n",
    "        return 'VIP'\n",
    "    elif clv >= 6000:\n",
    "        return 'High Value'\n",
    "    elif clv >= 3000:\n",
    "        return 'Medium Value'\n",
    "    else:\n",
    "        return 'Low Value'\n",
    "\n",
    "batch_results['CLV_Segment'] = batch_results['Predicted_CLV_Dollars'].apply(assign_segment)\n",
    "\n",
    "# Segment distribution\n",
    "segment_counts = batch_results['CLV_Segment'].value_counts()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Customer Segment Distribution:\")\n",
    "for segment, count in segment_counts.items():\n",
    "    pct = count / len(batch_results) * 100\n",
    "    print(f\"   {segment:12} {count:4} customers ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 High-Value Customer Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 highest predicted CLV\n",
    "print(\"=\" * 60)\n",
    "print(\"TOP 10 HIGH-VALUE CUSTOMERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "top_customers = batch_results.nlargest(10, 'Predicted_CLV_Dollars')\n",
    "\n",
    "print(\"\\n\ud83c\udfc6 Highest Predicted CLV Customers:\")\n",
    "for i, (_, row) in enumerate(top_customers.iterrows(), 1):\n",
    "    customer_id = row.get('Customer', 'N/A')\n",
    "    clv = row['Predicted_CLV_Dollars']\n",
    "    print(f\"   {i:2}. Customer {customer_id}: ${clv:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Loading trained artifacts** for production use\n",
    "2. **Creating a reusable inference function** that handles preprocessing\n",
    "3. **Single customer prediction** for real-time scoring\n",
    "4. **Batch prediction** for bulk processing\n",
    "5. **Business applications** including segmentation and high-value identification\n",
    "\n",
    "### Deployment Considerations\n",
    "\n",
    "For production deployment, consider:\n",
    "- Wrapping the inference function in an API (e.g., Flask, FastAPI)\n",
    "- Implementing input validation and error handling\n",
    "- Setting up model monitoring and drift detection\n",
    "- Establishing a model retraining pipeline\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook 05 - Project Complete**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb9d03",
   "metadata": {},
   "source": [
    "## Retention Analysis\n",
    "\n",
    "Below are the high-resolution figures generated by our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635da707",
   "metadata": {},
   "source": [
    "### 05 Retention Sweet Spot\n",
    "![05 Retention Sweet Spot](../report/figures/05_retention_sweet_spot.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}